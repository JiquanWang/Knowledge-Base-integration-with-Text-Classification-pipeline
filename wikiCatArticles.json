{"Software engineering": "software engineer programming for the wikimedia foundation software engineering is the study and an application of engineering to the design development and maintenance of software typical formal definitions of software engineering are research design develop and test operating systems level software compilers and network distribution software for medical industrial military communications aerospace business scientific and general computing applications the systematic application of scientific and technological knowledge methods and experience to the design implementation testing and documentation of software the application of systematic disciplined quantifiable approach to the development operation and maintenance of software an engineering discipline that is concerned with all aspects of software production and the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines history margaret hamilton who adopted oettinger term software engineering standing with the data results from the simulation of the code her team designed for apollo when the first digital computers appeared in the early the instructions to make them operate were wired into the machine practitioners quickly realized that this design was not flexible and came up with the stored program architecture or von neumann architecture thus the division between hardware and software began with abstraction being used to deal with the complexity of computing programming languages started to appear in the and this was also another major step in abstraction major languages such as fortran algol and cobol were released in the late to deal with scientific algorithmic and business problems respectively dijkstra wrote his seminal paper go to statement considered harmful in and david parnas introduced the key concept of modularity and information hiding in to help programmers deal with the ever increasing complexity of software systems the term software engineering coined first by anthony oettinger and then used by margaret hamilton was used in as title for the world first conference on software engineering sponsored and facilitated by nato the conference was attended by international experts on software who agreed on defining best practices for software grounded in the application of engineering the result of the conference is report that defines how software should be developed software engineering foundations the original report is publicly available the discipline of software engineering was created to address poor quality of software get projects exceeding time and budget under control and ensure that software is built systematically rigorously measurably on time on budget and within specification engineering already addresses all these issues hence the same principles used in engineering can be applied to software the widespread lack of best practices for software at the time was perceived as software crisis barry boehm documented several key advances to the field in his book software engineering economics these include his constructive cost model cocomo which relates software development effort for program in man years to source lines of code sloc the book analyzes sixty three software projects and concludes the cost of fixing errors escalates as the project moves toward field use the book also asserts that the key driver of software cost is the capability of the software development team in the software engineering institute sei was established as federally funded research and development center headquartered on the campus of carnegie mellon university in pittsburgh pennsylvania united states watts humphrey founded the sei software process program aimed at understanding and managing the software engineering process his book managing the software process asserts that the software development process can and should be controlled measured and improved the process maturity levels introduced would become the capability maturity model integration for development cmmi dev which has defined how the us government evaluates the abilities of software development team modern generally accepted best practices for software engineering have been collected by the iso iec jtc sc subcommittee and published as the software engineering body of knowledge swebok subdisciplines software engineering can be divided into ten subdisciplines they are requirements engineering the elicitation analysis specification and validation of requirements for software software design the process of defining the architecture components interfaces and other characteristics of system or component it is also defined as the result of that process software construction the detailed creation of working meaningful software through combination of coding verification unit testing integration testing and debugging software testing an empirical technical investigation conducted to provide stakeholders with information about the quality of the product or service under test software maintenance the totality of activities required to provide cost effective support to software software configuration management the identification of the configuration of system at distinct points in time for the purpose of systematically controlling changes to the configuration and maintaining the integrity and traceability of the configuration throughout the system life cycle software engineering management the application of management activities planning coordinating measuring monitoring controlling and reporting to ensure that the development and maintenance of software is systematic disciplined and quantified software engineering process the definition implementation assessment measurement management change and improvement of the software life cycle process itself software engineering tools and methods the computer based tools that are intended to assist the software life cycle processes see computer aided software engineering and the methods which impose structure on the software engineering activity with the goal of making the activity systematic and ultimately more likely to be successful software quality management the degree to which set of inherent characteristics fulfills requirements software evolution the process of developing software initially then repeatedly updating it for various reasons education knowledge of computer programming is prerequisite to becoming software engineer in the ieee computer society produced the swebok which has been published as iso iec technical report describing the body of knowledge that they recommend to be mastered by graduate software engineer with four years of experience many software engineers enter the profession by obtaining university degree or training at vocational school one standard international curriculum for undergraduate software engineering degrees was defined by the ccse and updated in number of universities have software engineering degree programs there were campus programs online programs masters level programs doctorate level programs and certificate level programs in the united states for practitioners who wish to become proficient and recognized as professional software engineers the ieee offers two certifications that extend knowledge above level achieved by an academic degree certified software development associate and certified software development professional in addition to university education many companies sponsor internships for students wishing to pursue careers in information technology these internships can introduce the student to interesting real world tasks that typical software engineers encounter every day similar experience can be gained through military service in software engineering profession legal requirements for the licensing or certification of professional software engineers vary around the world in the uk the british computer society licenses software engineers and members of the society can also become chartered engineers ceng while in some areas of canada such as alberta british columbia ontario and quebec software engineers can hold the professional engineer eng designation and or the information systems professional designation in canada there is legal requirement to have eng when one wants to use the title engineer or practice software engineering the united states starting from offers an ncees professional engineer exam for software engineering thereby allowing software engineers to be licensed and recognized mandatory licensing is currently still largely debated and perceived as controversial in some parts of the us such as texas the use of the term engineer is regulated by law and reserved only for use by individuals who have professional engineer license the ieee informs the professional engineer license is not required unless the individual would work for public where health of others could be at risk if the engineer was not fully qualified to required standards by the particular state professional engineer licenses are specific to the state which has awarded them and have to be regularly retaken the ieee computer society and the acm the two main us based professional organizations of software engineering publish guides to the profession of software engineering the ieee guide to the software engineering body of knowledge version or swebok defines the field and describes the knowledge the ieee expects practicing software engineer to have the most current swebok is an updated version and was released in the ieee also promulgates software engineering code of ethics employment in the bureau of labor statistics counted software engineers holding jobs in the in the same time period there were some million practitioners employed in the in all other engineering disciplines combined due to its relative newness as field of study formal education in software engineering is often taught as part of computer science curriculum and many software engineers hold computer science degrees and have no engineering background whatsoever many software engineers work as employees or contractors software engineers work with businesses government agencies civilian or military and non profit organizations some software engineers work for themselves as freelancers some organizations have specialists to perform each of the tasks in the software development process other organizations require software engineers to do many or all of them in large projects people may specialize in only one role in small projects people may fill several or all roles at the same time specializations include in industry analysts architects developers testers technical support middleware analysts managers and in academia educators researchers most software engineers and programmers work hours week but about percent of software engineers and percent of programmers worked more than hours week in injuries in these occupations are rare however like other workers who spend long periods in front of computer terminal typing at keyboard engineers and programmers are susceptible to eyestrain back discomfort and hand and wrist problems such as carpal tunnel syndrome the field future looks bright according to money magazine and salary com which rated software engineer as the best job in the united states in in software engineering was again ranked as the best job in the united states this time by careercast com certification the software engineering institute offers certifications on specific topics like security process improvement and software architecture apple ibm microsoft and other companies also sponsor their own certification examinations many it certification programs are oriented toward specific technologies and managed by the vendors of these technologies these certification programs are tailored to the institutions that would employ people who use these technologies broader certification of general software engineering skills is available through various professional societies the ieee had certified over software professionals as certified software development professional csdp in they added an entry level certification known as the certified software development associate csda the acm had professional certification program in the early which was discontinued due to lack of interest the acm examined the possibility of professional certification of software engineers in the late but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering in the the british computer society has developed legally recognized professional certification called chartered it professional citp available to fully qualified members mbcs software engineers may be eligible for membership of the institution of engineering and technology and so qualify for chartered engineer status in canada the canadian information processing society has developed legally recognized professional certification called information systems professional isp in ontario canada software engineers who graduate from canadian engineering accreditation board ceab accredited program successfully complete peo professional engineers ontario professional practice examination ppe and have at least months of acceptable engineering experience are eligible to be licensed through the professional engineers ontario and can become professional engineers eng the peo does not recognize any online or distance education however and does not consider computer science programs to be equivalent to software engineering programs despite the tremendous overlap between the two this has sparked controversy and certification war it has also held the number of eng holders for the profession exceptionally low the vast majority of working professionals in the field hold degree in cs not se given the difficult certification path for holders of non se degrees most never bother to pursue the license impact of globalization the initial impact of outsourcing and the relatively lower cost of international human resources in developing third world countries led to massive migration of software development activities from corporations in north america and europe to india and later china russia and other developing countries this approach had some flaws mainly the distance timezone difference that prevented human interaction between clients and developers and the massive job transfer this had negative impact on many aspects of the software engineering profession for example some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing importing software products or services from other countries and of being displaced by foreign visa workers although statistics do not currently show threat to software engineering itself related career computer programming does appear to have been affected nevertheless the ability to smartly leverage offshore and near shore resources via the follow the sun workflow has improved the overall operational capability of many organizations when north americans are leaving work asians are just arriving to work when asians are leaving work europeans are arriving to work this provides continuous ability to have human oversight on business critical processes hours per day without paying overtime compensation or disrupting key human resource sleep patterns while global outsourcing has several advantages global and generally distributed development can run into serious difficulties resulting from the distance between developers this is due to the key elements of this type of distance which have been identified as geographical temporal cultural and communication which includes the use of different languages and dialects of english in different locations research has been carried out in the area of global software development over the last years and an extensive body of relevant work published which highlights the benefits and problems associated with the complex activity as with other aspects of software engineering research is ongoing in this and related areas related fields software engineering is direct sub field of engineering and has an overlap with computer science and management science it is also considered part of overall systems engineering controversy over definition typical formal definitions of software engineering are the application of systematic disciplined quantifiable approach to the development operation and maintenance of software an engineering discipline that is concerned with all aspects of software production the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines the term has been used less formally as the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis as the broad term for all aspects of the practice of computer programming as opposed to the theory of computer programming which is called computer science as the term embodying the advocacy of specific approach to computer programming one that urges that it be treated as an engineering discipline rather than an art or craft and advocates the codification of recommended practices criticism software engineering sees its practitioners as individuals who follow well defined engineering approaches to problem solving these approaches are specified in various software engineering books and research papers always with the connotations of predictability precision mitigated risk and professionalism this perspective has led to calls for licensing certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field software craftsmanship has been proposed by body of software developers as an alternative that emphasizes the coding skills and accountability of the software developers themselves without professionalism or any prescribed curriculum leading to ad hoc problem solving craftmanship without engineering lack of predictability precision missing risk mitigation methods are informal and poorly defined the software craftsmanship manifesto extends the agile software manifesto and draws metaphor between modern software development and the apprenticeship model of medieval europe software engineering extends engineering and draws on the engineering model engineering process engineering project management engineering requirements engineering design engineering construction and engineering validation the concept is so new that it is rarely understood and it is widely misinterpreted including in software engineering textbooks papers and among the communities of programmers and crafters one of the core issues in software engineering is that its approaches are not empirical enough because real world validation of approaches is usually absent or very limited and hence software engineering is often misinterpreted as feasible only in theoretical environment dijkstra who developed computer languages in the last century refuted the concepts of software engineering which was prevalent thirty years ago in the arguing that those terms were poor analogies for what he called the radical novelty of computer science see also bachelor of science in information technology bachelor of software engineering list of software engineering conferences list of software engineering publications software craftsmanship software engineering institute notes references further reading external links guide to the software engineering body of knowledge the open systems engineering and software development life cycle framework opensdlc org the integrated creative commons sdlc software engineering institute carnegie mellon learn software engineering software engineering society", "Multimedia": "multimedia refers to content that uses combination of different content forms this contrasts with media that use only rudimentary computer displays such as text only or traditional forms of printed or hand produced material multimedia includes combination of text audio still images animation video or interactive content forms multimedia can be recorded and played displayed dynamic interacted with or accessed by information content processing devices such as computerized and electronic devices but can also be part of live performance multimedia devices are electronic media devices used to store and experience multimedia content multimedia is distinguished from mixed media in fine art by including audio for example it has broader scope the term rich media is synonymous for interactive multimedia hypermedia scales up the amount of media content in multimedia application examples of individual content forms combined in multimedia link writing link sound recording and reproduction link image text audio still images link animation link footage link interactivity animation video footage interactivity categorization of multimedia multimedia may be broadly divided into linear and non linear categories linear active content progresses often without any navigational control for the viewer such as cinema presentation non linear uses interactivity to control progress as with video game or self paced computer based training hypermedia is an example of non linear content multimedia presentations can be live or recorded recorded presentation may allow interactivity via navigation system live multimedia presentation may allow interactivity via an interaction with the presenter or performer major characteristics of multimedia multimedia presentations may be viewed by person on stage projected transmitted or played locally with media player broadcast may be live or recorded multimedia presentation broadcasts and recordings can be either analog or digital electronic media technology digital online multimedia may be downloaded or streamed streaming multimedia may be live or on demand multimedia games and simulations may be used in physical environment with special effects with multiple users in an online network or locally with an offline computer game system or simulator the various formats of technological or digital multimedia may be intended to enhance the users experience for example to make it easier and faster to convey information or in entertainment or art to transcend everyday experience lasershow is live multimedia performance enhanced levels of interactivity are made possible by combining multiple forms of media content online multimedia is increasingly becoming object oriented and data driven enabling applications with collaborative end user innovation and personalization on multiple forms of content over time examples of these range from multiple forms of content on web sites like photo galleries with both images pictures and title text user updated to simulations whose co efficients events illustrations animations or videos are modifiable allowing the multimedia experience to be altered without reprogramming in addition to seeing and hearing haptic technology enables virtual objects to be felt emerging technology involving illusions of taste and smell may also enhance the multimedia experience terminology history of the term the term multimedia was coined by singer and artist bob goldstein later bobb goldsteinn to promote the july opening of his lightworks at oursin show at southampton long island goldstein was perhaps aware of an american artist named dick higgins who had two years previously discussed new approach to art making he called intermedia on august richard albarino of variety borrowed the terminology reporting brainchild of songscribe comic bob washington square goldstein the lightworks is the latest multi media music cum visuals to debut as discoth\u00e8que fare two years later in the term multimedia was re appropriated to describe the work of political consultant david sawyer the husband of iris sawyer one of goldstein producers at oursin multimedia multi image setup for the ford new car announcement show august detroit mi in the intervening forty years the word has taken on different meanings in the late the term referred to presentations consisting of multi projector slide shows timed to an audio track however by the multimedia took on its current meaning in the first edition of mcgraw hill multimedia making it work tay vaughan declared multimedia is any combination of text graphic art sound animation and video that is delivered by computer when you allow the user the viewer of the project to control what and when these elements are delivered it is interactive multimedia when you provide structure of linked elements through which the user can navigate interactive multimedia becomes hypermedia the german language society gesellschaft f\u00fcr deutsche sprache decided to recognize the word significance and ubiquitousness in the by awarding it the title of word of the year in the institute summed up its rationale by stating multimedia has become central word in the wonderful new media world in common usage multimedia refers to an electronically delivered combination of media including video still images audio text in such way that can be accessed interactively much of the content on the web today falls within this definition as understood by millions some computers which were marketed in the were called multimedia computers because they incorporated cd rom drive which allowed for the delivery of several hundred megabytes of video picture and audio data that era saw also boost in the production of educational multimedia cd roms word usage and context since media is the plural of medium the term multimedia is used to describe multiple occurrences of only one form of media such as collection of audio cds this is why it important that the word multimedia is used exclusively to describe multiple forms of media and content the term multimedia is also ambiguous static content such as paper book may be considered multimedia if it contains both pictures and text or may be considered interactive if the user interacts by turning pages at will books may also be considered non linear if the pages are accessed non sequentially the term video if not used exclusively to describe motion photography is ambiguous in multimedia terminology video is often used to describe the file format delivery format or presentation format instead of footage which is used to distinguish motion photography from animation of rendered motion imagery multiple forms of information content are often not considered modern forms of presentation such as audio or video likewise single forms of information content with single methods of information processing non interactive audio are often called multimedia perhaps to distinguish static media from active media in the fine arts for example leda luss luyken modulart brings two key elements of musical composition and film into the world of painting variation of theme and movement of and within picture making modulart an interactive multimedia form of art performing arts may also be considered multimedia considering that performers and props are multiple forms of both content and media the gesellschaft f\u00fcr deutsche sprache chose multimedia as german word of the year usage application powerpoint corporate presentations may combine all forms of media content virtual reality uses multimedia content applications and delivery platforms of multimedia are virtually limitless vvo multimedia terminal in dresden wtc germany multimedia finds its application in various areas including but not limited to advertisements art education entertainment engineering medicine mathematics business scientific research and spatial temporal applications several examples are as follows creative industries creative industries use multimedia for variety of purposes ranging from fine arts to entertainment to commercial art to journalism to media and software services provided for any of the industries listed below an individual multimedia designer may cover the spectrum throughout their career request for their skills range from technical to analytical to creative commercial uses much of the electronic old and new media used by commercial artists and graphic designers is multimedia exciting presentations are used to grab and keep attention in advertising business to business and interoffice communications are often developed by creative services firms for advanced multimedia presentations beyond simple slide shows to sell ideas or liven up training commercial multimedia developers may be hired to design for governmental services and nonprofit services applications as well entertainment and fine arts in addition multimedia is heavily used in the entertainment industry especially to develop special effects in movies and animations vfx animation etc multimedia games are popular pastime and are software programs available either as cd roms or online some video games also use multimedia features multimedia applications that allow users to actively participate instead of just sitting by as passive recipients of information are called interactive multimedia in the arts there are multimedia artists whose minds are able to blend techniques using different media that in some way incorporates interaction with the viewer one of the most relevant could be peter greenaway who is melding cinema with opera and all sorts of digital media another approach entails the creation of multimedia that can be displayed in traditional fine arts arena such as an art gallery although multimedia display material may be volatile the survivability of the content is as strong as any traditional media digital recording material may be just as durable and infinitely reproducible with perfect copies every time education in education multimedia is used to produce computer based training courses popularly called cbts and reference books like encyclopedia and almanacs cbt lets the user go through series of presentations text about particular topic and associated illustrations in various information formats edutainment is the combination of education with entertainment especially multimedia entertainment learning theory in the past decade has expanded dramatically because of the introduction of multimedia several lines of research have evolved cognitive load multimedia learning and the list goes on the possibilities for learning and instruction are nearly endless the idea of media convergence is also becoming major factor in education particularly higher education defined as separate technologies such as voice and telephony features data and productivity applications and video that now share resources and interact with each other synergistically creating new efficiencies media convergence is rapidly changing the curriculum in universities all over the world likewise it is changing the availability or lack thereof of jobs requiring this savvy technological skill the english education in middle school in china is well invested and assisted with various equipments in contrast the original objective has not been achieved at the desired effect the government schools families and students spend lot of time working on improving scores but hardly gain practical skills english education today has gone into the vicious circle educators need to consider how to perfect the education system to improve students practical ability of english therefore an efficient way should be used to make the class vivid multimedia teaching will bring students into class where they can interact with the teacher and the subject multimedia teaching is more intuitive than old ways teachers can simulate situations in real life in many circumstances teachers do not have to be there students will learn by themselves in the class more importantly teachers will have more approaches to stimulating students passion of learning journalism newspaper companies all over are also trying to embrace the new phenomenon by implementing its practices in their work while some have been slow to come around other major newspapers like the new york times usa today and the washington post are setting the precedent for the positioning of the newspaper industry in globalized world news reporting is not limited to traditional media outlets freelance journalists can make use of different new media to produce multimedia pieces for their news stories it engages global audiences and tells stories with technology which develops new communication techniques for both media producers and consumers the common language project later renamed to the seattle globalist is an example of this type of multimedia journalism production multimedia reporters who are mobile usually driving around community with cameras audio and video recorders and laptop computers are often referred to as mojos from mo bile jo urnalist engineering software engineers may use multimedia in computer simulations for anything from entertainment to training such as military or industrial training multimedia for software interfaces are often done as collaboration between creative professionals and software engineers industry in the industrial sector multimedia is used as way to help present information to shareholders superiors and coworkers multimedia is also helpful for providing employee training advertising and selling products all over the world via virtually unlimited web based technology mathematical and scientific research in mathematical and scientific research multimedia is mainly used for modeling and simulation for example scientist can look at molecular model of particular substance and manipulate it to arrive at new substance representative research can be found in journals such as the journal of multimedia medicine in medicine doctors can get trained by looking at virtual surgery or they can simulate how the human body is affected by diseases spread by viruses and bacteria and then develop techniques to prevent it multimedia application like virtual surgeries also help doctors to get practical training document imaging document imaging is technique that takes hard copy of an image document and converts it into digital format for example scanners disabilities ability media allows those with disabilities to gain qualifications in the multimedia field so they can pursue careers that give them access to wide array of powerful communication forms miscellaneous in europe the reference organisation for multimedia industry is the european multimedia associations convention emmac structuring information in multimedia form multimedia represents the convergence of text pictures video and sound into single form the power of multimedia and the internet lies in the way in which information is linked multimedia and the internet require completely new approach to writing the style of writing that is appropriate for the on line world is highly optimized and designed to be able to be quickly scanned by readers good site must be made with specific purpose in mind and site with good interactivity and new technology can also be useful for attracting visitors the site must be attractive and innovative in its design function in terms of its purpose easy to navigate frequently updated and fast to download when users view page they can only view one page at time as result multimedia users must create mental model of information structure conferences there is large number of multimedia conferences the two main scholarly scientific conferences being acm multimedia ieee icme international conference on multimedia expo see also artmedia cross media multi image multimedia literacy multimedia messaging service multimedia search new media art postliterate society web documentary references external links history of multimedia from the university of calgary multimedia in answers com", "Information retrieval": "information retrieval ir is the activity of obtaining information resources relevant to an information need from collection of information resources searches can be based on metadata or on full text or other content based indexing automated information retrieval systems are used to reduce what has been called information overload many universities and public libraries use ir systems to provide access to books journals and other documents web search engines are the most visible ir applications overview an information retrieval process begins when user enters query into the system queries are formal statements of information needs for example search strings in web search engines in information retrieval query does not uniquely identify single object in the collection instead several objects may match the query perhaps with different degrees of relevancy an object is an entity that is represented by information in database user queries are matched against the database information depending on the application the data objects may be for example text documents images audio mind maps or videos often the documents themselves are not kept or stored directly in the ir system but are instead represented in the system by document surrogates or metadata most ir systems compute numeric score on how well each object in the database matches the query and rank the objects according to this value the top ranking objects are then shown to the user the process may then be iterated if the user wishes to refine the query history the idea of using computers to search for relevant pieces of information was popularized in the article as we may think by vannevar bush in it would appear that bush was inspired by patents for statistical machine filed by emanuel goldberg in the and that searched for documents stored on film the first description of computer searching for information was described by holmstrom in detailing an early mention of the univac computer automated information retrieval systems were introduced in the one even featured in the romantic comedy desk set in the the first large information retrieval research group was formed by gerard salton at cornell by the several different retrieval techniques had been shown to perform well on small text corpora such as the cranfield collection several thousand documents large scale retrieval systems such as the lockheed dialog system came into use early in the in the us department of defense along with the national institute of standards and technology nist cosponsored the text retrieval conference trec as part of the tipster text program the aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on very large text collection this catalyzed research on methods that scale to huge corpora the introduction of web search engines has boosted the need for very large scale retrieval systems even further model types german entry original source dominik kuropka for effectively retrieving relevant documents by ir strategies the documents are typically transformed into suitable representation each retrieval strategy incorporates specific model for its document representation purposes the picture on the right illustrates the relationship of some common models in the picture the models are categorized according to two dimensions the mathematical basis and the properties of the model first dimension mathematical basis set theoretic models represent documents as sets of words or phrases similarities are usually derived from set theoretic operations on those sets common models are standard boolean model extended boolean model fuzzy retrieval algebraic models represent documents and queries usually as vectors matrices or tuples the similarity of the query vector and document vector is represented as scalar value vector space model generalized vector space model enhanced topic based vector space model extended boolean model latent semantic indexing latent semantic analysis probabilistic models treat the process of document retrieval as probabilistic inference similarities are computed as probabilities that document is relevant for given query probabilistic theorems like the bayes theorem are often used in these models binary independence model probabilistic relevance model on which is based the okapi bm relevance function uncertain inference language models divergence from randomness model latent dirichlet allocation feature based retrieval models view documents as vectors of values of feature functions or just features and seek the best way to combine these features into single relevance score typically by learning to rank methods feature functions are arbitrary functions of document and query and as such can easily incorporate almost any other retrieval model as just yet another feature second dimension properties of the model models without term treat different terms words as independent this fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables models with immanent term allow representation of between terms however the degree of the interdependency between two terms is defined by the model itself it is usually directly or indirectly derived by dimensional reduction from the co occurrence of those terms in the whole set of documents models with transcendent term allow representation of between terms but they do not allege how the interdependency between two terms is defined they rely an external source for the degree of interdependency between two terms for example human or sophisticated algorithms performance and correctness measures the evaluation of an information retrieval system is the process of assessing how well system meets the information needs of its users traditional evaluation metrics designed for boolean retrieval or top retrieval include precision and recall many more measures for evaluating the performance of information retrieval systems have also been proposed in general measurement considers collection of documents to be searched and search query all common measures described here assume ground truth notion of relevancy every document is known to be either relevant or non relevant to particular query in practice queries may be ill posed and there may be different shades of relevancy virtually all modern evaluation metrics mean average precision discounted cumulative gain are designed for ranked retrieval without any explicit rank cutoff taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks the mathematical symbols used in the formulas below mean intersection in this case specifying the documents in both sets and cardinality in this case the number of documents in set integral summation symmetric difference precision precision is the fraction of the documents retrieved that are relevant to the user information need in binary classification precision is analogous to positive predictive value precision takes all retrieved documents into account it can also be evaluated at given cut off rank considering only the topmost results returned by the system this measure is called precision at or note that the meaning and usage of precision in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics recall recall is the fraction of the documents that are relevant to the query that are successfully retrieved in binary classification recall is often called sensitivity so it can be looked at as the probability that relevant document is retrieved by the query it is trivial to achieve recall of by returning all documents in response to any query therefore recall alone is not enough but one needs to measure the number of non relevant documents also for example by computing the precision fall out the proportion of non relevant documents that are retrieved out of all non relevant documents available in binary classification fall out is closely related to specificity and is equal to it can be looked at as the probability that non relevant document is retrieved by the query it is trivial to achieve fall out of by returning zero documents in response to any query score measure the weighted harmonic mean of precision and recall the traditional measure or balanced score is this is also known as the measure because recall and precision are evenly weighted the general formula for non negative real is two other commonly used measures are the measure which weights recall twice as much as precision and the measure which weights precision twice as much as recall the measure was derived by van rijsbergen so that measures the effectiveness of retrieval with respect to user who attaches times as much importance to recall as precision it is based on van rijsbergen effectiveness measure their relationship is where measure can be better single metric when compared to precision and recall both precision and recall give different information that can complement each other when combined if one of them excels more than the other measure will reflect it average precision precision and recall are single value metrics based on the whole list of documents returned by the system for systems that return ranked sequence of documents it is desirable to also consider the order in which the returned documents are presented by computing precision and recall at every position in the ranked sequence of documents one can plot precision recall curve plotting precision as function of recall average precision computes the average value of over the interval from to that is the area under the precision recall curve this integral is in practice replaced with finite sum over every position in the ranked sequence of documents where is the rank in the sequence of retrieved documents is the number of retrieved documents is the precision at cut off in the list and is the change in recall from items to this finite sum is equivalent to where is an indicator function equaling if the item at rank is relevant document zero otherwise note that the average is over all relevant documents and the relevant documents not retrieved get precision score of zero some authors choose to interpolate the function to reduce the impact of wiggles in the curve for example the pascal visual object classes challenge benchmark for computer vision object detection computes average precision by averaging the precision over set of evenly spaced recall levels where is an interpolated precision that takes the maximum precision over all recalls greater than an alternative is to derive an analytical function by assuming particular parametric distribution for the underlying decision values for example binormal precision recall curve can be obtained by assuming decision values in both classes to follow gaussian distribution precision at for modern web scale information retrieval recall is no longer meaningful metric as many queries have thousands of relevant documents and few users will be interested in reading all of them precision at documents is still useful metric or precision at corresponds to the number of relevant results on the first search results page but fails to take into account the positions of the relevant documents among the top another shortcoming is that on query with fewer relevant results than even perfect system will have score less than it easier to score manually since only the top results need to be examined to determine if they are relevant or not precision precision requires knowing all documents that are relevant to query the number of relevant documents is used as the cutoff for calculation and this varies from query to query for example if there are documents relevant to red in corpus precision for red looks at the top documents returned counts the number that are relevant turns that into relevancy fraction precision is equal to recall at the th position empirically this measure is often highly correlated to mean average precision mean average precision mean average precision for set of queries is the mean of the average precision scores for each query where is the number of queries discounted cumulative gain dcg uses graded relevance scale of documents from the result set to evaluate the usefulness or gain of document based on its position in the result list the premise of dcg is that highly relevant documents appearing lower in search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result the dcg accumulated at particular rank position is defined as since result set may vary in size among different queries or systems to compare performances the normalised version of dcg uses an ideal dcg to this end it sorts documents of result list by relevance producing an ideal dcg at position which normalizes the score the ndcg values for all queries can be averaged to obtain measure of the average performance of ranking algorithm note that in perfect ranking algorithm the will be the same as the producing an ndcg of all ndcg calculations are then relative values on the interval to and so are cross query comparable other measures mean reciprocal rank spearman rank correlation coefficient bpref summation based measure of how many relevant documents are ranked before irrelevant documents gmap geometric mean of per topic average precision gain improvement over random of any of the above measures measures based on marginal relevance and document diversity see visualization visualizations of information retrieval performance include graphs which chart precision on one axis and recall on the other histograms of average precision over various topics receiver operating characteristic roc curve confusion matrix timeline before the joseph marie jacquard invents the jacquard loom the first machine to use punched cards to control sequence of operations herman hollerith invents an electro mechanical data tabulator using punch cards as machine readable medium hollerith cards keypunches and tabulators used to process the us census data emanuel goldberg submits patents for his statistical machine document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents late the us military confronted problems of indexing and retrieval of wartime scientific research documents captured from germans vannevar bush as we may think appeared in atlantic monthly hans peter luhn research engineer at ibm since began work on mechanized punch card based system for searching chemical compounds growing concern in the us for science gap with the ussr motivated encouraged funding and provided backdrop for mechanized literature searching systems allen kent et al and the invention of citation indexing eugene garfield the term information retrieval appears to have been coined by calvin mooers philip bagley conducted the earliest experiment in computerized document retrieval in master thesis at mit allen kent joined case western reserve university and eventually became associate director of the center for documentation and communications research that same year kent and colleagues published paper in american documentation describing the precision and recall measures as well as detailing proposed framework for evaluating an ir system which included statistical sampling methods for determining the number of relevant documents not retrieved international conference on scientific information washington dc included consideration of ir systems as solution to problems identified see proceedings of the international conference on scientific information national academy of sciences washington dc hans peter luhn published auto encoding of documents for information retrieval early gerard salton began work on ir at harvard later moved to cornell melvin earl maron and john lary kuhns published on relevance probabilistic indexing and information retrieval in the journal of the acm july cyril cleverdon published early findings of the cranfield studies developing model for ir system evaluation see cyril cleverdon report on the testing and analysis of an investigation into the comparative efficiency of indexing systems cranfield collection of aeronautics cranfield england kent published information analysis and retrieval weinberg report science government and information gave full articulation of the idea of crisis of scientific information the report was named after dr alvin weinberg joseph becker and robert hayes published text on information retrieval becker joseph hayes robert mayo information storage and retrieval tools elements theories new york wiley karen sp\u00e4rck jones finished her thesis at cambridge synonymy and semantic classification and continued work on computational linguistics as it applies to ir the national bureau of standards sponsored symposium titled statistical association methods for mechanized documentation several highly significant papers including salton first published reference we believe to the smart system mid national library of medicine developed medlars medical literature analysis and retrieval system the first major machine readable database and batch retrieval system project intrex at mit licklider published libraries of the future don swanson was involved in studies at university of chicago on requirements for future catalogs late wilfrid lancaster completed evaluation studies of the medlars system and published the first edition of his text on information retrieval gerard salton published automatic information organization and retrieval john sammon jr radc tech report some mathematics of information storage and retrieval outlined the vector model sammon nonlinear mapping for data structure analysis ieee transactions on computers was the first proposal for visualization interface to an ir system early first online systems nlm aim twx medline lockheed dialog sdc orbit theodor nelson promoting concept of hypertext published computer lib dream machines nicholas jardine and cornelis van rijsbergen published the use of hierarchic clustering in information retrieval which articulated the cluster hypothesis three highly influential publications by salton fully articulated his vector processing framework and term discrimination model theory of indexing society for industrial and applied mathematics theory of term importance in automatic text analysis jasis vector space model for automatic indexing cacm the first acm sigir conference van rijsbergen published information retrieval butterworths heavy emphasis on probabilistic models tamas doszkocs implemented the cite natural language user interface for medline at the national library of medicine the cite system supported free form query input ranked output and relevance feedback first international acm sigir conference joint with british computer society ir group in cambridge nicholas belkin robert oddy and helen brooks proposed the ask anomalous state of knowledge viewpoint for information retrieval this was an important concept though their automated analysis tool proved ultimately disappointing salton and michael mcgill published introduction to modern information retrieval mcgraw hill with heavy emphasis on vector models david blair and bill maron publish an evaluation of retrieval effectiveness for full text document retrieval system mid efforts to develop end user versions of commercial ir systems key papers on and experimental systems for visualization interfaces work by donald crouch robert korfhage matthew chalmers anselm spoerri and others first world wide web proposals by tim berners lee at cern first trec conference publication of korfhage information storage and retrieval with emphasis on visualization and multi reference point systems late web search engines implementation of many features formerly found only in experimental ir systems search engines become the most common and maybe best instantiation of ir models awards in the field tony kent strix award gerard salton award see also adversarial information retrieval collaborative information seeking controlled vocabulary cross language information retrieval data mining european summer school in information retrieval human computer information retrieval information extraction information retrieval facility knowledge visualization multimedia information retrieval list of information retrieval systems personal information management relevance information retrieval relevance feedback rocchio classification search index social information seeking special interest group on information retrieval subject indexing temporal information retrieval tf idf xml retrieval references further reading christopher manning prabhakar raghavan and hinrich sch\u00fctze introduction to information retrieval cambridge university press stefan b\u00fcttcher charles clarke and gordon cormack information retrieval implementing and evaluating search engines mit press cambridge mass external links acm sigir information retrieval special interest group bcs irsg british computer society information retrieval specialist group text retrieval conference trec forum for information retrieval evaluation fire information retrieval online book by van rijsbergen information retrieval wiki information retrieval facility information retrieval duth trec report on information retrieval evaluation techniques how ebay measures search relevance", "Computer vision": "computer vision is field that includes methods for acquiring processing analyzing and understanding images and in general high dimensional data from the real world in order to produce numerical or symbolic information in the forms of decisions theme in the development of this field has been to duplicate the abilities of human vision by electronically perceiving and understanding an image this image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry physics statistics and learning theory computer vision has also been described as the enterprise of automating and integrating wide range of processes and representations for vision perception as scientific discipline computer vision is concerned with the theory behind artificial systems that extract information from images the image data can take many forms such as video sequences views from multiple cameras or multi dimensional data from medical scanner as technological discipline computer vision seeks to apply its theories and models to the construction of computer vision systems sub domains of computer vision include scene reconstruction event detection video tracking object recognition object pose estimation learning indexing motion estimation and image restoration related fields relation between computer vision and various other fields areas of artificial intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment detailed understanding of these environments is required to navigate through them information about the environment could be provided by computer vision system acting as vision sensor and providing high level information about the environment and the robot artificial intelligence and computer vision share other topics such as pattern recognition and learning techniques consequently computer vision is sometimes seen as part of the artificial intelligence field or the computer science field in general solid state physics is another field that is closely related to computer vision most computer vision systems rely on image sensors which detect electromagnetic radiation which is typically in the form of either visible or infra red light the sensors are designed using quantum physics the process by which light interacts with surfaces is explained using physics physics explains the behavior of optics which are core part of most imaging systems sophisticated image sensors even require quantum mechanics to provide complete understanding of the image formation process also various measurement problems in physics can be addressed using computer vision for example motion in fluids third field which plays an important role is neurobiology specifically the study of the biological vision system over the last century there has been an extensive study of eyes neurons and the brain structures devoted to processing of visual stimuli in both humans and various animals this has led to coarse yet complicated description of how real vision systems operate in order to solve certain vision related tasks these results have led to subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity also some of the learning based methods developed within computer vision neural net and deep learning based image and feature analysis and classification have their background in biology some strands of computer vision research are closely related to the study of biological vision indeed just as many strands of ai research are closely tied with research into human consciousness and the use of stored knowledge to interpret integrate and utilize visual information the field of biological vision studies and models the physiological processes behind visual perception in humans and other animals computer vision on the other hand studies and describes the processes implemented in software and hardware behind artificial vision systems exchange between biological and computer vision has proven fruitful for both fields yet another field related to computer vision is signal processing many methods for processing of one variable signals typically temporal signals can be extended in natural way to processing of two variable signals or multi variable signals in computer vision however because of the specific nature of images there are many methods developed within computer vision which have no counterpart in processing of one variable signals together with the multi dimensionality of the signal this defines subfield in signal processing as part of computer vision beside the above mentioned views on computer vision many of the related research topics can also be studied from purely mathematical point of view for example many methods in computer vision are based on statistics optimization or geometry finally significant part of the field is devoted to the implementation aspect of computer vision how existing methods can be realized in various combinations of software and hardware or how these methods can be modified in order to gain processing speed without losing too much performance the fields most closely related to computer vision are image processing image analysis and machine vision there is significant overlap in the range of techniques and applications that these cover this implies that the basic techniques that are used and developed in these fields are more or less identical something which can be interpreted as there is only one field with different names on the other hand it appears to be necessary for research groups scientific journals conferences and companies to present or market themselves as belonging specifically to one of these fields and hence various which distinguish each of the fields from the others have been presented computer vision is in some ways the inverse of computer graphics while computer graphics produces image data from models computer vision often produces models from image data there is also trend towards combination of the two disciplines as explored in augmented reality the following appear relevant but should not be taken as universally accepted image processing and image analysis tend to focus on images how to transform one image to another by pixel wise operations such as contrast enhancement local operations such as edge extraction or noise removal or geometrical transformations such as rotating the image this implies that image processing analysis neither require assumptions nor produce interpretations about the image content computer vision includes analysis from images this analyzes the scene projected onto one or several images how to reconstruct structure or other information about the scene from one or several images computer vision often relies on more or less complex assumptions about the scene depicted in an image machine vision is the process of applying range of technologies methods to provide imaging based automatic inspection process control and robot guidance in industrial applications machine vision tends to focus on applications mainly in manufacturing vision based autonomous robots and systems for vision based inspection or measurement this implies that image sensor technologies and control theory often are integrated with the processing of image data to control robot and that real time processing is emphasised by means of efficient implementations in hardware and software it also implies that the external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision which can enable the use of different algorithms there is also field called imaging which primarily focus on the process of producing images but sometimes also deals with processing and analysis of images for example medical imaging includes substantial work on the analysis of image data in medical applications finally pattern recognition is field which uses various methods to extract information from signals in general mainly based on statistical approaches and artificial neural networks significant part of this field is devoted to applying these methods to image data photogrammetry also overlaps with computer vision vs stereo computer vision applications for computer vision applications range from tasks such as industrial machine vision systems which say inspect bottles speeding by on production line to research into artificial intelligence and computers or robots that can comprehend the world around them the computer vision and machine vision fields have significant overlap computer vision covers the core technology of automated image analysis which is used in many fields machine vision usually refers to process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications in many computer vision applications the computers are pre programmed to solve particular task but methods based on learning are now becoming increasingly common examples of applications of computer vision include systems for controlling processes an industrial robot navigation by an autonomous vehicle or mobile robot detecting events for visual surveillance or people counting organizing information for indexing databases of images and image sequences modeling objects or environments medical image analysis or topographical modeling interaction as the input to device for computer human interaction and automatic inspection in manufacturing applications darpa visual media reasoning concept video one of the most prominent application fields is medical computer vision or medical image processing this area is characterized by the extraction of information from image data for the purpose of making medical diagnosis of patient generally image data is in the form of microscopy images ray images angiography images ultrasonic images and tomography images an example of information which can be extracted from such image data is detection of tumours or other malign changes it can also be measurements of organ dimensions blood flow etc this application area also supports medical research by providing new information about the structure of the brain or about the quality of medical treatments applications of computer vision in the medical area also includes enhancement of images that are interpreted by humans for example ultrasonic images or ray images to reduce the influence of noise second application area in computer vision is in industry sometimes called machine vision where information is extracted for the purpose of supporting manufacturing process one example is quality control where details or final products are being automatically inspected in order to find defects another example is measurement of position and orientation of details to be picked up by robot arm machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material process called optical sorting military applications are probably one of the largest areas for computer vision the obvious examples are detection of enemy soldiers or vehicles and missile guidance more advanced systems for missile guidance send the missile to an area rather than specific target and target selection is made when the missile reaches the area based on locally acquired image data modern military concepts such as battlefield awareness imply that various sensors including image sensors provide rich set of information about combat scene which can be used to support strategic decisions in this case automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability artist concept of rover on mars an example of an unmanned land based vehicle notice the stereo cameras mounted on top of the rover one of the newer application areas is autonomous vehicles which include submersibles land based vehicles small robots with wheels cars or trucks aerial vehicles and unmanned aerial vehicles uav the level of autonomy ranges from fully autonomous unmanned vehicles to vehicles where computer vision based systems support driver or pilot in various situations fully autonomous vehicles typically use computer vision for navigation for knowing where it is or for producing map of its environment slam and for detecting obstacles it can also be used for detecting certain task specific events uav looking for forest fires examples of supporting systems are obstacle warning systems in cars and systems for autonomous landing of aircraft several car manufacturers have demonstrated systems for autonomous driving of cars but this technology has still not reached level where it can be put on the market there are ample examples of military autonomous vehicles ranging from advanced missiles to uavs for recon missions or missile guidance space exploration is already being made with autonomous vehicles using computer vision nasa mars exploration rover and esa exomars rover other application areas include support of visual effects creation for cinema and broadcast camera tracking matchmoving surveillance typical tasks of computer vision each of the application areas described above employ range of computer vision tasks more or less well defined measurement problems or processing problems which can be solved using variety of methods some examples of typical computer vision tasks are presented below recognition the classical problem in computer vision image processing and machine vision is that of determining whether or not the image data contains some specific object feature or activity different varieties of the recognition problem are described in the literature object recognition also called object classification one or several pre specified or learned objects or object classes can be recognized usually together with their positions in the image or poses in the scene google goggles or likethat provide stand alone programs that illustrate this function identification an individual instance of an object is recognized examples include identification of specific person face or fingerprint identification of handwritten digits or identification of specific vehicle detection the image data are scanned for specific condition examples include detection of possible abnormal cells or tissues in medical images or detection of vehicle in an automatic road toll system detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce correct interpretation currently the best algorithms for such tasks are based on convolutional neural networks an illustration of their capabilities is given by the imagenet large scale visual recognition challenge this is benchmark in object classification and detection with millions of images and hundreds of object classes performance of convolutional neural networks on the imagenet tests is now close to that of humans the best algorithms still struggle with objects that are small or thin such as small ant on stem of flower or person holding quill in their hand they also have trouble with images that have been distorted with filters an increasingly common phenomenon with modern digital cameras by contrast those kinds of images rarely trouble humans humans however tend to have trouble with other issues for example they are not good at classifying objects into fine grained classes such as the particular breed of dog or species of bird whereas convolutional neural networks handle this with ease several specialized tasks based on recognition exist such as content based image retrieval finding all images in larger set of images which have specific content the content can be specified in different ways for example in terms of similarity relative target image give me all images similar to image or in terms of high level search criteria given as text input give me all images which contains many houses are taken during winter and have no cars in them computer vision for people counter purposes in public places malls shopping centres pose estimation estimating the position or orientation of specific object relative to the camera an example application for this technique would be assisting robot arm in retrieving objects from conveyor belt in an assembly line situation or picking parts from bin optical character recognition ocr identifying characters in images of printed or handwritten text usually with view to encoding the text in format more amenable to editing or indexing ascii code reading reading of codes such as data matrix and qr codes facial recognition shape recognition technology srt in people counter systems differentiating human beings head and shoulder patterns from objects motion analysis several tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the scene or even of the camera that produces the images examples of such tasks are egomotion determining the rigid motion rotation and translation of the camera from an image sequence produced by the camera tracking following the movements of usually smaller set of interest points or objects vehicles or humans in the image sequence optical flow to determine for each point in the image how that point is moving relative to the image plane its apparent motion this motion is result both of how the corresponding point is moving in the scene and how the camera is moving relative to the scene scene reconstruction given one or typically more images of scene or video scene reconstruction aims at computing model of the scene in the simplest case the model can be set of points more sophisticated methods produce complete surface model the advent of imaging not requiring motion or scanning and related processing algorithms is enabling rapid advances in this field grid based sensing can be used to acquire images from multiple angles algorithms are now available to stitch multiple images together into point clouds and models image restoration the aim of image restoration is the removal of noise sensor noise motion blur etc from images the simplest possible approach for noise removal is various types of filters such as low pass filters or median filters more sophisticated methods assume model of how the local image structures look like model which distinguishes them from the noise by first analysing the image data in terms of the local image structures such as lines or edges and then controlling the filtering based on local information from the analysis step better level of noise removal is usually obtained compared to the simpler approaches an example in this field is inpainting computer vision system methods the organization of computer vision system is highly application dependent some systems are stand alone applications which solve specific measurement or detection problem while others constitute sub system of larger design which for example also contains sub systems for control of mechanical actuators planning information databases man machine interfaces etc the specific implementation of computer vision system also depends on if its functionality is pre specified or if some part of it can be learned or modified during operation many functions are unique to the application there are however typical functions which are found in many computer vision systems image acquisition digital image is produced by one or several image sensors which besides various types of light sensitive cameras include range sensors tomography devices radar ultra sonic cameras etc depending on the type of sensor the resulting image data is an ordinary image volume or an image sequence the pixel values typically correspond to light intensity in one or several spectral bands gray images or colour images but can also be related to various physical measures such as depth absorption or reflectance of sonic or electromagnetic waves or nuclear magnetic resonance pre processing before computer vision method can be applied to image data in order to extract some specific piece of information it is usually necessary to process the data in order to assure that it satisfies certain assumptions implied by the method examples are re sampling in order to assure that the image coordinate system is correct noise reduction in order to assure that sensor noise does not introduce false information contrast enhancement to assure that relevant information can be detected scale space representation to enhance image structures at locally appropriate scales feature extraction image features at various levels of complexity are extracted from the image data typical examples of such features are lines edges and ridges localized interest points such as corners blobs or points more complex features may be related to texture shape or motion detection segmentation at some point in the processing decision is made about which image points or regions of the image are relevant for further processing examples are selection of specific set of interest points segmentation of one or multiple image regions which contain specific object of interest high level processing at this step the input is typically small set of data for example set of points or an image region which is assumed to contain specific object the remaining processing deals with for example verification that the data satisfy model based and application specific assumptions estimation of application specific parameters such as object pose or object size image detected object into different categories image and combining two different views of the same object decision making making the final decision required for the application for example pass fail on automatic inspection applications match no match in recognition applications flag for further human review in medical military security and recognition applications computer vision hardware there are many kinds of computer vision systems nevertheless all of them contain these basic elements power source at least one image acquisition device camera ccd etc processor as well as control and communication cables or some kind of wireless interconnection mechanism in addition practical vision system contains software as well as display in order to monitor the system vision systems for inner spaces as most industrial ones contain an illumination system and may be placed in controlled environment furthermore completed system includes many accessories like camera supports cables and connectors see also ai effect applications of artificial intelligence machine vision glossary artificial neural networks deep learning lists list of computer vision topics list of emerging technologies outline of artificial intelligence references further reading external links usc iris computer vision conference list computer vision papers on the web complete list of papers of the most relevant computer vision conferences computer vision online news source code datasets and job offers related to computer vision keith price annotated computer vision bibliography cvonline bob fisher compendium of computer vision british machine vision association supporting computer vision research within the uk via the bmvc and miua conferences annals of the bmva open source journal bmva summer school and one day meetings", "Graphics": "graphics from greek graphikos something written autograph are visual images or designs on some surface such as wall canvas screen paper or stone to inform illustrate or entertain in contemporary usage it includes neeke pictorial representation of data as in computer aided design and manufacture in typesetting and the graphic arts and in educational and neeke recreational software images that are generated by computer are called computer graphics examples are photographs drawings line art graphs diagrams typography numbers symbols geometric designs maps engineering drawings or other images graphics often combine text illustration and color graphic design may consist of the deliberate selection creation or arrangement of typography alone as in brochure flyer poster web site or book without any other element clarity or effective communication may be the objective association with other cultural elements may be sought or merely the creation of distinctive style graphics can be functional or artistic the latter can be recorded version such as photograph or an interpretation by scientist to highlight essential features or an artist in which case the distinction with imaginary graphics may become blurred history the earliest graphics known to anthropologists studying prehistoric periods are cave paintings and markings on boulders bone ivory and antlers which were created during the upper palaeolithic period from or earlier many of these were found to record astronomical seasonal and chronological details some of the earliest graphics and drawings known to the modern world from almost years ago are that of engraved stone tablets and ceramic cylinder seals marking the beginning of the historic periods and the keeping of records for accounting and inventory purposes records from egypt predate these and papyrus was used by the egyptians as material on which to plan the building of pyramids they also used slabs of limestone and wood from bc the greeks played major role in geometry they used graphics to represent their mathematical theories such as the circle theorem and the pythagorean theorem in art graphics is often used to distinguish work in monotone and made up of lines as opposed to painting drawing drawing generally involves making marks on surface by applying pressure from tool or moving tool across surface in which tool is always used as if there were no tools it would be art graphical drawing is an instrumental guided drawing printmaking woodblock printing including images is first seen in china after paper was invented about in the west the main techniques have been woodcut engraving and etching but there are many others etching etching is an intaglio method of printmaking in which the image is incised into the surface of metal plate using an acid the acid eats the metal leaving behind roughened areas or if the surface exposed to the acid is very thin burning line into the plate the use of the process in printmaking is believed to have been invented by daniel hopfer of augsburg germany who decorated armour in this way etching is also used in the manufacturing of printed circuit boards and semiconductor devices line art line art is rather non specific term sometimes used for any image that consists of distinct straight and curved lines placed against usually plain background without gradations in shade darkness or hue color to represent two dimensional or three dimensional objects line art is usually monochromatic although lines may be of different colors illustration an illustration of character from story also an illustration of illustrations an illustration is visual representation such as drawing painting photograph or other work of art that stresses subject more than form the aim of an illustration is to elucidate or decorate story poem or piece of textual information such as newspaper article traditionally by providing visual representation of something described in the text the editorial cartoon also known as political cartoon is an illustration containing political or social message illustrations can be used to display wide range of subject matter and serve variety of functions such as giving faces to characters in story displaying number of examples of an item described in an academic textbook typology visualising step wise sets of instructions in technical manual communicating subtle thematic tone in narrative linking brands to the ideas of human expression individuality and creativity making reader laugh or smile for fun to make laugh funny graphs graph or chart is information graphic that represents tabular numeric data charts are often used to make it easier to understand large quantities of data and the relationships between different parts of the data diagrams diagram is simplified and structured visual representation of concepts ideas constructions relations statistical data etc used to visualize and clarify the topic symbols symbol in its basic sense is representation of concept or quantity an idea object concept quality etc in more psychological and philosophical terms all concepts are symbolic in nature and representations for these concepts are simply token artifacts that are allegorical to but do not directly codify symbolic meaning or symbolism maps map is simplified depiction of space navigational aid which highlights relations between objects within that space usually map is two dimensional geometrically accurate representation of three dimensional space one of the first modern maps was made by waldseem\u00fcller photography one difference between photography and other forms of graphics is that photographer in principle just records single moment in reality with seemingly no interpretation however photographer can choose the field of view and angle and may also use other techniques such as various lenses to distort the view or filters to change the colors in recent times digital photography has opened the way to an infinite number of fast but strong manipulations even in the early days of photography there was controversy over photographs of enacted scenes that were presented as real life especially in war photography where it can be very difficult to record the original events shifting the viewer eyes ever so slightly with simple pinpricks in the negative could have dramatic effect the choice of the field of view can have strong effect effectively censoring out other parts of the scene accomplished by cropping them out or simply not including them in the photograph this even touches on the philosophical question of what reality is the human brain processes information based on previous experience making us see what we want to see or what we were taught to see photography does the same although the photographer interprets the scene for their viewer engineering drawings image of part represented in first angle projection an engineering drawing is type of drawing and is technical in nature used to fully and clearly define requirements for engineered items it is usually created in accordance with standardized conventions for layout nomenclature interpretation appearance such as typefaces and line styles size etc computer graphics there are two types of computer graphics raster graphics where each pixel is separately defined as in digital photograph and vector graphics where mathematical formulas are used to draw lines and shapes which are then interpreted at the viewer end to produce the graphic using vectors results in infinitely sharp graphics and often smaller files but when complex like vectors take time to render and may have larger file sizes than raster equivalent in the first computer driven display was attached to mit whirlwind computer to generate simple pictures this was followed by mit tx and tx interactive computing which increased interest in computer graphics during the late in ivan sutherland invented sketchpad an innovative program that influenced alternative forms of interaction with computers in the mid large computer graphics research projects were begun at mit general motors bell labs and lockheed corporation douglas ross of mit developed an advanced compiler language for graphics programming coons also at mit and ferguson at boeing began work in sculptured surfaces gm developed their dac system and other companies such as douglas lockheed and mcdonnell also made significant developments in ray tracing was first described by arthur appel of the ibm research center yorktown heights during the late personal computers became more powerful capable of drawing both basic and complex shapes and designs in the artists and graphic designers began to see the personal computer particularly the commodore amiga and macintosh as serious design tool one that could save time and draw more accurately than other methods computer graphics became possible in the late with the powerful sgi computers which were later used to create some of the first fully computer generated short films at pixar the macintosh remains one of the most popular tools for computer graphics in graphic design studios and businesses modern computer systems dating from the and onwards often use graphical user interface gui to present data and information with symbols icons and pictures rather than text graphics are one of the five key elements of multimedia technology graphics became more popular in the in gaming multimedia and animation in quake one of the first fully games was released in toy story the first full length computer generated animation film was released in cinemas since then computer graphics have become more accurate and detailed due to more advanced computers and better modeling software applications such as maya studio max and cinema another use of computer graphics is screensavers originally intended to preventing the layout of much used guis from burning into the computer screen they have since evolved into true pieces of art their practical purpose obsolete modern screens are not susceptible to such burn in artifacts web graphics signature art used on web forums in the internet speeds increased and internet browsers capable of viewing images were released the first being mosaic websites began to use the gif format to display small graphics such as banners advertisements and navigation buttons on web pages modern web browsers can now display jpeg png and increasingly svg images in addition to gifs on web pages svg and to some extent vml support in some modern web browsers have made it possible to display vector graphics that are clear at any size plugins expand the web browser functions to display animated interactive and graphics contained within file formats such as swf and modern web graphics can be made with software such as adobe photoshop the gimp or corel paint shop pro users of microsoft windows have ms paint which many find to be lacking in features this is because ms paint is drawing package and not graphics package numerous platforms and websites have been created to cater to web graphics artists and to host their communities growing number of people use create internet forum signatures generally appearing after user post and other digital artwork such as photo manipulations and large graphics with computer games developers creating their own communities around their products many more websites are being developed to offer graphics for the fans and to enable them to show their appreciation of such games in their own gaming profiles uses graphics are visual elements often used to point readers and viewers to particular information they are also used to supplement text in an effort to aid readers in their understanding of particular concept or make the concept more clear or interesting popular magazines such as time wired and newsweek usually contain graphic material in abundance to attract readers unlike the majority of scholarly journals in computing they are used to create graphical interface for the user and graphics are one of the five key elements of multimedia technology graphics are among the primary ways of advertising the sale of goods or services business graphics are commonly used in business and economics to create financial charts and tables the term business graphics came into use in the late when personal computers became capable of drawing graphs and charts instead of using tabular format business graphics can be used to highlight changes over period of time advertising advertising is one of the most profitable uses of graphics artists often do advertising work or take advertising potential into account when creating art to increase the chances of selling the artwork most importantly graphics gives good look to artwork whenever it is applied graphics contribute to the general outlook of designed artwork this in turn lure interested members of the public to look at the work of art or purchasing it any graphical work especially advertisement or any work of art that is poorly design will not persuade the audience therefore for an advertisement to persuade and convince readers or viewers it must be well designed with needed graphical tools so as to bring profit to the designer or advertiser political the use of graphics for overtly political purposes cartoons graffiti poster art flag design etc is centuries old practice which thrives today in every part of the world the northern irish murals are one such example more recent example is shepard fairey presidential election barack obama hope poster it was first published on the web but soon found its way onto streets throughout the united states education graphics are heavily used in textbooks especially those concerning subjects such as geography science and mathematics in order to illustrate theories and concepts such as the human anatomy diagrams are also used to label photographs and pictures educational animation is an important emerging field of graphics animated graphics have obvious advantages over static graphics when explaining subject matter that changes over time the oxford illustrated dictionary uses graphics and technical illustrations to make reading material more interesting and easier to understand in an encyclopedia graphics are used to illustrate concepts and show examples of the particular topic being discussed in order for graphic to function effectively as an educational aid the learner must be able to interpret it successfully this interpretative capacity is one aspect of graphicacy film and animation computer graphics are often used in the majority of new feature films especially those with large budget films that heavily use computer graphics include the lord of the rings film trilogy the harry potter films spider man and war of the worlds graphics education the majority of schools colleges and universities around the world educate students on the subject of graphics and art the subject is taught in broad variety of ways each course teaching its own distinctive balance of craft skills and intellectual response to the client needs some graphics courses prioritize traditional craft skills drawing printmaking and typography over modern craft skills other courses may place an emphasis on teaching digital craft skills still other courses may downplay the crafts entirely concentrating on training students to generate novel intellectual responses that engage with the brief despite these apparent differences in training and curriculum the staff and students on any of these courses will generally consider themselves to be graphic designers the typical pedagogy of graphic design or graphic communication visual communication graphic arts or any number of synonymous course titles will be broadly based on the teaching models developed in the bauhaus school in germany or vkhutemas in russia the teaching model will tend to expose students to variety of craft skills currently everything from drawing to motion capture combined with an effort to engage the student with the world of visual culture famous graphic designers aldus manutius designed the first italic type style which is often used in desktop publishing and graphic design april greiman is known for her influential poster design paul rand is well known as design pioneer for designing many popular corporate logos including the logo for ibm next and ups william caslon during the mid th century designed many typefaces including itc founder caslon itc founder caslon ornaments caslon graphique itc caslon no caslon old face and big caslon examples of graphics image tulip jpg photograph image portrait_ jpg drawing see also semiotics editorial cartoon references \u0987\u09a8 external links free graphics download historical timeline of computer graphics and animation", "Data mining": "data mining the analysis step of the knowledge discovery in databases process or kdd an subfield of computer science is the computational process of discovering patterns in large data sets big data involving methods at the intersection of artificial intelligence machine learning statistics and database systems the overall goal of the data mining process is to extract information from data set and transform it into an understandable structure for further use aside from the raw analysis step it involves database and data management aspects data pre processing model and inference considerations interestingness metrics complexity considerations post processing of discovered structures visualization and online updating the term is misnomer because the goal is the extraction of patterns and knowledge from large amount of data not the extraction of data itself it also is buzzword and is frequently applied to any form of large scale data or information processing collection extraction warehousing analysis and statistics as well as any application of computer decision support system including artificial intelligence machine learning and business intelligence the popular book data mining practical machine learning tools and techniques with java which covers mostly machine learning material was originally to be named just practical machine learning and the term data mining was only added for marketing reasons often the more general terms large scale data analysis or analytics or when referring to actual methods artificial intelligence and machine learning are more appropriate the actual data mining task is the automatic or semi automatic analysis of large quantities of data to extract previously unknown interesting patterns such as groups of data records cluster analysis unusual records anomaly detection and dependencies association rule mining this usually involves using database techniques such as spatial indices these patterns can then be seen as kind of summary of the input data and may be used in further analysis or for example in machine learning and predictive analytics for example the data mining step might identify multiple groups in the data which can then be used to obtain more accurate prediction results by decision support system neither the data collection data preparation nor result interpretation and reporting are part of the data mining step but do belong to the overall kdd process as additional steps the related terms data dredging data fishing and data snooping refer to the use of data mining methods to sample parts of larger population data set that are or may be too small for reliable statistical inferences to be made about the validity of any patterns discovered these methods can however be used in creating new hypotheses to test against the larger data populations etymology in the statisticians used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an priori hypothesis the term data mining appeared around in the database community for short time in phrase database mining was used but since it was trademarked by hnc san diego based company to pitch their database mining workstation researchers consequently turned to data mining other terms used include data archaeology information harvesting information discovery knowledge extraction etc gregory piatetsky shapiro coined the term knowledge discovery in databases for the first workshop on the same topic kdd and this term became more popular in ai and machine learning community however the term data mining became more popular in the business and press communities currently data mining and knowledge discovery are used interchangeably since about predictive analytics and since data science terms were also used to describe this field background the manual extraction of patterns from data has occurred for centuries early methods of identifying patterns in data include bayes theorem and regression analysis the proliferation ubiquity and increasing power of computer technology has dramatically increased data collection storage and manipulation ability as data sets have grown in size and complexity direct hands on data analysis has increasingly been augmented with indirect automated data processing aided by other discoveries in computer science such as neural networks cluster analysis genetic algorithms decision trees and decision rules and support vector machines data mining is the process of applying these methods with the intention of uncovering hidden patterns in large data sets it bridges the gap from applied statistics and artificial intelligence which usually provide the mathematical background to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently allowing such methods to be applied to ever larger data sets research and evolution the premier professional body in the field is the association for computing machinery acm special interest group sig on knowledge discovery and data mining sigkdd since this acm sig has hosted an annual international conference and published its proceedings and since it has published biannual academic journal titled sigkdd explorations computer science conferences on data mining include cikm conference acm conference on information and knowledge management dmin conference international conference on data mining dmkd conference research issues on data mining and knowledge discovery ecdm conference european conference on data mining ecml pkdd conference european conference on machine learning and principles and practice of knowledge discovery in databases edm conference international conference on educational data mining infocom conference ieee infocom icdm conference ieee international conference on data mining kdd conference acm sigkdd conference on knowledge discovery and data mining mldm conference machine learning and data mining in pattern recognition pakdd conference the annual pacific asia conference on knowledge discovery and data mining paw conference predictive analytics world sdm conference siam international conference on data mining siam sstd symposium symposium on spatial and temporal databases wsdm conference acm conference on web search and data mining data mining topics are also present on many data management database conferences such as the icde conference sigmod conference and international conference on very large data bases process the knowledge discovery in databases kdd process is commonly defined with the stages selection pre processing transformation data mining interpretation evaluation it exists however in many variations on this theme such as the cross industry standard process for data mining crisp dm which defines six phases business understanding data understanding data preparation modeling evaluation deployment or simplified process such as pre processing data mining and results validation polls conducted in and show that the crisp dm methodology is the leading methodology used by data miners the only other data mining standard named in these polls was semma however times as many people reported using crisp dm several teams of researchers have published reviews of data mining process models and azevedo and santos conducted comparison of crisp dm and semma in pre processing before data mining algorithms can be used target data set must be assembled as data mining can only uncover patterns actually present in the data the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit common source for data is data mart or data warehouse pre processing is essential to analyze the multivariate data sets before data mining the target set is then cleaned data cleaning removes the observations containing noise and those with missing data data mining data mining involves six common classes of tasks anomaly detection outlier change deviation detection the identification of unusual data records that might be interesting or data errors that require further investigation association rule learning dependency modelling searches for relationships between variables for example supermarket might gather data on customer purchasing habits using association rule learning the supermarket can determine which products are frequently bought together and use this information for marketing purposes this is sometimes referred to as market basket analysis clustering is the task of discovering groups and structures in the data that are in some way or another similar without using known structures in the data classification is the task of generalizing known structure to apply to new data for example an mail program might attempt to classify an mail as legitimate or as spam regression attempts to find function which models the data with the least error summarization providing more compact representation of the data set including visualization and report generation results validation data mining can unintentionally be misused and can then produce results which appear to be significant but which do not actually predict future behavior and cannot be reproduced on new sample of data and bear little use often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing simple version of this problem in machine learning is known as overfitting but the same problem can arise at different phases of the process and thus train test split when applicable at all may not be sufficient to prevent this from happening the final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set not all patterns found by the data mining algorithms are necessarily valid it is common for the data mining algorithms to find patterns in the training set which are not present in the general data set this is called overfitting to overcome this the evaluation uses test set of data on which the data mining algorithm was not trained the learned patterns are applied to this test set and the resulting output is compared to the desired output for example data mining algorithm trying to distinguish spam from legitimate emails would be trained on training set of sample mails once trained the learned patterns would be applied to the test set of mails on which it had not been trained the accuracy of the patterns can then be measured from how many mails they correctly classify number of statistical methods may be used to evaluate the algorithm such as roc curves if the learned patterns do not meet the desired standards subsequently it is necessary to re evaluate and change the pre processing and data mining steps if the learned patterns do meet the desired standards then the final step is to interpret the learned patterns and turn them into knowledge standards there have been some efforts to define standards for the data mining process for example the european cross industry standard process for data mining crisp dm and the java data mining standard jdm development on successors to these processes crisp dm and jdm was active in but has stalled since jdm was withdrawn without reaching final draft for exchanging the extracted models in particular for use in predictive analytics the key standard is the predictive model markup language pmml which is an xml based language developed by the data mining group dmg and supported as exchange format by many data mining applications as the name suggests it only covers prediction models particular data mining task of high importance to business applications however extensions to cover for example subspace clustering have been proposed independently of the dmg notable uses data mining is used wherever there is digital data available today notable examples of data mining can be found throughout business medicine science and surveillance privacy concerns and ethics while the term data mining itself has no ethical implications it is often associated with the mining of information in relation to peoples behavior ethical and otherwise the ways in which data mining can be used can in some cases and contexts raise questions regarding privacy legality and ethics in particular data mining government or commercial data sets for national security or law enforcement purposes such as in the total information awareness program or in advise has raised privacy concerns data mining requires data preparation which can uncover information or patterns which may compromise confidentiality and privacy obligations common way for this to occur is through data aggregation data aggregation involves combining data together possibly from various sources in way that facilitates analysis but that also might make identification of private individual level data deducible or otherwise apparent this is not data mining per se but result of the preparation of data before and for the purposes of the analysis the threat to an individual privacy comes into play when the data once compiled cause the data miner or anyone who has access to the newly compiled data set to be able to identify specific individuals especially when the data were originally anonymous it is recommended that an individual is made aware of the following before data are collected the purpose of the data collection and any known data mining projects how the data will be used who will be able to mine the data and use the data and their derivatives the status of security surrounding access to the data how collected data can be updated data may also be modified so as to become anonymous so that individuals may not readily be identified however even de identified anonymized data sets can potentially contain enough information to allow identification of individuals as occurred when journalists were able to find several individuals based on set of search histories that were inadvertently released by aol situation in europe europe has rather strong privacy laws and efforts are underway to further strengthen the rights of the consumers however the safe harbor principles currently effectively expose european users to privacy exploitation by companies as consequence of edward snowden global surveillance disclosure there has been increased discussion to revoke this agreement as in particular the data will be fully exposed to the national security agency and attempts to reach an agreement have failed situation in the united states in the united states privacy concerns have been addressed by the us congress via the passage of regulatory controls such as the health insurance portability and accountability act hipaa the hipaa requires individuals to give their informed consent regarding information they provide and its intended present and future uses according to an article in biotech business week in practice hipaa may not offer any greater protection than the longstanding regulations in the research arena says the aahc more importantly the rule goal of protection through informed consent is undermined by the complexity of consent forms that are required of patients and participants which approach level of to average individuals this underscores the necessity for data anonymity in data aggregation and mining practices information privacy legislation such as hipaa and the family educational rights and privacy act ferpa applies only to the specific areas that each such law addresses use of data mining by the majority of businesses in the is not controlled by any legislation copyright law situation in europe due to lack of flexibilities in european copyright and database law the mining of in copyright works such as web mining without the permission of the copyright owner is not legal where database is pure data in europe there is likely to be no copyright but database rights may exist so data mining becomes subject to regulations by the database directive on the recommendation of the hargreaves review this led to the uk government to amend its copyright law in to allow content mining as limitation and exception only the second country in the world to do so after japan which introduced an exception in for data mining however due to the restriction of the copyright directive the uk exception only allows content mining for non commercial purposes uk copyright law also does not allow this provision to be overridden by contractual terms and conditions the european commission facilitated stakeholder discussion on text and data mining in under the title of licences for europe the focus on the solution to this legal issue being licences and not limitations and exceptions led to representatives of universities researchers libraries civil society groups and open access publishers to leave the stakeholder dialogue in may situation in the united states by contrast to europe the flexible nature of us copyright law and in particular fair use means that content mining in america as well as other fair use countries such as israel taiwan and south korea is viewed as being legal as content mining is transformative that is it does not supplant the original work it is viewed as being lawful under fair use for example as part of the google book settlement the presiding judge on the case ruled that google digitisation project of in copyright books was lawful in part because of the transformative uses that the digitisation project displayed one being text and data mining software free open source data mining software and applications carrot text and search results clustering framework chemicalize org chemical structure miner and web search engine elki university research project with advanced cluster analysis and outlier detection methods written in the java language gate natural language processing and language engineering tool knime the konstanz information miner user friendly and comprehensive data analytics framework massive online analysis moa real time big data stream mining with concept drift tool in the java programming language ml flex software package that enables users to integrate with third party machine learning packages written in any programming language execute classification analyses in parallel across multiple computing nodes and produce html reports of classification results mlpack library collection of ready to use machine learning algorithms written in the language nltk natural language toolkit suite of libraries and programs for symbolic and statistical natural language processing nlp for the python language opennn open neural networks library orange component based data mining and machine learning software suite written in the python language programming language and software environment for statistical computing data mining and graphics it is part of the gnu project scavis java cross platform data analysis framework developed at argonne national laboratory senticnet api semantic and affective resource for opinion mining and sentiment analysis tanagra visualisation oriented data mining software also for teaching torch an open source deep learning library for the lua programming language and scientific computing framework with wide support for machine learning algorithms uima the uima unstructured information management architecture is component framework for analyzing unstructured content such as text audio and video originally developed by ibm weka suite of machine learning software applications written in the java programming language commercial data mining software and applications angoss knowledgestudio data mining tool provided by angoss clarabridge enterprise class text analytics solution grapheme data mining and visualization software provided by ichrome hp vertica analytics platform data mining software provided by hp ibm spss modeler data mining software provided by ibm kxen modeler data mining tool provided by kxen lionsolver an integrated software application for data mining business intelligence and modeling that implements the learning and intelligent optimization lion approach microsoft analysis services data mining software provided by microsoft netowl suite of multilingual text and entity analytics products that enable data mining opentext big data analytics visual data mining predictive analysis by open text corporation oracle data mining data mining software by oracle pseven platform for automation of engineering simulation and analysis optimization and data mining provided by datadvance qlucore omics explorer data mining software provided by qlucore rapidminer an environment for machine learning and data mining experiments sas enterprise miner data mining software provided by the sas institute statistica data miner data mining software provided by statsoft marketplace surveys several researchers and organizations have conducted reviews of data mining tools and surveys of data miners these identify some of the strengths and weaknesses of the software packages they also provide an overview of the behaviors preferences and views of data miners some of these reports include wiley reviews data mining and knowledge discovery rexer analytics data miner surveys forrester research predictive analytics and data mining solutions report gartner magic quadrant report robert nisbet three part series of articles data mining tools which one is best for crm haughton et al review of data mining software packages in the american statistician goebel gruenwald survey of data mining knowledge discovery software tools in sigkdd explorations see also methods application domains application examples related topics data mining is about analyzing data for information about extracting information out of data see references further reading cabena peter hadjnian pablo stadler rolf verhees jaap zanasi alessandro discovering data mining from concept to implementation prentice hall isbn chen han yu data mining an overview from database perspective knowledge and data engineering ieee transactions on feldman ronen sanger james the text mining handbook cambridge university press isbn guo yike and grossman robert editors high performance data mining scaling algorithms applications and systems kluwer academic publishers han jiawei micheline kamber and jian pei data mining concepts and techniques morgan kaufmann hastie trevor tibshirani robert and friedman jerome the elements of statistical learning data mining inference and prediction springer isbn liu bing web data mining exploring hyperlinks contents and usage data springer isbn nisbet robert elder john miner gary handbook of statistical analysis data mining applications academic press elsevier isbn poncelet pascal masseglia florent and teisseire maguelonne editors october data mining patterns new methods and applications information science reference isbn tan pang ning steinbach michael and kumar vipin introduction to data mining isbn theodoridis sergios and koutroumbas konstantinos pattern recognition th edition academic press isbn weiss sholom and indurkhya nitin predictive data mining morgan kaufmann see also free weka software ye nong the handbook of data mining mahwah nj lawrence erlbaum external links", "Simulation": "wooden mechanical horse simulator during world war simulation is the imitation of the operation of real world process or system over time the act of simulating something first requires that model be developed this model represents the key characteristics or behaviors functions of the selected physical or abstract system or process the model represents the system itself whereas the simulation represents the operation of the system over time simulation is used in many contexts such as simulation of technology for performance optimization safety engineering testing training education and video games often computer experiments are used to study simulation models simulation is also used with scientific modelling of natural systems or human systems to gain insight into their functioning simulation can be used to show the eventual real effects of alternative conditions and courses of action simulation is also used when the real system cannot be engaged because it may not be accessible or it may be dangerous or unacceptable to engage or it is being designed but not yet built or it may simply not exist key issues in simulation include acquisition of valid source information about the relevant selection of key characteristics and behaviours the use of simplifying approximations and assumptions within the simulation and fidelity and validity of the simulation outcomes classification and terminology human in the loop simulation of outer space visualization of direct numerical simulation model historically simulations used in different fields developed largely independently but th century studies of systems theory and cybernetics combined with spreading use of computers across all those fields have led to some unification and more systematic view of the concept physical simulation refers to simulation in which physical objects are substituted for the real thing some circles use the term for computer simulations modelling selected laws of physics but this article doesn these physical objects are often chosen because they are smaller or cheaper than the actual object or system interactive simulation is special kind of physical simulation often referred to as human in the loop simulation in which physical simulations include human operators such as in flight simulator or driving simulator human in the loop simulations can include computer simulation as so called synthetic environment simulation in failure analysis refers to simulation in which we create environment conditions to identify the cause of equipment failure this was the best and fastest method to identify the failure cause computer simulation computer simulation or sim is an attempt to model real life or hypothetical situation on computer so that it can be studied to see how the system works by changing variables in the simulation predictions may be made about the behaviour of the system it is tool to virtually investigate the behaviour of the system under study computer simulation has become useful part of modeling many natural systems in physics chemistry and biology and human systems in economics and social science computational sociology as well as in engineering to gain insight into the operation of those systems good example of the usefulness of using computers to simulate can be found in the field of network traffic simulation in such simulations the model behaviour will change each simulation according to the set of initial parameters assumed for the environment traditionally the formal modeling of systems has been via mathematical model which attempts to find analytical solutions enabling the prediction of the behaviour of the system from set of parameters and initial conditions computer simulation is often used as an adjunct to or substitution for modeling systems for which simple closed form analytic solutions are not possible there are many different types of computer simulation the common feature they all share is the attempt to generate sample of representative scenarios for model in which complete enumeration of all possible states would be prohibitive or impossible several software packages exist for running computer based simulation modeling monte carlo simulation stochastic modeling multimethod modeling that makes all the modeling almost effortless modern usage of the term computer simulation may encompass virtually any computer based representation computer science in computer science simulation has some specialized meanings alan turing used the term simulation to refer to what happens when universal machine executes state transition table in modern terminology computer runs program that describes the state transitions inputs and outputs of subject discrete state machine the computer simulates the subject machine accordingly in theoretical computer science the term simulation is relation between state transition systems useful in the study of operational semantics less theoretically an interesting application of computer simulation is to simulate computers using computers in computer architecture type of simulator typically called an emulator is often used to execute program that has to run on some inconvenient type of computer for example newly designed computer that has not yet been built or an obsolete computer that is no longer available or in tightly controlled testing environment see computer architecture simulator and platform virtualization for example simulators have been used to debug microprogram or sometimes commercial application programs before the program is downloaded to the target machine since the operation of the computer is simulated all of the information about the computer operation is directly available to the programmer and the speed and execution of the simulation can be varied at will simulators may also be used to interpret fault trees or test vlsi logic designs before they are constructed symbolic simulation uses variables to stand for unknown values in the field of optimization simulations of physical processes are often used in conjunction with evolutionary computation to optimize control strategies simulation in education and training simulation is extensively used for educational purposes it is frequently used by way of adaptive hypermedia simulation is often used in the training of civilian and military personnel this usually occurs when it is prohibitively expensive or simply too dangerous to allow trainees to use the real equipment in the real world in such situations they will spend time learning valuable lessons in safe virtual environment yet living lifelike experience or at least it is the goal often the convenience is to permit mistakes during training for safety critical system there is distinction though between simulations used for training and instructional simulation training simulations typically come in one of three categories live simulation where actual players use genuine systems in real environment virtual simulation where actual players use simulated systems in synthetic environment or constructive simulation where simulated players use simulated systems in synthetic environment constructive simulation is often referred to as wargaming since it bears some resemblance to table top war games in which players command armies of soldiers and equipment that move around board in standardized tests live simulations are sometimes called high fidelity producing samples of likely performance as opposed to low fidelity pencil and paper simulations producing only signs of possible performance but the distinction between high moderate and low fidelity remains relative depending on the context of particular comparison simulations in education are somewhat like training simulations they focus on specific tasks the term microworld is used to refer to educational simulations which model some abstract concept rather than simulating realistic object or environment or in some cases model real world environment in simplistic way so as to help learner develop an understanding of the key concepts normally user can create some sort of construction within the microworld that will behave in way consistent with the concepts being modeled seymour papert was one of the first to advocate the value of microworlds and the logo programming environment developed by papert is one of the most famous microworlds as another example the global challenge award online stem learning web site uses microworld simulations to teach science concepts related to global warming and the future of energy other projects for simulations in educations are open source physics netsim etc project management simulation is increasingly used to train students and professionals in the art and science of project management using simulation for project management training improves learning retention and enhances the learning process social simulations may be used in social science classrooms to illustrate social and political processes in anthropology economics history political science or sociology courses typically at the high school or university level these may for example take the form of civics simulations in which participants assume roles in simulated society or international relations simulations in which participants engage in negotiations alliance formation trade diplomacy and the use of force such simulations might be based on fictitious political systems or be based on current or historical events an example of the latter would be barnard college reacting to the past series of historical educational games the national science foundation has also supported the creation of reacting games that address science and math education in recent years there has been increasing use of social simulations for staff training in aid and development agencies the carana simulation for example was first developed by the united nations development programme and is now used in very revised form by the world bank for training staff to deal with fragile and conflict affected countries common user interaction systems for virtual simulations virtual simulations represent specific category of simulation that utilizes simulation equipment to create simulated world for the user virtual simulations allow users to interact with virtual world virtual worlds operate on platforms of integrated software and hardware components in this manner the system can accept input from the user body tracking voice sound recognition physical controllers and produce output to the user visual display aural display haptic display virtual simulations use the aforementioned modes of interaction to produce sense of immersion for the user virtual simulation input hardware motorcycle simulator of bienal do autom\u00f3vel exhibition in belo horizonte brazil there is wide variety of input hardware available to accept user input for virtual simulations the following list briefly describes several of them body tracking the motion capture method is often used to record the user movements and translate the captured data into inputs for the virtual simulation for example if user physically turns their head the motion would be captured by the simulation hardware in some way and translated to corresponding shift in view within the simulation capture suits and or gloves may be used to capture movements of users body parts the systems may have sensors incorporated inside them to sense movements of different body parts fingers alternatively these systems may have exterior tracking devices or marks that can be detected by external ultrasound optical receivers or electromagnetic sensors internal inertial sensors are also available on some systems the units may transmit data either wirelessly or through cables eye trackers can also be used to detect eye movements so that the system can determine precisely where user is looking at any given instant physical controllers physical controllers provide input to the simulation only through direct manipulation by the user in virtual simulations tactile feedback from physical controllers is highly desirable in number of simulation environments omni directional treadmills can be used to capture the users locomotion as they walk or run high fidelity instrumentation such as instrument panels in virtual aircraft cockpits provides users with actual controls to raise the level of immersion for example pilots can use the actual global positioning system controls from the real device in simulated cockpit to help them practice procedures with the actual device in the context of the integrated cockpit system voice sound recognition this form of interaction may be used either to interact with agents within the simulation virtual people or to manipulate objects in the simulation information voice interaction presumably increases the level of immersion for the user users may use headsets with boom microphones lapel microphones or the room may be equipped with strategically located microphones current research into user input systems research in future input systems hold great deal of promise for virtual simulations systems such as brain computer interfaces bcis brain computer interface offer the ability to further increase the level of immersion for virtual simulation users lee keinrath scherer bischof pfurtscheller proved that na\u00efve subjects could be trained to use bci to navigate virtual apartment with relative ease using the bci the authors found that subjects were able to freely navigate the virtual environment with relatively minimal effort it is possible that these types of systems will become standard input modalities in future virtual simulation systems virtual simulation output hardware there is wide variety of output hardware available to deliver stimulus to users in virtual simulations the following list briefly describes several of them visual display visual displays provide the visual stimulus to the user stationary displays can vary from conventional desktop display to degree wrap around screens to stereo three dimensional screens conventional desktop displays can vary in size from to inches wrap around screens are typically utilized in what is known as cave automatic virtual environment cave cave automatic virtual environment stereo three dimensional screens produce three dimensional images either with or without special glasses depending on the design head mounted displays hmds have small displays that are mounted on headgear worn by the user these systems are connected directly into the virtual simulation to provide the user with more immersive experience weight update rates and field of view are some of the key variables that differentiate hmds naturally heavier hmds are undesirable as they cause fatigue over time if the update rate is too slow the system is unable to update the displays fast enough to correspond with quick head turn by the user slower update rates tend to cause simulation sickness and disrupt the sense of immersion field of view or the angular extent of the world that is seen at given moment field of view can vary from system to system and has been found to affect the users sense of immersion aural display several different types of audio systems exist to help the user hear and localize sounds spatially special software can be used to produce audio effects audio to create the illusion that sound sources are placed within defined three dimensional space around the user stationary conventional speaker systems may be used provide dual or multi channel surround sound however external speakers are not as effective as headphones in producing audio effects conventional headphones offer portable alternative to stationary speakers they also have the added advantages of masking real world noise and facilitate more effective audio sound effects haptic display these displays provide sense of touch to the user haptic technology this type of output is sometimes referred to as force feedback tactile tile displays use different types of actuators such as inflatable bladders vibrators low frequency sub woofers pin actuators and or thermo actuators to produce sensations for the user end effector displays can respond to users inputs with resistance and force these systems are often used in medical applications for remote surgeries that employ robotic instruments vestibular display these displays provide sense of motion to the user motion simulator they often manifest as motion bases for virtual vehicle simulation such as driving simulators or flight simulators motion bases are fixed in place but use actuators to move the simulator in ways that can produce the sensations pitching yawing or rolling the simulators can also move in such way as to produce sense of acceleration on all axes the motion base can produce the sensation of falling clinical healthcare simulators medical simulators are increasingly being developed and deployed to teach therapeutic and diagnostic procedures as well as medical concepts and decision making to personnel in the health professions simulators have been developed for training procedures ranging from the basics such as blood draw to laparoscopic surgery and trauma care they are also important to help on prototyping new devices for biomedical engineering problems currently simulators are applied to research and develop tools for new therapies treatments and early diagnosis in medicine many medical simulators involve computer connected to plastic simulation of the relevant anatomy sophisticated simulators of this type employ life size mannequin that responds to injected drugs and can be programmed to create simulations of life threatening emergencies in other simulations visual components of the procedure are reproduced by computer graphics techniques while touch based components are reproduced by haptic feedback devices combined with physical simulation routines computed in response to the user actions medical simulations of this sort will often use ct or mri scans of patient data to enhance realism some medical simulations are developed to be widely distributed such as web enabled simulations and procedural simulations that can be viewed via standard web browsers and can be interacted with using standard computer interfaces such as the keyboard and mouse another important medical application of simulator although perhaps denoting slightly different meaning of simulator is the use of placebo drug formulation that simulates the active drug in trials of drug efficacy see placebo origins of technical term improving patient safety patient safety is concern in the medical industry patients have been known to suffer injuries and even death due to management error and lack of using best standards of care and training according to building national agenda for simulation based medical education eder van hook jackie health care provider ability to react prudently in an unexpected situation is one of the most critical factors in creating positive outcome in medical emergency regardless of whether it occurs on the battlefield freeway or hospital emergency room simulation eder van hook also noted that medical errors kill up to with an estimated cost between and million and to billion for preventable adverse events dollars per year deaths due to preventable adverse events exceed deaths attributable to motor vehicle accidents breast cancer or aids eder van hook with these types of statistics it is no wonder that improving patient safety is prevalent concern in the industry innovative simulation training solutions are now being used to train medical professionals in an attempt to reduce the number of safety concerns that have adverse effects on the patients however according to the article does simulation improve patient safety self efficacy competence operational performance and patient safety nishisaki keren and nadkarni the jury is still out nishisaki states that there is good evidence that simulation training improves provider and team self efficacy and competence on manikins there is also good evidence that procedural simulation improves actual operational performance in clinical settings however no evidence yet shows that crew resource management training through simulation despite its promise improves team operational performance at the bedside although evidence that simulation based training actually improves patient outcome has been slow to accrue today the ability of simulation to provide hands on experience that translates to the operating room is no longer in doubt one such attempt to improve patient safety through the use of simulations training is pediatric care to deliver just in time service or and just in place this training consists of minutes of simulated training just before workers report to shift it is hoped that the recentness of the training will increase the positive and reduce the negative results that have generally been associated with the procedure the purpose of this study is to determine if just in time training improves patient safety and operational performance of orotracheal intubation and decrease occurrences of undesired associated events and to test the hypothesis that high fidelity simulation may enhance the training efficacy and patient safety in simulation settings the conclusion as reported in abstract just in time simulation training improves icu physician trainee airway resuscitation participation without compromising procedural success or safety nishisaki were that simulation training improved resident participation in real cases but did not sacrifice the quality of service it could be therefore hypothesized that by increasing the number of highly trained residents through the use of simulation training that the simulation training does in fact increase patient safety this hypothesis would have to be researched for validation and the results may or may not generalize to other situations history of simulation in healthcare the first medical simulators were simple models of human patients since antiquity these representations in clay and stone were used to demonstrate clinical features of disease states and their effects on humans models have been found from many cultures and continents these models have been used in some cultures chinese culture as diagnostic instrument allowing women to consult male physicians while maintaining social laws of modesty models are used today to help students learn the anatomy of the musculoskeletal system and organ systems type of models active models active models that attempt to reproduce living anatomy or physiology are recent developments the famous harvey mannequin was developed at the university of miami and is able to recreate many of the physical findings of the cardiology examination including palpation auscultation and interactive models more recently interactive models have been developed that respond to actions taken by student or physician until recently these simulations were two dimensional computer programs that acted more like textbook than patient computer simulations have the advantage of allowing student to make judgments and also to make errors the process of iterative learning through assessment evaluation decision making and error correction creates much stronger learning environment than passive instruction computer simulators diteams learner is percussing the patient chest in virtual field hospital simulators have been proposed as an ideal tool for assessment of students for clinical skills for patients cybertherapy can be used for sessions simulating traumatic experiences from fear of heights to social anxiety programmed patients and simulated clinical situations including mock disaster drills have been used extensively for education and evaluation these lifelike simulations are expensive and lack reproducibility fully functional di simulator would be the most specific tool available for teaching and measurement of clinical skills gaming platforms have been applied to create these virtual medical environments to create an interactive method for learning and application of information in clinical context immersive disease state simulations allow doctor or hcp to experience what disease actually feels like using sensors and transducers symptomatic effects can be delivered to participant allowing them to experience the patients disease state such simulator meets the goals of an objective and standardized examination for clinical competence this system is superior to examinations that use standard patients because it permits the quantitative measurement of competence as well as reproducing the same objective findings simulation in entertainment simulation in entertainment encompasses many large and popular industries such as film television video games including serious games and rides in theme parks although modern simulation is thought to have its roots in training and the military in the th century it also became conduit for enterprises which were more hedonistic in nature advances in technology in the and caused simulation to become more widely used and it began to appear in movies such as jurassic park and in computer based games such as atari battlezone history early history and the first simulation game may have been created as early as by thomas goldsmith jr and estle ray mann this was straightforward game that simulated missile being fired at target the curve of the missile and its speed could be adjusted using several knobs in computer game called tennis for two was created by willy higginbotham which simulated tennis game between two players who could both play at the same time using hand controls and was displayed on an oscilloscope this was one of the first electronic video games to use graphical display modern simulation present advances in technology in the made the computer more affordable and more capable than they were in previous decades which facilitated the rise of computer such as the xbox gaming the first video game consoles released in the and early fell prey to the industry crash in but in nintendo released the nintendo entertainment system nes which became one of the best selling consoles in video game history in the computer games became widely popular with the release of such game as the sims and command conquer and the still increasing power of desktop computers today computer simulation games such as world of warcraft are played by millions of people around the world computer generated imagery was used in film to simulate objects as early as though in the film tron was the first film to use computer generated imagery for more than couple of minutes however the commercial failure of the movie may have caused the industry to step away from the technology in the film jurassic park became the first popular film to use computer generated graphics extensively integrating the simulated dinosaurs almost seamlessly into live action scenes this event transformed the film industry in the film toy story was the first film to use only computer generated images and by the new millennium computer generated graphics were the leading choice for special effects in films simulators have been used for entertainment since the link trainer in the the first modern simulator ride to open at theme park was disney star tours in soon followed by universal the funtastic world of hanna barbera in which was the first ride to be done entirely with computer graphics examples of entertainment simulation computer and video games simulation games as opposed to other genres of video and computer games represent or simulate an environment accurately moreover they represent the interactions between the playable characters and the environment realistically these kinds of games are usually more complex in terms of game play simulation games have become incredibly popular among people of all ages popular simulation games include simcity tiger woods pga tour and virtonomics there are also flight simulation and driving simulation games film computer generated imagery is the application of the field of computer graphics to special effects this technology is used for visual effects because they are high in quality controllable and can create effects that would not be feasible using any other technology either because of cost resources or safety computer generated graphics can be seen in many live action movies today especially those of the action genre further computer generated imagery has almost completely supplanted hand drawn animation in children movies which are increasingly computer generated only examples of movies that use computer generated imagery include finding nemo and iron man theme park rides simulator rides are the progeny of military training simulators and commercial simulators but they are different in fundamental way while military training simulators react realistically to the input of the trainee in real time ride simulators only feel like they move realistically and move according to prerecorded motion scripts one of the first simulator rides star tours which cost millon used hydraulic motion based cabin the movement was programmed by joystick today simulator rides such as the amazing adventures of spider man include elements to increase the amount of immersion experienced by the riders such as imagery physical effects spraying water or producing scents and movement through an environment examples of simulation rides include mission space and the simpsons ride there are many simulation rides at themeparks like disney universal etc examples are flint stones earth quake time machine king kong simulation and manufacturing manufacturing represents one of the most important applications of simulation this technique represents valuable tool used by engineers when evaluating the effect of capital investment in equipments and physical facilities like factory plants warehouses and distribution centers simulation can be used to predict the performance of an existing or planned system and to compare alternative solutions for particular design problem another important goal of manufacturing simulations is to quantify system performance common measures of system performance include the following throughput under average and peak loads system cycle time how long it take to produce one part utilization of resource labor and machines bottlenecks and choke points queuing at work locations queuing and delays caused by material handling devices and systems wip storages needs staffing requirements effectiveness of scheduling systems effectiveness of control systems more examples of simulation automobiles car racing simulator soldier tests out heavy wheeled vehicle driver simulator an automobile simulator provides an opportunity to reproduce the characteristics of real vehicles in virtual environment it replicates the external factors and conditions with which vehicle interacts enabling driver to feel as if they are sitting in the cab of their own vehicle scenarios and events are replicated with sufficient reality to ensure that drivers become fully immersed in the experience rather than simply viewing it as an educational experience the simulator provides constructive experience for the novice driver and enables more complex exercises to be undertaken by the more mature driver for novice drivers truck simulators provide an opportunity to begin their career by applying best practice for mature drivers simulation provides the ability to enhance good driving or to detect poor practice and to suggest the necessary steps for remedial action for companies it provides an opportunity to educate staff in the driving skills that achieve reduced maintenance costs improved productivity and most importantly to ensure the safety of their actions in all possible situations biomechanics an open source simulation platform for creating dynamic mechanical models built from combinations of rigid and deformable bodies joints constraints and various force actuators it is specialized for creating biomechanical models of human anatomical structures with the intention to study their function and eventually assist in the design and planning of medical treatment biomechanics simulator is used to analyze walking dynamics study sports performance simulate surgical procedures analyze joint loads design medical devices and animate human and animal movement neuromechanical simulator that combines biomechanical and biologically realistic neural network simulation it allows the user to test hypotheses on the neural basis of behavior in physically accurate virtual environment city and urban city simulator can be city building game but can also be tool used by urban planners to understand how cities are likely to evolve in response to various policy decisions anylogic is an example of modern large scale urban simulators designed for use by urban planners city simulators are generally agent based simulations with explicit representations for land use and transportation urbansim and leam are examples of large scale urban simulation models that are used by metropolitan planning agencies and military bases for land use and transportation planning classroom of the future the classroom of the future will probably contain several kinds of simulators in addition to textual and visual learning tools this will allow students to enter the clinical years better prepared and with higher skill level the advanced student or postgraduate will have more concise and comprehensive method of retraining or of incorporating new clinical procedures into their skill set and regulatory bodies and medical institutions will find it easier to assess the proficiency and competency of individuals the classroom of the future will also form the basis of clinical skills unit for continuing education of medical personnel and in the same way that the use of periodic flight training assists airline pilots this technology will assist practitioners throughout their career the simulator will be more than living textbook it will become an integral part of the practice of medicine the simulator environment will also provide standard platform for curriculum development in institutions of medical education communication satellites modern satellite communications systems satcom are often large and complex with many interacting parts and elements in addition the need for broadband connectivity on moving vehicle has increased dramatically in the past few years for both commercial and military applications to accurately predict and deliver high quality of service satcom system designers have to factor in terrain as well as atmospheric and meteorological conditions in their planning to deal with such complexity system designers and operators increasingly turn towards computer models of their systems to simulate real world operational conditions and gain insights into usability and requirements prior to final product sign off modeling improves the understanding of the system by enabling the satcom system designer or planner to simulate real world performance by injecting the models with multiple hypothetical atmospheric and environmental conditions simulation is often used in the training of civilian and military personnel this usually occurs when it is prohibitively expensive or simply too dangerous to allow trainees to use the real equipment in the real world in such situations they will spend time learning valuable lessons in safe virtual environment yet living lifelike experience or at least it is the goal often the convenience is to permit mistakes during training for safety critical system digital lifecycle simulation of airflow over an engine simulation solutions are being increasingly integrated with cax cad cam cae solutions and processes the use of simulation throughout the product lifecycle especially at the earlier concept and design stages has the potential of providing substantial benefits these benefits range from direct cost issues such as reduced prototyping and shorter time to market to better performing products and higher margins however for some companies simulation has not provided the expected benefits the research firm aberdeen group has found that nearly all best in class manufacturers use simulation early in the design process as compared to or laggards who do not the successful use of simulation early in the lifecycle has been largely driven by increased integration of simulation tools with the entire cad cam and plm solution set simulation solutions can now function across the extended enterprise in multi cad environment and include solutions for managing simulation data and processes and ensuring that simulation results are made part of the product lifecycle history the ability to use simulation across the entire lifecycle has been enhanced through improved user interfaces such as tailorable user interfaces and wizards which allow all appropriate plm participants to take part in the simulation process disaster preparedness simulation training has become method for preparing people for disasters simulations can replicate emergency situations and track how learners respond thanks to lifelike experience disaster preparedness simulations can involve training on how to handle terrorism attacks natural disasters pandemic outbreaks or other life threatening emergencies one organization that has used simulation training for disaster preparedness is cade center for advancement of distance education cade has used video game to prepare emergency workers for multiple types of attacks as reported by news medical net the video game is the first in series of simulations to address bioterrorism pandemic flu smallpox and other disasters that emergency personnel must prepare for developed by team from the university of illinois at chicago uic the game allows learners to practice their emergency skills in safe controlled environment the emergency simulation program esp at the british columbia institute of technology bcit vancouver british columbia canada is another example of an organization that uses simulation to train for emergency situations esp uses simulation to train on the following situations forest fire fighting oil or chemical spill response earthquake response law enforcement municipal fire fighting hazardous material handling military training and response to terrorist attack one feature of the simulation system is the implementation of dynamic run time clock which allows simulations to run simulated time frame speeding up or slowing down time as desired additionally the system allows session recordings picture icon based navigation file storage of individual simulations multimedia components and launch external applications at the university of qu\u00e9bec in chicoutimi research team at the outdoor research and expertise laboratory laboratoire expertise et de recherche en plein air lerpa specializes in using wilderness backcountry accident simulations to verify emergency response coordination instructionally the benefits of emergency training through simulations are that learner performance can be tracked through the system this allows the developer to make adjustments as necessary or alert the educator on topics that may require additional attention other advantages are that the learner can be guided or trained on how to respond appropriately before continuing to the next emergency segment this is an aspect that may not be available in the live environment some emergency training simulators also allows for immediate feedback while other simulations may provide summary and instruct the learner to engage in the learning topic again in live emergency situation emergency responders do not have time to waste simulation training in this environment provides an opportunity for learners to gather as much information as they can and practice their knowledge in safe environment they can make mistakes without risk of endangering lives and be given the opportunity to correct their errors to prepare for the real life emergency economics in economics and especially macroeconomics the effects of proposed policy actions such as fiscal policy changes or monetary policy changes are simulated to judge their desirability mathematical model of the economy having been fitted to historical economic data is used as proxy for the actual economy proposed values of government spending taxation open market operations etc are used as inputs to the simulation of the model and various variables of interest such as the inflation rate the unemployment rate the balance of trade deficit the government budget deficit etc are the outputs of the simulation the simulated values of these variables of interest are compared for different proposed policy inputs to determine which set of outcomes is most desirable engineering technology and processes simulation is an important feature in engineering systems or any system that involves many processes for example in electrical engineering delay lines may be used to simulate propagation delay and phase shift caused by an actual transmission line similarly dummy loads may be used to simulate impedance without simulating propagation and is used in situations where propagation is unwanted simulator may imitate only few of the operations and functions of the unit it simulates contrast with emulate most engineering simulations entail mathematical modeling and computer assisted investigation there are many cases however where mathematical modeling is not reliable simulation of fluid dynamics problems often require both mathematical and physical simulations in these cases the physical models require dynamic similitude physical and chemical simulations have also direct realistic uses rather than research uses in chemical engineering for example process simulations are used to give the process parameters immediately used for operating chemical plants such as oil refineries simulators are also used for plant operator training it is called operator training simulator ots and has been widely adopted by many industries from chemical to oil gas and to power industry this created safe and realistic virtual environment to train board operators and engineers mimic is capable of providing high fidelity dynamic models of nearly all chemical plants for operator training and control system testing equipment due to the dangerous and expensive nature of training on heavy equipment simulation has become common solution across many industries types of simulated equipment include cranes mining reclaimers and construction equipment among many others often the simulation units will include pre built scenarios by which to teach trainees as well as the ability to customize new scenarios such equipment simulators are intended to create safe and cost effective alternative to training on live equipment ergonomics ergonomic simulation involves the analysis of virtual products or manual tasks within virtual environment in the engineering process the aim of ergonomics is to develop and to improve the design of products and work environments ergonomic simulation utilizes an anthropometric virtual representation of the human commonly referenced as mannequin or digital human models dhms to mimic the postures mechanical loads and performance of human operator in simulated environment such as an airplane automobile or manufacturing facility dhms are recognized as evolving and valuable tool for performing proactive ergonomics analysis and design the simulations employ graphics and physics based models to animate the virtual humans ergonomics software uses inverse kinematics ik capability for posing the dhms several ergonomic simulation tools have been developed including jack safework ramsis and sammie the software tools typically calculate biomechanical properties including individual muscle forces joint forces and moments most of these tools employ standard ergonomic evaluation methods such as the niosh lifting equation and rapid upper limb assessment rula some simulations also analyze physiological measures including metabolism energy expenditure and fatigue limits cycle time studies design and process validation user comfort reachability and line of sight are other human factors that may be examined in ergonomic simulation packages modeling and simulation of task can be performed by manually manipulating the virtual human in the simulated environment some ergonomics simulation software permits interactive real time simulation and evaluation through actual human input via motion capture technologies however motion capture for ergonomics requires expensive equipment and the creation of props to represent the environment or product some applications of ergonomic simulation in include analysis of solid waste collection disaster management tasks interactive gaming automotive assembly line virtual prototyping of rehabilitation aids and aerospace product design ford engineers use ergonomics simulation software to perform virtual product design reviews using engineering data the simulations assist evaluation of assembly ergonomics the company uses siemen jack and jill ergonomics simulation software in improving worker safety and efficiency without the need to build expensive prototypes finance in finance computer simulations are often used for scenario planning risk adjusted net present value for example is computed from well defined but not always known or fixed inputs by imitating the performance of the project under evaluation simulation can provide distribution of npv over range of discount rates and other variables simulations are frequently used in financial training to engage participants in experiencing various historical as well as fictional situations there are stock market simulations portfolio simulations risk management simulations or models and forex simulations such simulations are typically based on stochastic asset models using these simulations in training program allows for the application of theory into something akin to real life as with other industries the use of simulations can be technology or case study driven flight flight simulation training devices fstd are used to train pilots on the ground in comparison to training in an actual aircraft simulation based training allows for the training of maneuvers or situations that may be impractical or even dangerous to perform in the aircraft while keeping the pilot and instructor in relatively low risk environment on the ground for example electrical system failures instrument failures hydraulic system failures and even flight control failures can be simulated without risk to the pilots or an aircraft instructors can also provide students with higher concentration of training tasks in given period of time than is usually possible in the aircraft for example conducting multiple instrument approaches in the actual aircraft may require significant time spent repositioning the aircraft while in simulation as soon as one approach has been completed the instructor can immediately preposition the simulated aircraft to an ideal or less than ideal location from which to begin the next approach flight simulation also provides an economic advantage over training in an actual aircraft once fuel maintenance and insurance costs are taken into account the operating costs of an fstd are usually substantially lower than the operating costs of the simulated aircraft for some large transport category airplanes the operating costs may be several times lower for the fstd than the actual aircraft some people who use simulator software especially flight simulator software build their own simulator at home some people to further the realism of their homemade simulator buy used cards and racks that run the same software used by the original machine while this involves solving the problem of matching hardware and software and the problem that hundreds of cards plug into many different racks many still find that solving these problems is well worthwhile some are so serious about realistic simulation that they will buy real aircraft parts like complete nose sections of written off aircraft at aircraft boneyards this permits people to simulate hobby that they are unable to pursue in real life marine bearing resemblance to flight simulators marine simulators train ships personnel the most common marine simulators include ship bridge simulators engine room simulators cargo handling simulators communication gmdss simulators rov simulators simulators like these are mostly used within maritime colleges training institutions and navies they often consist of replication of ships bridge with operating console and number of screens on which the virtual surroundings are projected military military simulations also known informally as war games are models in which theories of warfare can be tested and refined without the need for actual hostilities they exist in many different forms with varying degrees of realism in recent times their scope has widened to include not only military but also political and social factors for example the nationlab series of strategic exercises in latin america while many governments make use of simulation both individually and collaboratively little is known about the model specifics outside professional circles payment and securities settlement system simulation techniques have also been applied to payment and securities settlement systems among the main users are central banks who are generally responsible for the oversight of market infrastructure and entitled to contribute to the smooth functioning of the payment systems central banks have been using payment system simulations to evaluate things such as the adequacy or sufficiency of liquidity available in the form of account balances and intraday credit limits to participants mainly banks to allow efficient settlement of payments the need for liquidity is also dependent on the availability and the type of netting procedures in the systems thus some of the studies have focus on system comparisons another application is to evaluate risks related to events such as communication network breakdowns or the inability of participants to send payments in case of possible bank failure this kind of analysis falls under the concepts of stress testing or scenario analysis common way to conduct these simulations is to replicate the settlement logics of the real payment or securities settlement systems under analysis and then use real observed payment data in case of system comparison or system development naturally also the other settlement logics need to be implemented to perform stress testing and scenario analysis the observed data needs to be altered some payments delayed or removed to analyze the levels of liquidity initial liquidity levels are varried system comparisons benchmarking or evaluations of new netting algorithms or rules are performed by running simulations with fixed set of data and varying only the system setups inference is usually done by comparing the benchmark simulation results to the results of altered simulation setups by comparing indicators such as unsettled transactions or settlement delays project management project management simulation is simulation used for project management training and analysis it is often used as training simulation for project managers in other cases it is used for what if analysis and for supporting decision making in real projects frequently the simulation is conducted using software tools robotics robotics simulator is used to create embedded applications for specific or not robot without being dependent on the real robot in some cases these applications can be transferred to the real robot or rebuilt without modifications robotics simulators allow reproducing situations that cannot be created in the real world because of cost time or the uniqueness of resource simulator also allows fast robot prototyping many robot simulators feature physics engines to simulate robot dynamics production simulations of production systems is used mainly to examine the effect of improvements or investments in production system most often this is done using static spreadsheet with process times and transportation times for more sophisticated simulations discrete event simulation des is used with the advantages to simulate dynamics in the production system production system is very much dynamic depending on variations in manufacturing processes assembly times machine set ups breaks breakdowns and small stoppages there are lots of software commonly used for discrete event simulation they differ in usability and markets but do often share the same foundation sales process simulations are useful in modeling the flow of transactions through business processes such as in the field of sales process engineering to study and improve the flow of customer orders through various stages of completion say from an initial proposal for providing goods services through order acceptance and installation such simulations can help predict the impact of how improvements in methods might impact variability cost labor time and the quantity of transactions at various stages in the process full featured computerized process simulator can be used to depict such models as can simpler educational demonstrations using spreadsheet software pennies being transferred between cups based on the roll of die or dipping into tub of colored beads with scoop sports in sports computer simulations are often done to predict the outcome of events and the performance of individual sportspeople they attempt to recreate the event through models built from statistics the increase in technology has allowed anyone with knowledge of programming the ability to run simulations of their models the simulations are built from series of mathematical algorithms or models and can vary with accuracy accuscore which is licensed by companies such as espn is well known simulation program for all major sports it offers detailed analysis of games through simulated betting lines projected point totals and overall probabilities with the increased interest in fantasy sports simulation models that predict individual player performance have gained popularity companies like what if sports and statfox specialize in not only using their simulations for predicting game results but how well individual players will do as well many people use models to determine who to start in their fantasy leagues another way simulations are helping the sports field is in the use of biomechanics models are derived and simulations are run from data received from sensors attached to athletes and video equipment sports biomechanics aided by simulation models answer questions regarding training techniques such as the effect of fatigue on throwing performance height of throw and biomechanical factors of the upper limbs reactive strength index hand contact time computer simulations allow their users to take models which before were too complex to run and give them answers simulations have proven to be some of the best insights into both play performance and team predictability space shuttle countdown firing room configured for space shuttle launches simulation is used at kennedy space center ksc to train and certify space shuttle engineers during simulated launch countdown operations the space shuttle engineering community participates in launch countdown integrated simulation before each shuttle flight this simulation is virtual simulation where real people interact with simulated space shuttle vehicle and ground support equipment gse hardware the shuttle final countdown phase simulation also known as involves countdown processes that integrate many of the space shuttle vehicle and gse systems some of the shuttle systems integrated in the simulation are the main propulsion system main engines solid rocket boosters ground liquid hydrogen and liquid oxygen external tank flight controls navigation and avionics the high level objectives of the shuttle final countdown phase simulation are to demonstrate firing room final countdown phase operations to provide training for system engineers in recognizing reporting and evaluating system problems in time critical environment to exercise the launch teams ability to evaluate prioritize and respond to problems in an integrated manner within time critical environment to provide procedures to be used in performing failure recovery testing of the operations performed in the final countdown phase the shuttle final countdown phase simulation takes place at the kennedy space center launch control center firing rooms the firing room used during the simulation is the same control room where real launch countdown operations are executed as result equipment used for real launch countdown operations is engaged command and control computers application software engineering plotting and trending tools launch countdown procedure documents launch commit criteria documents hardware requirement documents and any other items used by the engineering launch countdown teams during real launch countdown operations are used during the simulation the space shuttle vehicle hardware and related gse hardware is simulated by mathematical models written in shuttle ground operations simulator sgos modeling language that behave and react like real hardware during the shuttle final countdown phase simulation engineers command and control hardware via real application software executing in the control consoles just as if they were commanding real vehicle hardware however these real software applications do not interface with real shuttle hardware during simulations instead the applications interface with mathematical model representations of the vehicle and gse hardware consequently the simulations bypass sensitive and even dangerous mechanisms while providing engineering measurements detailing how the hardware would have reacted since these math models interact with the command and control application software models and simulations are also used to debug and verify the functionality of application software satellite navigation the only true way to test gnss receivers commonly known as sat nav in the commercial world is by using an rf constellation simulator receiver that may for example be used on an aircraft can be tested under dynamic conditions without the need to take it on real flight the test conditions can be repeated exactly and there is full control over all the test parameters this is not possible in the real world using the actual signals for testing receivers that will use the new galileo satellite navigation there is no alternative as the real signals do not yet exist weather predicting weather conditions by extrapolating interpolating previous data is one of the real use of simulation most of the weather forecasts use this information published by weather buereaus this kind of simulations help in predicting and forewarning about extreme weather conditions like the path of an active hurricane cyclone numerical weather prediction for forecasting involves complicated numeric computer models to predict weather accurately by taking many parameters into account simulation games strategy games both traditional and modern may be viewed as simulations of abstracted decision making for the purpose of training military and political leaders see history of go for an example of such tradition or kriegsspiel for more recent example many other video games are simulators of some kind such games can simulate various aspects of reality from business to government to construction to piloting vehicles see above historical usage historically the word had negative connotations however the connection between simulation and dissembling later faded out and is now only of linguistic interest see also references external links bibliographies containing more references to be found on the website of the journal simulation gaming", "Artificial intelligence": "artificial intelligence ai is the intelligence exhibited by machines or software it is also the name of the academic field of study which studies how to create computers and computer software that are capable of intelligent behavior major ai researchers and textbooks define this field as the study and design of intelligent agents in which an intelligent agent is system that perceives its environment and takes actions that maximize its chances of success john mccarthy who coined the term in defines it as the science and engineering of making intelligent machines ai research is highly technical and specialized and is deeply divided into subfields that often fail to communicate with each other some of the division is due to social and cultural factors subfields have grown up around particular institutions and the work of individual researchers ai research is also divided by several technical issues some subfields focus on the solution of specific problems others focus on one of several possible approaches or on the use of particular tool or towards the accomplishment of particular applications the central problems or goals of ai research include reasoning knowledge planning learning natural language processing communication perception and the ability to move and manipulate objects general intelligence is still among the field long term goals currently popular approaches include statistical methods computational intelligence and traditional symbolic ai there are large number of tools used in ai including versions of search and mathematical optimization logic methods based on probability and economics and many others the ai field is in which number of sciences and professions converge including computer science mathematics psychology linguistics philosophy and neuroscience as well as other specialized fields such as artificial psychology the field was founded on the claim that central property of humans human intelligence the sapience of homo sapiens can be so precisely described that machine can be made to simulate it this raises philosophical issues about the nature of the mind and the ethics of creating artificial beings endowed with human like intelligence issues which have been addressed by myth fiction and philosophy since antiquity artificial intelligence has been the subject of tremendous optimism but has also suffered stunning setbacks today it has become an essential part of the technology industry providing the heavy lifting for many of the most challenging problems in computer science history thinking machines and artificial beings appear in greek myths such as talos of crete the bronze robot of hephaestus and pygmalion galatea human likenesses believed to have intelligence were built in every major civilization animated cult images were worshiped in egypt and greece and humanoid automatons were built by yan shi hero of alexandria and al jazari it was also widely believed that artificial beings had been created by j\u0101bir ibn hayy\u0101n judah loew and paracelsus by the th and th centuries artificial beings had become common feature in fiction as in mary shelley frankenstein or karel \u010dapek rossum universal robots pamela mccorduck argues that all of these are some examples of an ancient urge as she describes it to forge the gods stories of these creatures and their fates discuss many of the same hopes fears and ethical concerns that are presented by artificial intelligence mechanical or formal reasoning has been developed by philosophers and mathematicians since antiquity the study of logic led directly to the invention of the programmable digital electronic computer based on the work of mathematician alan turing and others turing theory of computation suggested that machine by shuffling symbols as simple as and could simulate any conceivable act of mathematical deduction this along with concurrent discoveries in neurology information theory and cybernetics inspired small group of researchers to begin to seriously consider the possibility of building an electronic brain the field of ai research was founded at conference on the campus of dartmouth college in the summer of the attendees including john mccarthy marvin minsky allen newell arthur samuel and herbert simon became the leaders of ai research for many decades they and their students wrote programs that were to most people simply astonishing computers were winning at checkers solving word problems in algebra proving logical theorems and speaking english by the middle of the research in the was heavily funded by the department of defense and laboratories had been established around the world ai founders were profoundly optimistic about the future of the new field herbert simon predicted that machines will be capable within twenty years of doing any work man can do and marvin minsky agreed writing that within generation the problem of creating artificial intelligence will substantially be solved they had failed to recognize the difficulty of some of the problems they faced in in response to the criticism of sir james lighthill and ongoing pressure from the us congress to fund more productive projects both the and british governments cut off all undirected exploratory research in ai the next few years would later be called an ai winter period when funding for ai projects was hard to find in the early ai research was revived by the commercial success of expert systems form of ai program that simulated the knowledge and analytical skills of one or more human experts by the market for ai had reached over billion dollars at the same time japan fifth generation computer project inspired the and british governments to restore funding for academic research in the field however beginning with the collapse of the lisp machine market in ai once again fell into disrepute and second longer lasting ai winter began in the and early st century ai achieved its greatest successes albeit somewhat behind the scenes artificial intelligence is used for logistics data mining medical diagnosis and many other areas throughout the technology industry the success was due to several factors the increasing computational power of computers see moore law greater emphasis on solving specific subproblems the creation of new ties between ai and other fields working on similar problems and new commitment by researchers to solid mathematical methods and rigorous scientific standards on may deep blue became the first computer chess playing system to beat reigning world chess champion garry kasparov in february in jeopardy quiz show exhibition match ibm question answering system watson defeated the two greatest jeopardy champions brad rutter and ken jennings by significant margin the kinect which provides body motion interface for the xbox and the xbox one uses algorithms that emerged from lengthy ai research as do intelligent personal assistants in smartphones research goals the general problem of simulating or creating intelligence has been broken down into number of specific sub problems these consist of particular traits or capabilities that researchers would like an intelligent system to display the traits described below have received the most attention deduction reasoning problem solving early ai researchers developed algorithms that imitated the step by step reasoning that humans use when they solve puzzles or make logical deductions by the late and ai research had also developed highly successful methods for dealing with uncertain or incomplete information employing concepts from probability and economics for difficult problems most of these algorithms can require enormous computational resources most experience combinatorial explosion the amount of memory or computer time required becomes astronomical when the problem goes beyond certain size the search for more efficient problem solving algorithms is high priority for ai research human beings solve most of their problems using fast intuitive judgements rather than the conscious step by step deduction that early ai research was able to model ai has made some progress at imitating this kind of sub symbolic problem solving embodied agent approaches emphasize the importance of sensorimotor skills to higher reasoning neural net research attempts to simulate the structures inside the brain that give rise to this skill statistical approaches to ai mimic the probabilistic nature of the human ability to guess knowledge representation an ontology represents knowledge as set of concepts within domain and the relationships between those concepts knowledge representation and knowledge engineering are central to ai research many of the problems machines are expected to solve will require extensive knowledge about the world among the things that ai needs to represent are objects properties categories and relations between objects situations events states and time causes and effects knowledge about knowledge what we know about what other people know and many other less well researched domains representation of what exists is an ontology the set of objects relations concepts and so on that the machine knows about the most general are called upper ontologies which attempt to provide foundation for all other knowledge among the most difficult problems in knowledge representation are default reasoning and the qualification problem many of the things people know take the form of working assumptions for example if bird comes up in conversation people typically picture an animal that is fist sized sings and flies none of these things are true about all birds john mccarthy identified this problem in as the qualification problem for any commonsense rule that ai researchers care to represent there tend to be huge number of exceptions almost nothing is simply true or false in the way that abstract logic requires ai research has explored number of solutions to this problem the breadth of commonsense knowledge the number of atomic facts that the average person knows is astronomical research projects that attempt to build complete knowledge base of commonsense knowledge cyc require enormous amounts of laborious ontological engineering they must be built by hand one complicated concept at time major goal is to have the computer understand enough concepts to be able to learn by reading from sources like the internet and thus be able to add to its own ontology the subsymbolic form of some commonsense knowledge much of what people know is not represented as facts or statements that they could express verbally for example chess master will avoid particular chess position because it feels too exposed or an art critic can take one look at statue and instantly realize that it is fake these are intuitions or tendencies that are represented in the brain non consciously and sub symbolically knowledge like this informs supports and provides context for symbolic conscious knowledge as with the related problem of sub symbolic reasoning it is hoped that situated ai computational intelligence or statistical ai will provide ways to represent this kind of knowledge planning hierarchical control system is form of control system in which set of devices and governing software is arranged in hierarchy intelligent agents must be able to set goals and achieve them they need way to visualize the future they must have representation of the state of the world and be able to make predictions about how their actions will change it and be able to make choices that maximize the utility or value of the available choices in classical planning problems the agent can assume that it is the only thing acting on the world and it can be certain what the consequences of its actions may be however if the agent is not the only actor it must periodically ascertain whether the world matches its predictions and it must change its plan as this becomes necessary requiring the agent to reason under uncertainty multi agent planning uses the cooperation and competition of many agents to achieve given goal emergent behavior such as this is used by evolutionary algorithms and swarm intelligence learning machine learning is the study of computer algorithms that improve automatically through experience and has been central to ai research since the field inception unsupervised learning is the ability to find patterns in stream of input supervised learning includes both classification and numerical regression classification is used to determine what category something belongs in after seeing number of examples of things from several categories regression is the attempt to produce function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change in reinforcement learning the agent is rewarded for good responses and punished for bad ones the agent uses this sequence of rewards and punishments to form strategy for operating in its problem space these three types of learning can be analyzed in terms of decision theory using concepts like utility the mathematical analysis of machine learning algorithms and their performance is branch of theoretical computer science known as computational learning theory within developmental robotics developmental learning approaches were elaborated for lifelong cumulative acquisition of repertoires of novel skills by robot through autonomous self exploration and social interaction with human teachers and using guidance mechanisms such as active learning maturation motor synergies and imitation natural language processing communication parse tree represents the syntactic structure of sentence according to some formal grammar natural language processing gives machines the ability to read and understand the languages that humans speak sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human written sources such as newswire texts some straightforward applications of natural language processing include information retrieval or text mining question answering and machine translation common method of processing and extracting meaning from natural language is through semantic indexing increases in processing speeds and the drop in the cost of data storage makes indexing large volumes of abstractions of the user input much more efficient perception machine perception is the ability to use input from sensors such as cameras microphones tactile sensors sonar and others more exotic to deduce aspects of the world computer vision is the ability to analyze visual input few selected subproblems are speech recognition facial recognition and object recognition motion and manipulation the field of robotics is closely related to ai intelligence is required for robots to be able to handle such tasks as object manipulation and navigation with sub problems of localization knowing where you are or finding out where other things are mapping learning what is around you building map of the environment and motion planning figuring out how to get there or path planning going from one point in space to another point which may involve compliant motion where the robot moves while maintaining physical contact with an object long term goals among the long term goals in the research pertaining to artificial intelligence are social intelligence creativity and general intelligence social intelligence kismet robot with rudimentary social skills affective computing is the study and development of systems and devices that can recognize interpret process and simulate human affects it is an field spanning computer sciences psychology and cognitive science while the origins of the field may be traced as far back as to early philosophical inquiries into emotion the more modern branch of computer science originated with rosalind picard paper on affective computing motivation for the research is the ability to simulate empathy the machine should interpret the emotional state of humans and adapt its behaviour to them giving an appropriate response for those emotions emotion and social skills play two roles for an intelligent agent first it must be able to predict the actions of others by understanding their motives and emotional states this involves elements of game theory decision theory as well as the ability to model human emotions and the perceptual skills to detect emotions also in an effort to facilitate human computer interaction an intelligent machine might want to be able to display emotions even if it does not actually experience them itself in order to appear sensitive to the emotional dynamics of human interaction creativity sub field of ai addresses creativity both theoretically from philosophical and psychological perspective and practically via specific implementations of systems that generate outputs that can be considered creative or systems that identify and assess creativity related areas of computational research are artificial intuition and artificial thinking general intelligence many researchers think that their work will eventually be incorporated into machine with general intelligence known as strong ai combining all the skills above and exceeding human abilities at most or all of them few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such project many of the problems above may require general intelligence to be considered solved for example even straightforward specific task like machine translation requires that the machine read and write in both languages nlp follow the author argument reason know what is being talked about knowledge and faithfully reproduce the author intention social intelligence problem like machine translation is considered ai complete in order to solve this particular problem you must solve all the problems approaches there is no established unifying theory or paradigm that guides ai research researchers disagree about many issues few of the most long standing questions that have remained unanswered are these should artificial intelligence simulate natural intelligence by studying psychology or neurology or is human biology as irrelevant to ai research as bird biology is to aeronautical engineering can intelligent behavior be described using simple elegant principles such as logic or optimization or does it necessarily require solving large number of completely unrelated problems can intelligence be reproduced using high level symbols similar to words and ideas or does it require sub symbolic processing john haugeland who coined the term gofai good old fashioned artificial intelligence also proposed that ai should more properly be referred to as synthetic intelligence term which has since been adopted by some non gofai researchers cybernetics and brain simulation in the and number of researchers explored the connection between neurology information theory and cybernetics some of them built machines that used electronic networks to exhibit rudimentary intelligence such as grey walter turtles and the johns hopkins beast many of these researchers gathered for meetings of the teleological society at princeton university and the ratio club in england by this approach was largely abandoned although elements of it would be revived in the symbolic when access to digital computers became possible in the middle ai research began to explore the possibility that human intelligence could be reduced to symbol manipulation the research was centered in three institutions carnegie mellon university stanford and mit and each one developed its own style of research john haugeland named these approaches to ai good old fashioned ai or gofai during the symbolic approaches had achieved great success at simulating high level thinking in small demonstration programs approaches based on cybernetics or neural networks were abandoned or pushed into the background researchers in the and the were convinced that symbolic approaches would eventually succeed in creating machine with artificial general intelligence and considered this the goal of their field cognitive simulation economist herbert simon and allen newell studied human problem solving skills and attempted to formalize them and their work laid the foundations of the field of artificial intelligence as well as cognitive science operations research and management science their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems this tradition centered at carnegie mellon university would eventually culminate in the development of the soar architecture in the middle logic based unlike newell and simon john mccarthy felt that machines did not need to simulate human thought but should instead try to find the essence of abstract reasoning and problem solving regardless of whether people used the same algorithms his laboratory at stanford sail focused on using formal logic to solve wide variety of problems including knowledge representation planning and learning logic was also the focus of the work at the university of edinburgh and elsewhere in europe which led to the development of the programming language prolog and the science of logic programming anti logic or scruffy researchers at mit such as marvin minsky and seymour papert found that solving difficult problems in vision and natural language processing required ad hoc solutions they argued that there was no simple and general principle like logic that would capture all the aspects of intelligent behavior roger schank described their anti logic approaches as scruffy as opposed to the neat paradigms at cmu and stanford commonsense knowledge bases such as doug lenat cyc are an example of scruffy ai since they must be built by hand one complicated concept at time knowledge based when computers with large memories became available around researchers from all three traditions began to build knowledge into ai applications this knowledge revolution led to the development and deployment of expert systems introduced by edward feigenbaum the first truly successful form of ai software the knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple ai applications sub symbolic by the progress in symbolic ai seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition especially perception robotics learning and pattern recognition number of researchers began to look into sub symbolic approaches to specific ai problems bottom up embodied situated behavior based or nouvelle ai researchers from the related field of robotics such as rodney brooks rejected symbolic ai and focused on the basic engineering problems that would allow robots to move and survive their work revived the non symbolic viewpoint of the early cybernetics researchers of the and reintroduced the use of control theory in ai this coincided with the development of the embodied mind thesis in the related field of cognitive science the idea that aspects of the body such as movement perception and visualization are required for higher intelligence computational intelligence and soft computing interest in neural networks and connectionism was revived by david rumelhart and others in the middle neural networks are an example of soft computing they are solutions to problems which cannot be solved with complete logical certainty and where an approximate solution is often enough other soft computing approaches to ai include fuzzy systems evolutionary computation and many statistical tools the application of soft computing to ai is studied collectively by the emerging discipline of computational intelligence statistical in the ai researchers developed sophisticated mathematical tools to solve specific subproblems these tools are truly scientific in the sense that their results are both measurable and verifiable and they have been responsible for many of ai recent successes the shared mathematical language has also permitted high level of collaboration with more established fields like mathematics economics or operations research stuart russell and peter norvig describe this movement as nothing less than revolution and the victory of the neats critics argue that these techniques with few exceptions are too focused on particular problems and have failed to address the long term goal of general intelligence there is an ongoing debate about the relevance and validity of statistical approaches in ai exemplified in part by exchanges between peter norvig and noam chomsky integrating the approaches intelligent agent paradigm an intelligent agent is system that perceives its environment and takes actions which maximize its chances of success the simplest intelligent agents are programs that solve specific problems more complicated agents include human beings and organizations of human beings such as firms the paradigm gives researchers license to study isolated problems and find solutions that are both verifiable and useful without agreeing on one single approach an agent that solves specific problem can use any approach that works some agents are symbolic and logical some are sub symbolic neural networks and others may use new approaches the paradigm also gives researchers common language to communicate with other fields such as decision theory and economics that also use concepts of abstract agents the intelligent agent paradigm became widely accepted during the agent architectures and cognitive architectures researchers have designed systems to build intelligent systems out of interacting intelligent agents in multi agent system system with both symbolic and sub symbolic components is hybrid intelligent system and the study of such systems is artificial intelligence systems integration hierarchical control system provides bridge between sub symbolic ai at its lowest reactive levels and traditional symbolic ai at its highest levels where relaxed time constraints permit planning and world modelling rodney brooks subsumption architecture was an early proposal for such hierarchical system tools in the course of years of research ai has developed large number of tools to solve the most difficult problems in computer science few of the most general of these methods are discussed below search and optimization many problems in ai can be solved in theory by intelligently searching through many possible solutions reasoning can be reduced to performing search for example logical proof can be viewed as searching for path that leads from premises to conclusions where each step is the application of an inference rule planning algorithms search through trees of goals and subgoals attempting to find path to target goal process called means ends analysis robotics algorithms for moving limbs and grasping objects use local searches in configuration space many learning algorithms use search algorithms based on optimization simple exhaustive searches are rarely sufficient for most real world problems the search space the number of places to search quickly grows to astronomical numbers the result is search that is too slow or never completes the solution for many problems is to use heuristics or rules of thumb that eliminate choices that are unlikely to lead to the goal called pruning the search tree heuristics supply the program with best guess for the path on which the solution lies heuristics limit the search for solutions into smaller sample size very different kind of search came to prominence in the based on the mathematical theory of optimization for many problems it is possible to begin the search with some form of guess and then refine the guess incrementally until no more refinements can be made these algorithms can be visualized as blind hill climbing we begin the search at random point on the landscape and then by jumps or steps we keep moving our guess uphill until we reach the top other optimization algorithms are simulated annealing beam search and random optimization evolutionary computation uses form of optimization search for example they may begin with population of organisms the guesses and then allow them to mutate and recombine selecting only the fittest to survive each generation refining the guesses forms of evolutionary computation include swarm intelligence algorithms such as ant colony or particle swarm optimization and evolutionary algorithms such as genetic algorithms gene expression programming and genetic programming logic logic is used for knowledge representation and problem solving but it can be applied to other problems as well for example the satplan algorithm uses logic for planning and inductive logic programming is method for learning several different forms of logic are used in ai research propositional or sentential logic is the logic of statements which can be true or false first order logic also allows the use of quantifiers and predicates and can express facts about objects their properties and their relations with each other fuzzy logic is version of first order logic which allows the truth of statement to be represented as value between and rather than simply true or false fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems subjective logic models uncertainty in different and more explicit manner than fuzzy logic given binomial opinion satisfies belief disbelief uncertainty within beta distribution by this method ignorance can be distinguished from probabilistic statements that an agent makes with high confidence default logics non monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem several extensions of logic have been designed to handle specific domains of knowledge such as description logics situation calculus event calculus and fluent calculus for representing events and time causal calculus belief calculus and modal logics probabilistic methods for uncertain reasoning many problems in ai in reasoning planning learning perception and robotics require the agent to operate with incomplete or uncertain information ai researchers have devised number of powerful tools to solve these problems using methods from probability theory and economics bayesian networks are very general tool that can be used for large number of problems reasoning using the bayesian inference algorithm learning using the expectation maximization algorithm planning using decision networks and perception using dynamic bayesian networks probabilistic algorithms can also be used for filtering prediction smoothing and finding explanations for streams of data helping perception systems to analyze processes that occur over time hidden markov models or kalman filters key concept from the science of economics is utility measure of how valuable something is to an intelligent agent precise mathematical tools have been developed that analyze how an agent can make choices and plan using decision theory decision analysis and information value theory these tools include models such as markov decision processes dynamic decision networks game theory and mechanism design classifiers and statistical learning methods the simplest ai applications can be divided into two types classifiers if shiny then diamond and controllers if shiny then pick up controllers do however also classify conditions before inferring actions and therefore classification forms central part of many ai systems classifiers are functions that use pattern matching to determine closest match they can be tuned according to examples making them very attractive for use in ai these examples are known as observations or patterns in supervised learning each pattern belongs to certain predefined class class can be seen as decision that has to be made all the observations combined with their class labels are known as data set when new observation is received that observation is classified based on previous experience classifier can be trained in various ways there are many statistical and machine learning approaches the most widely used classifiers are the neural network kernel methods such as the support vector machine nearest neighbor algorithm gaussian mixture model naive bayes classifier and decision tree the performance of these classifiers have been compared over wide range of tasks classifier performance depends greatly on the characteristics of the data to be classified there is no single classifier that works best on all given problems this is also referred to as the no free lunch theorem determining suitable classifier for given problem is still more an art than science neural networks neural network is an interconnected group of nodes akin to the vast network of neurons in the human brain the study of artificial neural networks began in the decade before the field of ai research was founded in the work of walter pitts and warren mccullough other important early researchers were frank rosenblatt who invented the perceptron and paul werbos who developed the backpropagation algorithm the main categories of networks are acyclic or feedforward neural networks where the signal passes in only one direction and recurrent neural networks which allow feedback among the most popular feedforward networks are perceptrons multi layer perceptrons and radial basis networks among recurrent networks the most famous is the hopfield net form of attractor network which was first described by john hopfield in neural networks can be applied to the problem of intelligent control for robotics or learning using such techniques as hebbian learning and competitive learning hierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex the term deep learning gained traction in the mid after publication by geoffrey hinton and ruslan salakhutdinov showed how many layered feedforward neural network could be effectively pre trained one layer at time treating each layer in turn as an unsupervised restricted boltzmann machine then using supervised backpropagation for fine tuning control theory control theory the grandchild of cybernetics has many important applications especially in robotics languages ai researchers have developed several specialized languages for ai research including lisp and prolog evaluating progress in alan turing proposed general procedure to test the intelligence of an agent now known as the turing test this procedure allows almost all the major problems of artificial intelligence to be tested however it is very difficult challenge and at present all agents fail artificial intelligence can also be evaluated on specific problems such as small problems in chemistry hand writing recognition and game playing such tests have been termed subject matter expert turing tests smaller problems provide more achievable goals and there are an ever increasing number of positive results one classification for outcomes of an ai test is optimal it is not possible to perform better strong super human performs better than all humans super human performs better than most humans sub human performs worse than most humans for example performance at draughts checkers is optimal performance at chess is super human and nearing strong super human see computer chess computers versus human and performance at many everyday tasks such as recognizing face or crossing room without bumping into something is sub human quite different approach measures machine intelligence through tests which are developed from mathematical definitions of intelligence examples of these kinds of tests start in the late nineties devising intelligence tests using notions from kolmogorov complexity and data compression two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of requirement for human testers derivative of the turing test is the completely automated public turing test to tell computers and humans apart captcha as the name implies this helps to determine that user is an actual person and not computer posing as human in contrast to the standard turing test captcha administered by machine and targeted to human as opposed to being administered by human and targeted to machine computer asks user to complete simple test then generates grade for that test computers are unable to solve the problem so correct solutions are deemed to be the result of person taking the test common type of captcha is the test that requires the typing of distorted letters numbers or symbols that appear in an image undecipherable by computer applications an automated online assistant providing customer service on web page one of many very primitive applications of artificial intelligence artificial intelligence techniques are pervasive and are too numerous to list frequently when technique reaches mainstream use it is no longer considered artificial intelligence this phenomenon is described as the ai effect an area that artificial intelligence has contributed greatly to is intrusion detection competitions and prizes there are number of competitions and prizes to promote research in artificial intelligence the main areas promoted are general machine intelligence conversational behavior data mining robotic cars robot soccer and games platforms platform or computing platform is defined as some sort of hardware architecture or software framework including application frameworks that allows software to run as rodney brooks pointed out many years ago it is not just the artificial intelligence software that defines the ai features of the platform but rather the actual platform itself that affects the ai that results there needs to be work in ai problems on real world platforms rather than in isolation wide variety of platforms has allowed different aspects of ai to develop ranging from expert systems albeit pc based but still an entire real world system to various robot platforms such as the widely available roomba with open interface toys aibo the first robotic pet grew out of sony computer science laboratory csl famed engineer toshitada doi is credited as aibo original progenitor in he had started work on robots with artificial intelligence expert masahiro fujita at csl doi friend the artist hajime sorayama was enlisted to create the initial designs for the aibo body those designs are now part of the permanent collections of museum of modern art and the smithsonian institution with later versions of aibo being used in studies in carnegie mellon university in aibo was added into carnegie mellon university robot hall of fame philosophy and ethics there are three philosophical questions related to ai is artificial general intelligence possible can machine solve any problem that human being can solve using intelligence or are there hard limits to what machine can accomplish are intelligent machines dangerous how can we ensure that machines behave ethically and that they are used ethically can machine have mind consciousness and mental states in exactly the same sense that human beings do can it machine be sentient and thus deserve certain rights can machine intentionally cause harm the limits of artificial general intelligence can machine be intelligent can it think turing polite convention we need not decide if machine can think we need only decide if machine can act as intelligently as human being this approach to the philosophical problems associated with artificial intelligence forms the basis of the turing test the dartmouth proposal every aspect of learning or any other feature of intelligence can be so precisely described that machine can be made to simulate it this conjecture was printed in the proposal for the dartmouth conference of and represents the position of most working ai researchers newell and simon physical symbol system hypothesis physical symbol system has the necessary and sufficient means of general intelligent action newell and simon argue that intelligence consists of formal operations on symbols hubert dreyfus argued that on the contrary human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having feel for the situation rather than explicit symbolic knowledge see dreyfus critique of ai g\u00f6delian arguments g\u00f6del himself john lucas in and roger penrose in more detailed argument from onwards argued that humans are not reducible to turing machines the detailed arguments are complex but in essence they derive from kurt g\u00f6del proof in his first incompleteness theorem that it is always possible to create statements that formal system could not prove human being however can with some thought see the truth of these g\u00f6del statements any turing program designed to search for these statements can have its methods reduced to formal system and so will always have g\u00f6del statement derivable from its program which it can never discover however if humans are indeed capable of understanding mathematical truth it doesn seem possible that we could be limited in the same way this is quite general result if accepted since it can be shown that hardware neural nets and computers based on random processes annealing approaches and quantum computers based on entangled qubits so long as they involve no new physics can all be reduced to turing machines all they do is reduce the complexity of the tasks not permit new types of problems to be solved roger penrose speculates that there may be new physics involved in our brain perhaps at the intersection of gravity and quantum mechanics at the planck scale this argument if accepted does not rule out the possibility of true artificial intelligence but means it has to be biological in basis or based on new physical principles the argument has been followed up by many counter arguments and then roger penrose has replied to those with counter counter examples and it is now an intricate complex debate for details see philosophy of artificial intelligence lucas penrose and g\u00f6del the artificial brain argument the brain can be simulated by machines and because brains are intelligent simulated brains must also be intelligent thus machines can be intelligent hans moravec ray kurzweil and others have argued that it is technologically feasible to copy the brain directly into hardware and software and that such simulation will be essentially identical to the original the ai effect machines are already intelligent but observers have failed to recognize it when deep blue beat gary kasparov in chess the machine was acting intelligently however onlookers commonly discount the behavior of an artificial intelligence program by arguing that it is not real intelligence after all thus real intelligence is whatever intelligent behavior people can do that machines still can not this is known as the ai effect ai is whatever hasn been done yet intelligent behaviour and machine ethics as minimum an ai system must be able to reproduce aspects of human intelligence this raises the issue of how ethically the machine should behave towards both humans and other ai agents this issue was addressed by wendell wallach in his book titled moral machines in which he introduced the concept of artificial moral agents ama for wallach amas have become part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as does humanity want computers making moral decisions and can ro bots really be moral for wallach the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior in contrast to the constraints which society may place on the development of amas machine ethics the field of machine ethics is concerned with giving machines ethical principles or procedure for discovering way to resolve the ethical dilemmas they might encounter enabling them to function in an ethically responsible manner through their own ethical decision making the field was delineated in the aaai fall symposium on machine ethics past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings with few people being interested in how human beings ought to treat machines in all cases only human beings have engaged in ethical reasoning the time has come for adding an ethical dimension to at least some machines recognition of the ethical ramifications of behavior involving machines as well as recent and potential developments in machine autonomy necessitate this in contrast to computer hacking software property issues privacy issues and other topics normally ascribed to computer ethics machine ethics is concerned with the behavior of machines towards human users and other machines research in machine ethics is key to alleviating concerns with autonomous systems it could be argued that the notion of autonomous machines without such dimension is at the root of all fear concerning machine intelligence further investigation of machine ethics could enable the discovery of problems with current ethical theories advancing our thinking about ethics machine ethics is sometimes referred to as machine morality computational ethics or computational morality variety of perspectives of this nascent field can be found in the collected edition machine ethics that stems from the aaai fall symposium on machine ethics malevolent and friendly ai political scientist charles rubin believes that ai can be neither designed nor guaranteed to be benevolent he argues that any sufficiently advanced benevolence may be from malevolence humans should not assume machines or robots would treat us favorably because there is no priori reason to believe that they would be sympathetic to our system of morality which has evolved along with our particular biology which ais would not share hyper intelligent software may not necessarily decide to support the continued existence of mankind and would be extremely difficult to stop this topic has also recently begun to be discussed in academic publications as real source of risks to civilization humans and planet earth physicist stephen hawking microsoft founder bill gates and spacex founder elon musk have expressed concerns about the possibility that ai could evolve to the point that humans could not control it with hawking theorizing that this could spell the end of the human race one proposal to deal with this is to ensure that the first generally intelligent ai is friendly ai and will then be able to control subsequently developed ais some question whether this kind of check could really remain in place leading ai researcher rodney brooks writes think it is mistake to be worrying about us developing malevolent ai anytime in the next few hundred years think the worry stems from fundamental error in not distinguishing the difference between the very real recent advances in particular aspect of ai and the enormity and complexity of building sentient volitional intelligence devaluation of humanity joseph weizenbaum wrote that ai applications can not by definition successfully simulate genuine human empathy and that the use of ai technology in fields such as customer service or psychotherapy was deeply misguided weizenbaum was also bothered that ai researchers and some philosophers were willing to view the human mind as nothing more than computer program position now known as to weizenbaum these points suggest that ai research devalues human life decrease in demand for human labor martin ford author of the lights in the tunnel automation accelerating technology and the economy of the future and others argue that specialized artificial intelligence applications robotics and other forms of automation will ultimately result in significant unemployment as machines begin to match and exceed the capability of workers to perform most routine and repetitive jobs ford predicts that many knowledge based occupations and in particular entry level jobs will be increasingly susceptible to automation via expert systems machine learning and other ai enhanced applications ai based applications may also be used to amplify the capabilities of low wage offshore workers making it more feasible to outsource knowledge work machine consciousness sentience and mind if an ai system replicates all key aspects of human intelligence will that system also be sentient will it have mind which has conscious experiences this question is closely related to the philosophical problem as to the nature of human consciousness generally referred to as the hard problem of consciousness consciousness there are no objective criteria for knowing whether an intelligent agent is sentient that it has conscious experiences we assume that other people do because we do and they tell us that they do but this is only subjective determination the lack of any hard criteria is known as the hard problem in the theory of consciousness the problem applies not only to other people but to the higher animals and by extension to ai agents are human intelligence consciousness and mind products of information processing is the brain essentially computer is the idea that the human mind or the human brain or both is an information processing system and that thinking is form of computing ai or implementing machines with human intelligence was founded on the claim that central property of humans intelligence can be so precisely described that machine can be made to simulate it program can then be derived from this human computer and implemented into an artificial one to create efficient artificial intelligence this program would act upon set of outputs that result from set inputs of the internal memory of the computer that is the machine can only act with what it has implemented in it to start with long term goal for ai researchers is to provide machines with deep understanding of the many abilities of human being to replicate general intelligence or strong ai defined as machine surpassing human abilities to perform the skills implanted in it scary thought to many who fear losing control of such powerful machine obstacles for researchers are mainly time contstraints that is ai scientists cannot establish much of database for commonsense knowledge because it must be ontologically crafted into the machine which takes up tremendous amount of time to combat this ai research looks to have the machine able to understand enough concepts in order to add to its own ontology but how can it do this when machine ethics is primarily concerned with behavior of machines towards humans or other machines limiting the extent of developing ai in order to function like common human ai must also display the ability to solve subsymbolic commonsense knowledge tasks such as how artists can tell statues are fake or how chess masters don move certain spots to avoid exposure but by developing machines who can do it all ai research is faced with the difficulty of potentially putting lot of people out of work while on the economy side of things businesses would boom from efficiency thus forcing ai into bottleneck trying to developing self improving machines strong ai hypothesis searle strong ai hypothesis states that the appropriately programmed computer with the right inputs and outputs would thereby have mind in exactly the same sense human beings have minds john searle counters this assertion with his chinese room argument which asks us to look inside the computer and try to find where the mind might be robot rights mary shelley frankenstein considers key issue in the ethics of artificial intelligence if machine can be created that has intelligence could it also feel if it can feel does it have the same rights as human the idea also appears in modern science fiction such as the film artificial intelligence in which humanoid machines have the ability to feel emotions this issue now known as robot rights is currently being considered by for example california institute for the future although many critics believe that the discussion is premature the subject is profoundly discussed in the documentary film plug pray are there limits to how intelligent machines or human machine hybrids can be or superhuman intelligence is hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind may also refer to the form or degree of intelligence possessed by such an agent technological singularity if research into strong ai produced sufficiently intelligent software it might be able to reprogram and improve itself the improved software would be even better at improving itself leading to recursive self improvement the new intelligence could thus increase exponentially and dramatically surpass humans science fiction writer vernor vinge named this scenario singularity technological singularity is when accelerating progress in technologies will cause runaway effect wherein artificial intelligence will exceed human intellectual capacity and control thus radically changing or even ending civilization because the capabilities of such an intelligence may be impossible to comprehend the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable ray kurzweil has used moore law which describes the relentless exponential improvement in digital technology to calculate that desktop computers will have the same processing power as human brains by the year and predicts that the singularity will occur in transhumanism robot designer hans moravec cyberneticist kevin warwick and inventor ray kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either this idea called transhumanism which has roots in aldous huxley and robert ettinger has been illustrated in fiction as well for example in the manga ghost in the shell and the science fiction series dune in the artist hajime sorayama sexy robots series were painted and published in japan depicting the actual organic human form with lifelike muscular metallic skins and later the gynoids book followed that was used by or influenced movie makers including george lucas and other creatives sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind fantasy existing in the mind even when realized in actual form edward fredkin argues that artificial intelligence is the next stage in evolution an idea first proposed by samuel butler darwin among the machines and expanded upon by george dyson in his book of the same name in in fiction the implications of artificial intelligence have been persistent theme in science fiction early stories typically revolved around intelligent robots the word robot itself was coined by karel \u010dapek in his play the title standing for rossum universal robots later the sf writer isaac asimov developed the three laws of robotics which he subsequently explored in long series of robot stories these laws have since gained some traction in genuine ai research other influential fictional intelligences include hal the computer in charge of the spaceship in space odyssey released as both film and book in and written by arthur clarke since then ai has become firmly rooted in popular culture see also ai takeover artificial intelligence journal artificial intelligence video games artificial stupidity nick bostrom computer go effective altruism existential risk existential risk of artificial general intelligence future of humanity institute human cognome project list of artificial intelligence projects list of artificial intelligence researchers list of emerging technologies list of important artificial intelligence publications list of machine learning algorithms list of scientific journals machine ethics machine learning never ending language learning our final invention outline of artificial intelligence outline of human intelligence philosophy of mind simulated reality symbolic artificial intelligence notes references ai textbooks history of ai other sources bibtex in cited by presidential address to the association for the advancement of artificial intelligence later published as further reading techcast article series john sagi framing consciousness boden margaret mind as machine oxford university press johnston john the allure of machinic life cybernetics artificial life and the new ai mit press myers courtney boyd ed the ai report forbes june sun bookman eds computational architectures integrating neural and symbolic processes kluwer academic publishers needham ma external links what is ai an introduction to artificial intelligence by ai founder john mccarthy the handbook of artificial intelligence volume by avron barr and edward feigenbaum stanford university aitopics large directory of links and other resources maintained by the association for the advancement of artificial intelligence the leading organization of academic ai researchers"}