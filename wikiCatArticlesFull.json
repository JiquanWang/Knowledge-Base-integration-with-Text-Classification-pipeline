{"Telecommunications engineering": "engineer working to maintain london phone service during world war january engineering or telecom engineering is an engineering discipline that brings together electrical engineering with computer science to enhance systems the work ranges from basic circuit design to strategic mass developments engineer is responsible for designing and overseeing the installation of equipment and facilities such as complex electronic switching systems copper wire telephone facilities and fiber optics engineering also overlaps heavily with broadcast engineering is diverse field of engineering which is connected to electronics civil structural and electrical engineering ultimately telecom engineers are responsible for providing the method for customers to have telephone and high speed data services it helps people who are closely working in political and social fields as well accounting and project management telecom engineers use variety of equipment and transport media available from multitude of manufacturers to design the telecom network infrastructure the most common media used by wired companies today are copper wires coaxial cable and fiber optics engineers use their technical expertise to also provide range of services and engineering solutions revolving around wireless mode of communication and other information transfer such as wireless telephony services radio and satellite communications internet and broadband technologies telecom engineers are often expected as most engineers are to provide the best solution possible for the lowest cost to the company most of the work is carried out on project basis with tight deadlines and well defined milestones for the delivery of project objectives engineers are involved across all aspects of service delivery from carrying out feasibility exercises and determining connectivity to preparing detailed technical and operational documentation this often leads to creative solutions to problems that often would have been designed differently without the budget constraints dictated by modern society in the earlier days of the telecom industry massive amounts of cable were placed that were never used or have been replaced by modern technology such as fiber optic cable and digital multiplexing techniques telecom engineers are also responsible for overseeing the companies records of equipment and facility assets their work directly impacts assigning appropriate accounting codes for taxes and maintenance purposes budgeting and overseeing projects history systems are generally designed by engineers which sprang from technological improvements in the telegraph industry in the late th century and the radio and the telephone industries in the early th century today is widespread and devices that assist the process such as the television radio and telephone are common in many parts of the world there are also many networks that connect these devices including computer networks public switched telephone network pstn radio networks and television networks computer communication across the internet is one of many examples of plays vital role in the part of world economy and the industry revenue has been placed at just under of the gross world product telegraph and telephone alexander graham bell big box telephone one of the first commercially available telephones national museum of american history samuel morse independently developed version of the electrical telegraph that he unsuccessfully demonstrated on september soon after he was joined by alfred vail who developed the register telegraph terminal that integrated logging device for recording messages to paper tape this was demonstrated successfully over three miles five kilometres on january and eventually over forty miles sixty four kilometres between washington and baltimore on may the patented invention proved lucrative and by telegraph lines in the united states spanned over miles kilometres the first successful transatlantic telegraph cable was completed on july allowing transatlantic for the first time earlier transatlantic cables installed in and only operated for few days or weeks before they failed the international use of the telegraph has sometimes been dubbed the victorian internet the first commercial telephone services were set up in and on both sides of the atlantic in the cities of new haven and london alexander graham bell held the master patent for the telephone that was needed for such services in both countries the technology grew quickly from this point with inter city lines being built and telephone exchanges in every major city of the united states by the mid despite this transatlantic voice communication remained impossible for customers until january when connection was established using radio however no cable connection existed until tat was inaugurated on september providing telephone circuits in bell and co inventor charles sumner tainter conducted the world first wireless telephone call via modulated lightbeams projected by photophones the scientific principles of their invention would not be utilized for several decades when they were first deployed in military and fiber optic communications radio and television marconi crystal radio receiver over several years starting in the italian inventor guglielmo marconi built the first complete commercially successful wireless telegraphy system based on airborne electromagnetic waves radio transmission in december he would go on to established wireless communication between britain and newfoundland earning him the nobel prize in physics in which he shared with karl braun in reginald fessenden was able to wirelessly transmit human voice on march scottish inventor john logie baird publicly demonstrated the transmission of moving silhouette pictures at the london department store selfridges in october baird was successful in obtaining moving pictures with halftone shades which were by most accounts the first true television pictures this led to public demonstration of the improved device on january again at selfridges baird first devices relied upon the nipkow disk and thus became known as the mechanical television it formed the basis of semi experimental broadcasts done by the british broadcasting corporation beginning september satellite the first satellite to relay communications was project score in which used tape recorder to store and forward voice messages it was used to send christmas greeting to the world from president dwight eisenhower in nasa launched an echo satellite the aluminized pet film balloon served as passive reflector for radio communications courier built by philco also launched in was the world first active repeater satellite satellites these days are used for many applications such as uses in gps television internet and telephone uses telstar was the first active direct relay commercial communications satellite belonging to at as part of multi national agreement between at bell telephone laboratories nasa the british general post office and the french national ptt post office to develop satellite communications it was launched by nasa from cape canaveral on july the first privately sponsored space launch relay was launched on december and became the first satellite to broadcast across the pacific on november the first and historically most important application for communication satellites was in long distance telephony the fixed public switched telephone network relays telephone calls from land line telephones to an earth station where they are then transmitted receiving satellite dish via geostationary satellite in earth orbit improvements in submarine communications cables through the use of fiber optics caused some decline in the use of satellites for fixed telephony in the late th century but they still exclusively service remote islands such as ascension island saint helena diego garcia and easter island where no submarine cables are in service there are also some continents and some regions of countries where landline are rare to nonexistent for example antarctica plus large regions of australia south america africa northern canada china russia and greenland after commercial long distance telephone service was established via communication satellites host of other commercial were also adapted to similar satellites starting in including mobile satellite phones satellite radio satellite television and satellite internet access the earliest adaption for most such services occurred in the as the pricing for commercial satellite transponder channels continued to drop significantly computer networks and the internet symbolic representation of the arpanet as of september on september george stibitz was able to transmit problems using teleprinter to his complex number calculator in new york and receive the computed results back at dartmouth college in new hampshire this configuration of centralized computer or mainframe computer with remote dumb terminals remained popular throughout the and into the however it was not until the that researchers started to investigate packet switching technology that allows chunks of data to be sent between different computers without first passing through centralized mainframe four node network emerged on december this network soon became the arpanet which by would consist of nodes arpanet development centered around the request for comment process and on april rfc was published this process is important because arpanet would eventually merge with other networks to form the internet and many of the communication protocols that the internet relies upon today were specified through the request for comment process in september rfc introduced the internet protocol version ipv and rfc introduced the transmission control protocol tcp thus creating the tcp ip protocol that much of the internet relies upon today optical fiber optical fiber optical fiber can be used as medium for and computer networking because it is flexible and can be bundled as cables it is especially advantageous for long distance communications because light propagates through the fiber with little attenuation compared to electrical cables this allows long distances to be spanned with few repeaters in charles kao and george hockham proposed optical fibers at stc laboratories stl at harlow england when they showed that the losses of db km in existing glass compared to db km in coaxial cable was due to contaminants which could potentially be removed optical fiber was successfully developed in by corning glass works with attenuation low enough for communication purposes about db km and at the same time gaas gallium arsenide semiconductor lasers were developed that were compact and therefore suitable for transmitting light through fiber optic cables for long distances after period of research starting from the first commercial fiber optic communications system was developed which operated at wavelength around \u00b5m and used gaas semiconductor lasers this first generation system operated at bit rate of mbps with repeater spacing of up to km soon on april general telephone and electronics sent the first live telephone traffic through fiber optics at mbit throughput in long beach california the first wide area network fibre optic cable system in the world seems to have been installed by rediffusion in hastings east sussex uk in the cables were placed in ducting throughout the town and had over subscribers they were used at that time for the transmission of television channels not available because of local reception problems the first transatlantic telephone cable to use optical fiber was tat based on desurvire optimized laser amplification technology it went into operation in in the late through industry promoters and research companies such as kmi and rhk predicted massive increases in demand for communications bandwidth due to increased use of the internet and of various bandwidth intensive consumer services such as video on demand internet protocol data traffic was increasing exponentially at faster rate than integrated circuit complexity had increased under moore law concepts radio transmitter room basic elements of system transmitter transmitter information source that takes information and converts it to signal for transmission in electronics and transmitter or radio transmitter is an electronic device which with the aid of an antenna produces radio waves in addition to their use in broadcasting transmitters are necessary component parts of many electronic devices that communicate by radio such as cell phones copper wires transmission medium transmission medium over which the signal is transmitted for example the transmission medium for sounds is usually air but solids and liquids may also act as transmission media for sound many transmission media are used as communications channel one of the most common physical medias used in networking is copper wire copper wire to carry signals to long distances using relatively low amounts of power another example of physical medium is optical fiber which has emerged as the most commonly used transmission medium for long distance communications optical fiber is thin strand of glass that guides light along its length the absence of material medium in vacuum may also constitute transmission medium for electromagnetic waves such as light and radio waves receiver receiver information sink that receives and converts the signal back into required information in radio communications radio receiver is an electronic device that receives radio waves and converts the information carried by them to usable form it is used with an antenna the information produced by the receiver may be in the form of sound an audio signal images video signal or data digital signal wireless communication tower cell site wired communication wired communications make use of underground communications cables less often overhead lines electronic signal amplifiers repeaters inserted into connecting cables at specified points and terminal apparatus of various types depending on the type of wired communications used wireless communication wireless communication involves the transmission of information over distance without help of wires cables or any other forms of electrical conductors wireless operations permit services such as long range communications that are impossible or impractical to implement with the use of wires the term is commonly used in the industry to refer to systems radio transmitters and receivers remote controls etc which use some form of energy radio waves acoustic energy etc to transfer information without the use of wires information is transferred in this manner over both short and long distances roles telecom equipment engineer telecom equipment engineer is an electronics engineer that designs equipment such as routers switches multiplexers and other specialized computer electronics equipment designed to be used in the network infrastructure network engineer network engineer is computer engineer that is in charge of designing deploying and maintaining computer networks in addition he she oversees network operations from network operations center designs backbone infrastructure or supervises in data center central office engineer central office installation central office engineer is responsible for designing and overseeing the implementation of equipment in central office co for short also referred to as wire center or telephone exchange co engineer is responsible for integrating new technology into the existing network assigning the equipment location in the wire center and providing power clocking for digital equipment and alarm monitoring facilities for the new equipment the co engineer is also responsible for providing more power clocking and alarm monitoring facilities if there are currently not enough available to support the new equipment being installed finally the co engineer is responsible for designing how the massive amounts of cable will be distributed to various equipment and wiring frames throughout the wire center and overseeing the installation and turn up of all new equipment subroles as structural engineers co engineers are responsible for the structural design and placement of racking and bays for the equipment to be installed in as well as for the plant to be placed on as electrical engineers co engineers are responsible for the resistance capacitance and inductance rcl design of all new plant to ensure telephone service is clear and crisp and data service is clean as well as reliable attenuation or gradual loss in intensity and loop loss calculations are required to determine cable length and size required to provide the service called for in addition power requirements have to be calculated and provided to power any electronic equipment being placed in the wire center overall co engineers have seen new challenges emerging in the co environment with the advent of data centers internet protocol ip facilities cellular radio sites and other emerging technology equipment environments within networks it is important that consistent set of established practices or requirements be implemented installation suppliers or their sub contractors are expected to provide requirements with their products features or services these services might be associated with the installation of new or expanded equipment as well as the removal of existing equipment several other factors must be considered such as regulations and safety in installation removal of hazardous material commonly used tools to perform installation and removal of equipment outside plant engineer engineers working on cross connect box also known as serving area interface outside plant osp engineers are also often called field engineers because they frequently spend much time in the field taking notes about the civil environment aerial above ground and below ground osp engineers are responsible for taking plant copper fiber etc from wire center to distribution point or destination point directly if distribution point design is used then cross connect box is placed in strategic location to feed determined distribution area the cross connect box also known as serving area interface is then installed to allow connections to be made more easily from the wire center to the destination point and ties up fewer facilities by not having dedication facilities from the wire center to every destination point the plant is then taken directly to its destination point or to another small closure called terminal where access can also be gained to the plant if necessary these access points are preferred as they allow faster repair times for customers and save telephone operating companies large amounts of money the plant facilities can be delivered via underground facilities either direct buried or through conduit or in some cases laid under water via aerial facilities such as telephone or power poles or via microwave radio signals for long distances where either of the other two methods is too costly subroles engineer osp climbing the telephone pole as structural engineers osp engineers are responsible for the structural design and placement of cellular towers and telephone poles as well as calculating pole capabilities of existing telephone or power poles onto which new plant is being added structural calculations are required when boring under heavy traffic areas such as highways or when attaching to other structures such as bridges shoring also has to be taken into consideration for larger trenches or pits conduit structures often include encasements of slurry that needs to be designed to support the structure and withstand the environment around it soil type high traffic areas etc as electrical engineers osp engineers are responsible for the resistance capacitance and inductance rcl design of all new plant to ensure telephone service is clear and crisp and data service is clean as well as reliable attenuation or gradual loss in intensity and loop loss calculations are required to determine cable length and size required to provide the service called for in addition power requirements have to be calculated and provided to power any electronic equipment being placed in the field ground potential has to be taken into consideration when placing equipment facilities and plant in the field to account for lightning strikes high voltage intercept from improperly grounded or broken power company facilities and from various sources of electromagnetic interference as civil engineers osp engineers are responsible for drafting plans either by hand or using computer aided design cad software for how telecom plant facilities will be placed often when working with municipalities trenching or boring permits are required and drawings must be made for these often these drawings include about or so of the detailed information required to pave road or add turn lane to an existing street structural calculations are required when boring under heavy traffic areas such as highways or when attaching to other structures such as bridges as civil engineers telecom engineers provide the modern communications backbone for all technological communications distributed throughout civilizations today unique to telecom engineering is the use of air core cable which requires an extensive network of air handling equipment such as compressors manifolds regulators and hundreds of miles of air pipe per system that connects to pressurized splice cases all designed to pressurize this special form of copper cable to keep moisture out and provide clean signal to the customer as political and social ambassador the osp engineer is telephone operating company face and voice to the local authorities and other utilities osp engineers often meet with municipalities construction companies and other utility companies to address their concerns and educate them about how the telephone utility works and operates additionally the osp engineer has to secure real estate to place outside facilities on such as an easement to place cross connect box on see also computer engineering electronic design automation electronic media information theory list of electrical engineering topics alphabetical list of electrical engineering topics thematic professional engineer radio telephone television two way radio computer networking history of wireless wired communication transmitter fiber optic communication receiver radio transmission medium references further reading external links telephone engineer resources telephone engineering directory http www phoneengineers co uk telephone engineers directory http info", "Computer network": "computer network or data network is network which allows computers to exchange data in computer networks networked computing devices exchange data with each other along network links data connections the connections between nodes are established using either cable media or wireless media the best known computer network is the internet network computer devices that originate route and terminate the data are called network nodes nodes can include hosts such as personal computers phones servers as well as networking hardware two such devices can be said to be networked together when one device is able to exchange information with the other device whether or not they have direct connection to each other computer networks differ in the transmission media used to carry their signals the communications protocols to organize network traffic the network size topology and organizational intent in most cases communications protocols are layered on work using other more specific or more general communications protocols except for the physical layer that directly deals with the transmission media computer networks support applications such as access to the world wide web shared use of application and storage servers printers and fax machines and use of email and instant messaging applications history the chronology of significant computer network developments includes in the late early networks of communicating computers included the military radar system semi automatic ground environment sage in anatolii ivanovich kitov proposed to the central committee of the communist party of the soviet union detailed plan for the re organisation of the control of the soviet armed forces and of the soviet economy on the basis of network of computing centres in the commercial airline reservation system semi automatic business research environment sabre went online with two connected mainframes in licklider developed working group he called the intergalactic computer network precursor to the arpanet at the advanced research projects agency arpa in researchers at dartmouth college developed the dartmouth time sharing system for distributed users of large computer systems the same year at massachusetts institute of technology research group supported by general electric and bell labs used computer to route and manage telephone connections throughout the leonard kleinrock paul baran and donald davies independently developed network systems that used packets to transfer information between computers over network in thomas marill and lawrence roberts created the first wide area network wan this was an immediate precursor to the arpanet of which roberts became program manager also in western electric introduced the first widely used telephone switch that implemented true computer control in the university of california at los angeles the stanford research institute the university of california at santa barbara and the university of utah became connected as the beginning of the arpanet network using kbit circuits in commercial services using were deployed and later used as an underlying infrastructure for expanding tcp ip networks in robert metcalfe wrote formal memo at xerox parc describing ethernet networking system that was based on the aloha network developed in the by norman abramson and colleagues at the university of hawaii in july robert metcalfe and david boggs published their paper ethernet distributed packet switching for local computer networks and collaborated on several patents received in and in robert metcalfe pursued making ethernet an open standard in john murphy of datapoint corporation created arcnet token passing network first used to share storage devices in the transmission speed capacity for ethernet increased from mbit to mbit by ethernet supported transmission speeds of gigabit the ability of ethernet to scale easily such as quickly adapting to support new fiber optic cable speeds is contributing factor to its continued use properties computer networking may be considered branch of electrical engineering computer science information technology or computer engineering since it relies upon the theoretical and practical application of the related disciplines computer network facilitates interpersonal communications allowing people to communicate efficiently and easily via email instant messaging chat rooms telephone video telephone calls and video conferencing providing access to information on shared storage devices is an important feature of many networks network allows sharing of files data and other types of information giving authorized users the ability to access information stored on other computers on the network network allows sharing of network and computing resources users may access and use resources provided by devices on the network such as printing document on shared network printer distributed computing uses computing resources across network to accomplish tasks computer network may be used by computer crackers to deploy computer viruses or computer worms on devices connected to the network or to prevent these devices from accessing the network via denial of service attack network packet computer communication links that do not support packets such as traditional point to point links simply transmit data as bit stream however most information in computer networks is carried in packets network packet is formatted unit of data list of bits or bytes usually few tens of bytes to few kilobytes long carried by packet switched network in packet networks the data is formatted into packets that are sent through the network to their destination once the packets arrive they are reassembled into their original message with packets the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched when one user is not sending packets the link can be filled with packets from others users and so the cost can be shared with relatively little interference provided the link isn overused packets consist of two kinds of data control information and user data also known as payload the control information provides data the network needs to deliver the user data for example source and destination network addresses error detection codes and sequencing information typically control information is found in packet headers and trailers with payload data in between often the route packet needs to take through network is not immediately available in that case the packet is queued and waits until link is free network topology the physical layout of network is usually less important than the topology that connects network nodes most diagrams that describe physical network are therefore topological rather than geographic the symbols on these diagrams usually denote network links and network nodes network links the transmission media often referred to in the literature as the physical media used to link devices to form computer network include electrical cable ethernet homepna power line communication hn optical fiber fiber optic communication and radio waves wireless networking in the osi model these are defined at layers and the physical layer and the data link layer widely adopted family of transmission media used in local area network lan technology is collectively known as ethernet the media and protocol standards that enable communication between networked devices over ethernet are defined by ieee ethernet transmits data over both copper and fiber cables wireless lan standards those defined by ieee use radio waves or others use infrared signals as transmission medium power line communication uses building power cabling to transmit data wired technologies fiber optic cables are used to transmit light from one computer network node to another the orders of the following wired technologies are roughly from slowest to fastest transmission speed coaxial cable is widely used for cable television systems office buildings and other work sites for local area networks the cables consist of copper or aluminum wire surrounded by an insulating layer typically flexible material with high dielectric constant which itself is surrounded by conductive layer the insulation helps minimize interference and distortion transmission speed ranges from million bits per second to more than million bits per second itu hn technology uses existing home wiring coaxial cable phone lines and power lines to create high speed up to gigabit local area network twisted pair wire is the most widely used medium for all twisted pair cabling consist of copper wires that are twisted into pairs ordinary telephone wires consist of two insulated copper wires twisted into pairs computer network cabling wired ethernet as defined by ieee consists of pairs of copper cabling that can be utilized for both voice and data transmission the use of two wires twisted together helps to reduce crosstalk and electromagnetic induction the transmission speed ranges from million bits per second to billion bits per second twisted pair cabling comes in two forms unshielded twisted pair utp and shielded twisted pair stp each form comes in several category ratings designed for use in various scenarios map showing submarine optical fiber cables around the world an optical fiber is glass fiber it carries pulses of light that represent data some advantages of optical fibers over metal wires are very low transmission loss and immunity from electrical interference optical fibers can simultaneously carry multiple wavelengths of light which greatly increases the rate that data can be sent and helps enable data rates of up to trillions of bits per second optic fibers can be used for long runs of cable carrying very high data rates and are used for undersea cables to interconnect continents price is main factor distinguishing wired and wireless technology options in business wireless options command price premium that can make purchasing wired computers printers and other devices financial benefit before making the decision to purchase hard wired technology products review of the restrictions and limitations of the selections is necessary business and employee needs may override any cost considerations wireless technologies computers are very often connected to networks using wireless links terrestrial microwave terrestrial microwave communication uses earth based transmitters and receivers resembling satellite dishes terrestrial microwaves are in the low gigahertz range which limits all communications to line of sight relay stations are spaced approximately apart communications satellites satellites communicate via microwave radio waves which are not deflected by the earth atmosphere the satellites are stationed in space typically in geosynchronous orbit above the equator these earth orbiting systems are capable of receiving and relaying voice data and tv signals cellular and pcs systems use several radio communications technologies the systems divide the region covered into multiple geographic areas each area has low power transmitter or radio relay antenna device to relay calls from one area to the next area radio and spread spectrum technologies wireless local area networks use high frequency radio technology similar to digital cellular and low frequency radio technology wireless lans use spread spectrum technology to enable communication between multiple devices in limited area ieee defines common flavor of open standards wireless radio wave technology known as wifi free space optical communication uses visible or invisible light for communications in most cases line of sight propagation is used which limits the physical positioning of communicating devices exotic technologies there have been various attempts at transporting data over exotic media ip over avian carriers was humorous april fool request for comments issued as rfc it was implemented in real life in extending the internet to interplanetary dimensions via radio waves both cases have large round trip delay time which gives slow two way communication but doesn prevent sending large amounts of information network nodes apart from any physical transmission medium there may be networks comprise additional basic system building blocks such as network interface controller nics repeaters hubs bridges switches routers modems and firewalls network interfaces atm network interface in the form of an accessory card lot of network interfaces are built in network interface controller nic is computer hardware that provides computer with the ability to access the transmission media and has the ability to process low level network information for example the nic may have connector for accepting cable or an aerial for wireless transmission and reception and the associated circuitry the nic responds to traffic addressed to network address for either the nic or the computer as whole in ethernet networks each network interface controller has unique media access control mac address usually stored in the controller permanent memory to avoid address conflicts between network devices the institute of electrical and electronics engineers ieee maintains and administers mac address uniqueness the size of an ethernet mac address is six octets the three most significant octets are reserved to identify nic manufacturers these manufacturers using only their assigned prefixes uniquely assign the three least significant octets of every ethernet interface they produce repeaters and hubs repeater is an electronic device that receives network signal cleans it of unnecessary noise and regenerates it the signal is retransmitted at higher power level or to the other side of an obstruction so that the signal can cover longer distances without degradation in most twisted pair ethernet configurations repeaters are required for cable that runs longer than meters with fiber optics repeaters can be tens or even hundreds of kilometers apart repeater with multiple ports is known as hub repeaters work on the physical layer of the osi model repeaters require small amount of time to regenerate the signal this can cause propagation delay that affects network performance as result many network architectures limit the number of repeaters that can be used in row the ethernet rule hubs have been mostly obsoleted by modern switches but repeaters are used for long distance links notably undersea cabling bridges network bridge connects and filters traffic between two network segments at the data link layer layer of the osi model to form single network this breaks the network collision domain but maintains unified broadcast domain network segmentation breaks down large congested network into an aggregation of smaller more efficient networks bridges come in three basic types local bridges directly connect lans remote bridges can be used to create wide area network wan link between lans remote bridges where the connecting link is slower than the end networks largely have been replaced with routers wireless bridges can be used to join lans or connect remote devices to lans switches network switch is device that forwards and filters osi layer datagrams between ports based on the mac addresses in the packets switch is distinct from hub in that it only forwards the frames to the physical ports involved in the communication rather than all ports connected it can be thought of as multi port bridge it learns to associate physical ports to mac addresses by examining the source addresses of received frames if an unknown destination is targeted the switch broadcasts to all ports but the source switches normally have numerous ports facilitating star topology for devices and cascading additional switches multi layer switches are capable of routing based on layer addressing or additional logical levels the term switch is often used loosely to include devices such as routers and bridges as well as devices that may distribute traffic based on load or based on application content web url identifier routers typical home or small office router showing the adsl telephone line and ethernet network cable connections router is an internetworking device that forwards packets between networks by processing the routing information included in the packet or datagram internet protocol information from layer the routing information is often processed in conjunction with the routing table or forwarding table router uses its routing table to determine where to forward packets destination in routing table can include null interface also known as the black hole interface because data can go into it however no further processing is done for said data modems modems modulator demodulator are used to connect network nodes via wire not originally designed for digital network traffic or for wireless to do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission modems are commonly used for telephone lines using digital subscriber line technology firewalls firewall is network device for controlling network security and access rules firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones the vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks network structure network topology is the layout or organizational hierarchy of interconnected nodes of computer network different network topologies can affect throughput but reliability is often more critical with many technologies such as bus networks single failure can cause the network to fail entirely in general the more there are the more robust the network is but the more expensive it is to install common layouts common network topologies common layouts are bus network all nodes are connected to common medium along this medium this was the layout used in the original ethernet called base and base star network all nodes are connected to special central node this is the typical layout found in wireless lan where each wireless client connects to the central wireless access point ring network each node is connected to its left and right neighbour node such that all nodes are connected and that each node can reach each other node by traversing nodes left or rightwards the fiber distributed data interface fddi made use of such topology mesh network each node is connected to an arbitrary number of neighbours in such way that there is at least one traversal from any node to any other fully connected network each node is connected to every other node in the network tree network nodes are arranged hierarchically note that the physical layout of the nodes in network may not necessarily reflect the network topology as an example with fddi the network topology is ring actually two counter rotating rings but the physical topology is often star because all neighboring connections can be routed via central physical location overlay network sample overlay network an overlay network is virtual computer network that is built on top of another network nodes in the overlay network are connected by virtual or logical links each link corresponds to path perhaps through many physical links in the underlying network the topology of the overlay network may and often does differ from that of the underlying one for example many peer to peer networks are overlay networks they are organized as nodes of virtual system of links that run on top of the internet overlay networks have been around since the invention of networking when computer systems were connected over telephone lines using modems before any data network existed the most striking example of an overlay network is the internet itself the internet itself was initially built as an overlay on the telephone network even today each internet node can communicate with virtually any other through an underlying mesh of sub networks of wildly different topologies and technologies address resolution and routing are the means that allow mapping of fully connected ip overlay network to its underlying network another example of an overlay network is distributed hash table which maps keys to nodes in the network in this case the underlying network is an ip network and the overlay network is table actually map indexed by keys overlay networks have also been proposed as way to improve internet routing such as through quality of service guarantees to achieve higher quality streaming media previous proposals such as intserv diffserv and ip multicast have not seen wide acceptance largely because they require modification of all routers in the network on the other hand an overlay network can be incrementally deployed on end hosts running the overlay protocol software without cooperation from internet service providers the overlay network has no control over how packets are routed in the underlying network between two overlay nodes but it can control for example the sequence of overlay nodes that message traverses before it reaches its destination for example akamai technologies manages an overlay network that provides reliable efficient content delivery kind of multicast academic research includes end system multicast resilient routing and quality of service studies among others communications protocols the tcp ip model or internet layering scheme and its relation to common protocols often layered on top of it figure message flows in the presence of router red flows are effective communication paths black paths are the actual paths communications protocol is set of rules for exchanging information over network links in protocol stack also see the osi model each protocol leverages the services of the protocol below it an important example of protocol stack is http the world wide web protocol running over tcp over ip the internet protocols over ieee the wi fi protocol this stack is used between the wireless router and the home user personal computer when the user is surfing the web whilst the use of protocol layering is today ubiquitous across the field of computer networking it has been historically criticized by many researchers for two principal reasons firstly abstracting the protocol stack in this way may cause higher layer to duplicate functionality of lower layer prime example being error recovery on both per link basis and an end to end basis secondly it is common that protocol implementation at one layer may require data state or addressing information that is only present at another layer thus defeating the point of separating the layers in the first place for example tcp uses the ecn field in the ipv header as an indication of congestion ip is network layer protocol whereas tcp is transport layer protocol communication protocols have various characteristics they may be connection oriented or connectionless they may use circuit mode or packet switching and they may use hierarchical addressing or flat addressing there are many communication protocols few of which are described below ieee the complete ieee protocol suite provides diverse set of networking capabilities the protocols have flat addressing scheme they operate mostly at levels and of the osi model for example mac bridging ieee deals with the routing of ethernet packets using spanning tree protocol ieee describes vlans and ieee defines port based network access control protocol which forms the basis for the authentication mechanisms used in vlans but it is also found in wlans it is what the home user sees when the user has to enter wireless access key ethernet ethernet sometimes simply called lan is family of protocols used in wired lans described by set of standards together called ieee published by the institute of electrical and electronics engineers wireless lan wireless lan also widely known as wlan or wifi is probably the most well known member of the ieee protocol family for home users today it is standarized by ieee and shares many properties with wired ethernet internet protocol suite the internet protocol suite also called tcp ip is the foundation of all modern networking it offers connection less as well as connection oriented services over an inherently unreliable network traversed by data gram transmission at the internet protocol ip level at its core the protocol suite defines the addressing identification and routing specifications for internet protocol version ipv and for ipv the next generation of the protocol with much enlarged addressing capability sonet sdh synchronous optical networking sonet and synchronous digital hierarchy sdh are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers they were originally designed to transport circuit mode communications from variety of different sources primarily to support real time uncompressed circuit switched voice encoded in pcm pulse code modulation format however due to its protocol neutrality and transport oriented features sonet sdh also was the obvious choice for transporting asynchronous transfer mode atm frames asynchronous transfer mode asynchronous transfer mode atm is switching technique for networks it uses asynchronous time division multiplexing and encodes data into small fixed sized cells this differs from other protocols such as the internet protocol suite or ethernet that use variable sized packets or frames atm has similarity with both circuit and packet switched networking this makes it good choice for network that must handle both traditional high throughput data traffic and real time low latency content such as voice and video atm uses connection oriented model in which virtual circuit must be established between two endpoints before the actual data exchange begins while the role of atm is diminishing in favor of next generation networks it still plays role in the last mile which is the connection between an internet service provider and the home user geographic scale network can be characterized by its physical capacity or its organizational purpose use of the network including user authorization and access rights differ accordingly nanoscale network nanoscale communication network has key components implemented at the nanoscale including message carriers and leverages physical principles that differ from macroscale communication mechanisms nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for classical communication personal area network personal area network pan is computer network used for communication among computer and different information technological devices close to one person some examples of devices that are used in pan are personal computers printers fax machines telephones pdas scanners and even video game consoles pan may include wired and wireless devices the reach of pan typically extends to meters wired pan is usually constructed with usb and firewire connections while technologies such as bluetooth and infrared communication typically form wireless pan local area network local area network lan is network that connects computers and devices in limited geographical area such as home school office building or closely positioned group of buildings each computer or device on the network is node wired lans are most likely based on ethernet technology newer standards such as itu hn also provide way to create wired lan using existing wiring such as coaxial cables telephone lines and power lines all interconnected devices use the network layer layer to handle multiple subnets represented by different colors those inside the library have mbit ethernet connections to the user device and gigabit ethernet connection to the central router they could be called layer switches because they only have ethernet interfaces and support the internet protocol it might be more correct to call them access routers where the router at the top is distribution router that connects to the internet and to the academic networks customer access routers the defining characteristics of lan in contrast to wide area network wan include higher data transfer rates limited geographic range and lack of reliance on leased lines to provide connectivity current ethernet or other ieee lan technologies operate at data transfer rates up to gbit the ieee investigates the standardization of and gbit rates lan can be connected to wan using router home area network home area network han is residential lan used for communication between digital devices typically deployed in the home usually small number of personal computers and accessories such as printers and mobile computing devices an important function is the sharing of internet access often broadband service through cable tv or digital subscriber line dsl provider storage area network storage area network san is dedicated network that provides access to consolidated block level data storage sans are primarily used to make storage devices such as disk arrays tape libraries and optical jukeboxes accessible to servers so that the devices appear like locally attached devices to the operating system san typically has its own network of storage devices that are generally not accessible through the local area network by other devices the cost and complexity of sans dropped in the early to levels allowing wider adoption across both enterprise and small to medium sized business environments campus area network campus area network can is made up of an interconnection of lans within limited geographical area the networking equipment switches routers and transmission media optical fiber copper plant cat cabling etc are almost entirely owned by the campus tenant owner an enterprise university government etc for example university campus network is likely to link variety of campus buildings to connect academic colleges or departments the library and student residence halls backbone network backbone network is part of computer network infrastructure that provides path for the exchange of information between different lans or sub networks backbone can tie together diverse networks within the same building across different buildings or over wide area for example large company might implement backbone network to connect departments that are located around the world the equipment that ties together the departmental networks constitutes the network backbone when designing network backbone network performance and network congestion are critical factors to take into account normally the backbone network capacity is greater than that of the individual networks connected to it another example of backbone network is the internet backbone which is the set of wide area networks wans and core routers that tie together all networks connected to the internet metropolitan area network metropolitan area network man is large computer network that usually spans city or large campus wide area network wide area network wan is computer network that covers large geographic area such as city country or spans even distances wan uses communications channel that combines many types of media such as telephone lines cables and air waves wan often makes use of transmission facilities provided by common carriers such as telephone companies wan technologies generally function at the lower three layers of the osi reference model the physical layer the data link layer and the network layer enterprise private network an enterprise private network is network that single organization builds to interconnect its office locations production sites head offices remote offices shops so they can share computer resources virtual private network virtual private network vpn is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network the internet instead of by physical wires the data link layer protocols of the virtual network are said to be tunneled through the larger network when this is the case one common application is secure communications through the public internet but vpn need not have explicit security features such as authentication or content encryption vpns for example can be used to separate the traffic of different user communities over an underlying network with strong security features vpn may have best effort performance or may have defined service level agreement sla between the vpn customer and the vpn service provider generally vpn has topology more complex than point to point global area network global area network gan is network used for supporting mobile across an arbitrary number of wireless lans satellite coverage areas etc the key challenge in mobile communications is handing off user communications from one local coverage area to the next in ieee project this involves succession of terrestrial wireless lans organizational scope networks are typically managed by the organizations that own them private enterprise networks may use combination of intranets and extranets they may also provide network access to the internet which has no single owner and permits virtually unlimited global connectivity intranets an intranet is set of networks that are under the control of single administrative entity the intranet uses the ip protocol and ip based tools such as web browsers and file transfer applications the administrative entity limits use of the intranet to its authorized users most commonly an intranet is the internal lan of an organization large intranet typically has at least one web server to provide users with organizational information an intranet is also anything behind the router on local area network extranet an extranet is network that is also under the administrative control of single organization but supports limited connection to specific external network for example an organization may provide access to some aspects of its intranet to share data with its business partners or customers these other entities are not necessarily trusted from security standpoint network connection to an extranet is often but not always implemented via wan technology internetwork an internetwork is the connection of multiple computer networks via common routing technology using routers internet file internet map jpg thumb right partial map of the internet based on the january data found on opte org each line is drawn between two nodes representing two ip addresses the length of the lines are indicative of the delay between those two nodes this graph represents less than of the class networks reachable the internet is the largest example of an internetwork it is global system of interconnected governmental academic corporate public and private computer networks it is based on the networking technologies of the internet protocol suite it is the successor of the advanced research projects agency network arpanet developed by darpa of the united states department of defense the internet is also the communications backbone underlying the world wide web www participants in the internet use diverse array of methods of several hundred documented and often standardized protocols compatible with the internet protocol suite and an addressing system ip addresses administered by the internet assigned numbers authority and address registries service providers and large enterprises exchange information about the reachability of their address spaces through the border gateway protocol bgp forming redundant worldwide mesh of transmission paths darknet darknet is an overlay network typically running on the internet that is only accessible through specialized software darknet is an anonymizing network where connections are made only between trusted peers sometimes called friends using non standard protocols and ports darknets are distinct from other distributed peer to peer networks as sharing is anonymous that is ip addresses are not publicly shared and therefore users can communicate with little fear of governmental or corporate interference routing routing calculates good paths through network for information to take for example from node to node the best routes are likely to be or as this has the thickest routes routing is the process of selecting network paths to carry network traffic routing is performed for many kinds of networks including circuit switching networks and packet switched networks in packet switched networks routing directs packet forwarding the transit of logically addressed network packets from their source toward their ultimate destination through intermediate nodes intermediate nodes are typically network hardware devices such as routers bridges gateways firewalls or switches general purpose computers can also forward packets and perform routing though they are not specialized hardware and may suffer from limited performance the routing process usually directs forwarding on the basis of routing tables which maintain record of the routes to various network destinations thus constructing routing tables which are held in the router memory is very important for efficient routing most routing algorithms use only one network path at time multipath routing techniques enable the use of multiple alternative paths there are usually multiple routes that can be taken and to choose between them different elements can be considered to decide which routes get installed into the routing table such as sorted by priority prefix length where longer subnet masks are preferred independent if it is within routing protocol or over different routing protocol metric where lower metric cost is preferred only valid within one and the same routing protocol administrative distance where lower distance is preferred only valid between different routing protocols routing in more narrow sense of the term is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network structured addresses allow single routing table entry to represent the route to group of devices in large networks structured addressing routing in the narrow sense outperforms unstructured addressing bridging routing has become the dominant form of addressing on the internet bridging is still widely used within localized environments network service network services are applications hosted by servers on computer network to provide some functionality for members or users of the network or to help the network itself to operate the world wide web mail printing and network file sharing are examples of well known network services network services such as dns domain name system give names for ip and mac addresses people remember names like nm lan better than numbers like and dhcp to ensure that the equipment on the network has valid ip address services are usually based on service protocol that defines the format and sequencing of messages between clients and servers of that network service network performance quality of service depending on the installation requirements network performance is usually measured by the quality of service of product the parameters that affect this typically can include throughput jitter bit error rate and latency the following list gives examples of network performance measures for circuit switched network and one type of packet switched network viz atm circuit switched networks in circuit switched networks network performance is synonymous with the grade of service the number of rejected calls is measure of how well the network is performing under heavy traffic loads other types of performance measures can include the level of noise and echo atm in an asynchronous transfer mode atm network performance can be measured by line rate quality of service qos data throughput connect time stability technology modulation technique and modem enhancements there are many ways to measure the performance of network as each network is different in nature and design performance can also be modelled instead of measured for example state transition diagrams are often used to model queuing performance in circuit switched network the network planner uses these diagrams to analyze how the network performs in each state ensuring that the network is optimally designed network congestion network congestion occurs when link or node is carrying so much data that its quality of service deteriorates typical effects include queueing delay packet loss or the blocking of new connections consequence of these latter two is that incremental increases in offered load lead either only to small increase in network throughput or to an actual reduction in network throughput network protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in state of network congestion even after the initial load is reduced to level that would not normally induce network congestion thus networks using these protocols can exhibit two stable states under the same level of load the stable state with low throughput is known as congestive collapse modern networks use congestion control and congestion avoidance techniques to try to avoid congestion collapse these include exponential backoff in protocols such as csma ca and the original ethernet window reduction in tcp and fair queueing in devices such as routers another method to avoid the negative effects of network congestion is implementing priority schemes so that some packets are transmitted with higher priority than others priority schemes do not solve network congestion by themselves but they help to alleviate the effects of congestion for some services an example of this is third method to avoid network congestion is the explicit allocation of network resources to specific flows one example of this is the use of contention free transmission opportunities cftxops in the itu hn standard which provides high speed up to gbit local area networking over existing home wires power lines phone lines and coaxial cables for the internet rfc addresses the subject of congestion control in detail network resilience network resilience is the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation security network security network security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access misuse modification or denial of the computer network and its network accessible resources network security is the authorization of access to data in network which is controlled by the network administrator users are assigned an id and password that allows them access to information and programs within their authority network security is used on variety of computer networks both public and private to secure daily transactions and communications among businesses government agencies and individuals network surveillance network surveillance is the monitoring of data being transferred over computer networks such as the internet the monitoring is often done surreptitiously and may be done by or at the behest of governments by corporations criminal organizations or individuals it may or may not be legal and may or may not require authorization from court or other independent agency computer and network surveillance programs are widespread today and almost all internet traffic is or could potentially be monitored for clues to illegal activity surveillance is very useful to governments and law enforcement to maintain social control recognize and monitor threats and prevent investigate criminal activity with the advent of programs such as the total information awareness program technologies such as high speed surveillance computers and biometrics software and laws such as the communications assistance for law enforcement act governments now possess an unprecedented ability to monitor the activities of citizens however many civil rights and privacy groups such as reporters without borders the electronic frontier foundation and the american civil liberties union have expressed concern that increasing surveillance of citizens may lead to mass surveillance society with limited political and personal freedoms fears such as this have led to numerous lawsuits such as hepting at the hacktivist group anonymous has hacked into government websites in protest of what it considers draconian surveillance end to end encryption end to end encryption ee is digital communications paradigm of uninterrupted protection of data traveling between two communicating parties it involves the originating party encrypting data so only the intended recipient can decrypt it with no dependency on third parties end to end encryption prevents intermediaries such as internet providers or application service providers from discovering or tampering with communications end to end encryption generally protects both confidentiality and integrity examples of end to end encryption include pgp for email otr for instant messaging zrtp for telephony and tetra for radio typical server based communications systems do not include end to end encryption these systems can only guarantee protection of communications between clients and servers not between the communicating parties themselves examples of non ee systems are google talk yahoo messenger facebook and dropbox some such systems for example lavabit and secretink have even described themselves as offering end to end encryption when they do not some systems that normally offer end to end encryption have turned out to contain back door that subverts negotiation of the encryption key between the communicating parties for example skype or hushmail the end to end encryption paradigm does not directly address risks at the communications endpoints themselves such as the technical exploitation of clients poor quality random number generators or key escrow ee also does not address traffic analysis which relates to things such as the identities of the end points and the times and quantities of messages that are sent views of networks users and network administrators typically have different views of their networks users can share printers and some servers from workgroup which usually means they are in the same geographic location and are on the same lan whereas network administrator is responsible to keep that network up and running community of interest has less of connection of being in local area and should be thought of as set of arbitrarily located users who share set of servers and possibly also communicate via peer to peer technologies network administrators can see networks from both physical and logical perspectives the physical perspective involves geographic locations physical cabling and the network elements routers bridges and application layer gateways that interconnect via the transmission media logical networks called in the tcp ip architecture subnets map onto one or more transmission media for example common practice in campus of buildings is to make set of lan cables in each building appear to be common subnet using virtual lan vlan technology both users and administrators are aware to varying extents of the trust and scope characteristics of network again using tcp ip architectural terminology an intranet is community of interest under private administration usually by an enterprise and is only accessible by authorized users employees intranets do not have to be connected to the internet but generally have limited connection an extranet is an extension of an intranet that allows secure communications to users outside of the intranet business partners customers unofficially the internet is the set of users enterprises and content providers that are interconnected by internet service providers isp from an engineering viewpoint the internet is the set of subnets and aggregates of subnets which share the registered ip address space and exchange information about the reachability of those ip addresses using the border gateway protocol typically the human readable names of servers are translated to ip addresses transparently to users via the directory function of the domain name system dns over the internet there can be business to business business to consumer and consumer to consumer communications when money or sensitive information is exchanged the communications are apt to be protected by some form of communications security mechanism intranets and extranets can be securely superimposed onto the internet without any access by general internet users and administrators using secure virtual private network vpn technology see also comparison of network diagram software cyberspace history of the internet network simulation network planning and design references further reading shelly gary et al discovering computers edition wendell odom rus healy denise donohue ccie routing and switching indianapolis in cisco press kurose james and keith ross computer networking top down approach featuring the internet pearson education william stallings computer networking with internet protocols and technology pearson education important publications in computer networks network communication architecture and protocols osi network architecture layers model dimitri bertsekas and robert gallager data networks prentice hall external links ieee ethernet manufacturer information", "Multimedia": "multimedia refers to content that uses combination of different content forms this contrasts with media that use only rudimentary computer displays such as text only or traditional forms of printed or hand produced material multimedia includes combination of text audio still images animation video or interactive content forms multimedia can be recorded and played displayed dynamic interacted with or accessed by information content processing devices such as computerized and electronic devices but can also be part of live performance multimedia devices are electronic media devices used to store and experience multimedia content multimedia is distinguished from mixed media in fine art by including audio for example it has broader scope the term rich media is synonymous for interactive multimedia hypermedia scales up the amount of media content in multimedia application examples of individual content forms combined in multimedia link writing link sound recording and reproduction link image text audio still images link animation link footage link interactivity animation video footage interactivity categorization of multimedia multimedia may be broadly divided into linear and non linear categories linear active content progresses often without any navigational control for the viewer such as cinema presentation non linear uses interactivity to control progress as with video game or self paced computer based training hypermedia is an example of non linear content multimedia presentations can be live or recorded recorded presentation may allow interactivity via navigation system live multimedia presentation may allow interactivity via an interaction with the presenter or performer major characteristics of multimedia multimedia presentations may be viewed by person on stage projected transmitted or played locally with media player broadcast may be live or recorded multimedia presentation broadcasts and recordings can be either analog or digital electronic media technology digital online multimedia may be downloaded or streamed streaming multimedia may be live or on demand multimedia games and simulations may be used in physical environment with special effects with multiple users in an online network or locally with an offline computer game system or simulator the various formats of technological or digital multimedia may be intended to enhance the users experience for example to make it easier and faster to convey information or in entertainment or art to transcend everyday experience lasershow is live multimedia performance enhanced levels of interactivity are made possible by combining multiple forms of media content online multimedia is increasingly becoming object oriented and data driven enabling applications with collaborative end user innovation and personalization on multiple forms of content over time examples of these range from multiple forms of content on web sites like photo galleries with both images pictures and title text user updated to simulations whose co efficients events illustrations animations or videos are modifiable allowing the multimedia experience to be altered without reprogramming in addition to seeing and hearing haptic technology enables virtual objects to be felt emerging technology involving illusions of taste and smell may also enhance the multimedia experience terminology history of the term the term multimedia was coined by singer and artist bob goldstein later bobb goldsteinn to promote the july opening of his lightworks at oursin show at southampton long island goldstein was perhaps aware of an american artist named dick higgins who had two years previously discussed new approach to art making he called intermedia on august richard albarino of variety borrowed the terminology reporting brainchild of songscribe comic bob washington square goldstein the lightworks is the latest multi media music cum visuals to debut as discoth\u00e8que fare two years later in the term multimedia was re appropriated to describe the work of political consultant david sawyer the husband of iris sawyer one of goldstein producers at oursin multimedia multi image setup for the ford new car announcement show august detroit mi in the intervening forty years the word has taken on different meanings in the late the term referred to presentations consisting of multi projector slide shows timed to an audio track however by the multimedia took on its current meaning in the first edition of mcgraw hill multimedia making it work tay vaughan declared multimedia is any combination of text graphic art sound animation and video that is delivered by computer when you allow the user the viewer of the project to control what and when these elements are delivered it is interactive multimedia when you provide structure of linked elements through which the user can navigate interactive multimedia becomes hypermedia the german language society gesellschaft f\u00fcr deutsche sprache decided to recognize the word significance and ubiquitousness in the by awarding it the title of word of the year in the institute summed up its rationale by stating multimedia has become central word in the wonderful new media world in common usage multimedia refers to an electronically delivered combination of media including video still images audio text in such way that can be accessed interactively much of the content on the web today falls within this definition as understood by millions some computers which were marketed in the were called multimedia computers because they incorporated cd rom drive which allowed for the delivery of several hundred megabytes of video picture and audio data that era saw also boost in the production of educational multimedia cd roms word usage and context since media is the plural of medium the term multimedia is used to describe multiple occurrences of only one form of media such as collection of audio cds this is why it important that the word multimedia is used exclusively to describe multiple forms of media and content the term multimedia is also ambiguous static content such as paper book may be considered multimedia if it contains both pictures and text or may be considered interactive if the user interacts by turning pages at will books may also be considered non linear if the pages are accessed non sequentially the term video if not used exclusively to describe motion photography is ambiguous in multimedia terminology video is often used to describe the file format delivery format or presentation format instead of footage which is used to distinguish motion photography from animation of rendered motion imagery multiple forms of information content are often not considered modern forms of presentation such as audio or video likewise single forms of information content with single methods of information processing non interactive audio are often called multimedia perhaps to distinguish static media from active media in the fine arts for example leda luss luyken modulart brings two key elements of musical composition and film into the world of painting variation of theme and movement of and within picture making modulart an interactive multimedia form of art performing arts may also be considered multimedia considering that performers and props are multiple forms of both content and media the gesellschaft f\u00fcr deutsche sprache chose multimedia as german word of the year usage application powerpoint corporate presentations may combine all forms of media content virtual reality uses multimedia content applications and delivery platforms of multimedia are virtually limitless vvo multimedia terminal in dresden wtc germany multimedia finds its application in various areas including but not limited to advertisements art education entertainment engineering medicine mathematics business scientific research and spatial temporal applications several examples are as follows creative industries creative industries use multimedia for variety of purposes ranging from fine arts to entertainment to commercial art to journalism to media and software services provided for any of the industries listed below an individual multimedia designer may cover the spectrum throughout their career request for their skills range from technical to analytical to creative commercial uses much of the electronic old and new media used by commercial artists and graphic designers is multimedia exciting presentations are used to grab and keep attention in advertising business to business and interoffice communications are often developed by creative services firms for advanced multimedia presentations beyond simple slide shows to sell ideas or liven up training commercial multimedia developers may be hired to design for governmental services and nonprofit services applications as well entertainment and fine arts in addition multimedia is heavily used in the entertainment industry especially to develop special effects in movies and animations vfx animation etc multimedia games are popular pastime and are software programs available either as cd roms or online some video games also use multimedia features multimedia applications that allow users to actively participate instead of just sitting by as passive recipients of information are called interactive multimedia in the arts there are multimedia artists whose minds are able to blend techniques using different media that in some way incorporates interaction with the viewer one of the most relevant could be peter greenaway who is melding cinema with opera and all sorts of digital media another approach entails the creation of multimedia that can be displayed in traditional fine arts arena such as an art gallery although multimedia display material may be volatile the survivability of the content is as strong as any traditional media digital recording material may be just as durable and infinitely reproducible with perfect copies every time education in education multimedia is used to produce computer based training courses popularly called cbts and reference books like encyclopedia and almanacs cbt lets the user go through series of presentations text about particular topic and associated illustrations in various information formats edutainment is the combination of education with entertainment especially multimedia entertainment learning theory in the past decade has expanded dramatically because of the introduction of multimedia several lines of research have evolved cognitive load multimedia learning and the list goes on the possibilities for learning and instruction are nearly endless the idea of media convergence is also becoming major factor in education particularly higher education defined as separate technologies such as voice and telephony features data and productivity applications and video that now share resources and interact with each other synergistically creating new efficiencies media convergence is rapidly changing the curriculum in universities all over the world likewise it is changing the availability or lack thereof of jobs requiring this savvy technological skill the english education in middle school in china is well invested and assisted with various equipments in contrast the original objective has not been achieved at the desired effect the government schools families and students spend lot of time working on improving scores but hardly gain practical skills english education today has gone into the vicious circle educators need to consider how to perfect the education system to improve students practical ability of english therefore an efficient way should be used to make the class vivid multimedia teaching will bring students into class where they can interact with the teacher and the subject multimedia teaching is more intuitive than old ways teachers can simulate situations in real life in many circumstances teachers do not have to be there students will learn by themselves in the class more importantly teachers will have more approaches to stimulating students passion of learning journalism newspaper companies all over are also trying to embrace the new phenomenon by implementing its practices in their work while some have been slow to come around other major newspapers like the new york times usa today and the washington post are setting the precedent for the positioning of the newspaper industry in globalized world news reporting is not limited to traditional media outlets freelance journalists can make use of different new media to produce multimedia pieces for their news stories it engages global audiences and tells stories with technology which develops new communication techniques for both media producers and consumers the common language project later renamed to the seattle globalist is an example of this type of multimedia journalism production multimedia reporters who are mobile usually driving around community with cameras audio and video recorders and laptop computers are often referred to as mojos from mo bile jo urnalist engineering software engineers may use multimedia in computer simulations for anything from entertainment to training such as military or industrial training multimedia for software interfaces are often done as collaboration between creative professionals and software engineers industry in the industrial sector multimedia is used as way to help present information to shareholders superiors and coworkers multimedia is also helpful for providing employee training advertising and selling products all over the world via virtually unlimited web based technology mathematical and scientific research in mathematical and scientific research multimedia is mainly used for modeling and simulation for example scientist can look at molecular model of particular substance and manipulate it to arrive at new substance representative research can be found in journals such as the journal of multimedia medicine in medicine doctors can get trained by looking at virtual surgery or they can simulate how the human body is affected by diseases spread by viruses and bacteria and then develop techniques to prevent it multimedia application like virtual surgeries also help doctors to get practical training document imaging document imaging is technique that takes hard copy of an image document and converts it into digital format for example scanners disabilities ability media allows those with disabilities to gain qualifications in the multimedia field so they can pursue careers that give them access to wide array of powerful communication forms miscellaneous in europe the reference organisation for multimedia industry is the european multimedia associations convention emmac structuring information in multimedia form multimedia represents the convergence of text pictures video and sound into single form the power of multimedia and the internet lies in the way in which information is linked multimedia and the internet require completely new approach to writing the style of writing that is appropriate for the on line world is highly optimized and designed to be able to be quickly scanned by readers good site must be made with specific purpose in mind and site with good interactivity and new technology can also be useful for attracting visitors the site must be attractive and innovative in its design function in terms of its purpose easy to navigate frequently updated and fast to download when users view page they can only view one page at time as result multimedia users must create mental model of information structure conferences there is large number of multimedia conferences the two main scholarly scientific conferences being acm multimedia ieee icme international conference on multimedia expo see also artmedia cross media multi image multimedia literacy multimedia messaging service multimedia search new media art postliterate society web documentary references external links history of multimedia from the university of calgary multimedia in answers com", "Computer vision": "computer vision is field that includes methods for acquiring processing analyzing and understanding images and in general high dimensional data from the real world in order to produce numerical or symbolic information in the forms of decisions theme in the development of this field has been to duplicate the abilities of human vision by electronically perceiving and understanding an image this image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry physics statistics and learning theory computer vision has also been described as the enterprise of automating and integrating wide range of processes and representations for vision perception as scientific discipline computer vision is concerned with the theory behind artificial systems that extract information from images the image data can take many forms such as video sequences views from multiple cameras or multi dimensional data from medical scanner as technological discipline computer vision seeks to apply its theories and models to the construction of computer vision systems sub domains of computer vision include scene reconstruction event detection video tracking object recognition object pose estimation learning indexing motion estimation and image restoration related fields relation between computer vision and various other fields areas of artificial intelligence deal with autonomous planning or deliberation for robotical systems to navigate through an environment detailed understanding of these environments is required to navigate through them information about the environment could be provided by computer vision system acting as vision sensor and providing high level information about the environment and the robot artificial intelligence and computer vision share other topics such as pattern recognition and learning techniques consequently computer vision is sometimes seen as part of the artificial intelligence field or the computer science field in general solid state physics is another field that is closely related to computer vision most computer vision systems rely on image sensors which detect electromagnetic radiation which is typically in the form of either visible or infra red light the sensors are designed using quantum physics the process by which light interacts with surfaces is explained using physics physics explains the behavior of optics which are core part of most imaging systems sophisticated image sensors even require quantum mechanics to provide complete understanding of the image formation process also various measurement problems in physics can be addressed using computer vision for example motion in fluids third field which plays an important role is neurobiology specifically the study of the biological vision system over the last century there has been an extensive study of eyes neurons and the brain structures devoted to processing of visual stimuli in both humans and various animals this has led to coarse yet complicated description of how real vision systems operate in order to solve certain vision related tasks these results have led to subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity also some of the learning based methods developed within computer vision neural net and deep learning based image and feature analysis and classification have their background in biology some strands of computer vision research are closely related to the study of biological vision indeed just as many strands of ai research are closely tied with research into human consciousness and the use of stored knowledge to interpret integrate and utilize visual information the field of biological vision studies and models the physiological processes behind visual perception in humans and other animals computer vision on the other hand studies and describes the processes implemented in software and hardware behind artificial vision systems exchange between biological and computer vision has proven fruitful for both fields yet another field related to computer vision is signal processing many methods for processing of one variable signals typically temporal signals can be extended in natural way to processing of two variable signals or multi variable signals in computer vision however because of the specific nature of images there are many methods developed within computer vision which have no counterpart in processing of one variable signals together with the multi dimensionality of the signal this defines subfield in signal processing as part of computer vision beside the above mentioned views on computer vision many of the related research topics can also be studied from purely mathematical point of view for example many methods in computer vision are based on statistics optimization or geometry finally significant part of the field is devoted to the implementation aspect of computer vision how existing methods can be realized in various combinations of software and hardware or how these methods can be modified in order to gain processing speed without losing too much performance the fields most closely related to computer vision are image processing image analysis and machine vision there is significant overlap in the range of techniques and applications that these cover this implies that the basic techniques that are used and developed in these fields are more or less identical something which can be interpreted as there is only one field with different names on the other hand it appears to be necessary for research groups scientific journals conferences and companies to present or market themselves as belonging specifically to one of these fields and hence various which distinguish each of the fields from the others have been presented computer vision is in some ways the inverse of computer graphics while computer graphics produces image data from models computer vision often produces models from image data there is also trend towards combination of the two disciplines as explored in augmented reality the following appear relevant but should not be taken as universally accepted image processing and image analysis tend to focus on images how to transform one image to another by pixel wise operations such as contrast enhancement local operations such as edge extraction or noise removal or geometrical transformations such as rotating the image this implies that image processing analysis neither require assumptions nor produce interpretations about the image content computer vision includes analysis from images this analyzes the scene projected onto one or several images how to reconstruct structure or other information about the scene from one or several images computer vision often relies on more or less complex assumptions about the scene depicted in an image machine vision is the process of applying range of technologies methods to provide imaging based automatic inspection process control and robot guidance in industrial applications machine vision tends to focus on applications mainly in manufacturing vision based autonomous robots and systems for vision based inspection or measurement this implies that image sensor technologies and control theory often are integrated with the processing of image data to control robot and that real time processing is emphasised by means of efficient implementations in hardware and software it also implies that the external conditions such as lighting can be and are often more controlled in machine vision than they are in general computer vision which can enable the use of different algorithms there is also field called imaging which primarily focus on the process of producing images but sometimes also deals with processing and analysis of images for example medical imaging includes substantial work on the analysis of image data in medical applications finally pattern recognition is field which uses various methods to extract information from signals in general mainly based on statistical approaches and artificial neural networks significant part of this field is devoted to applying these methods to image data photogrammetry also overlaps with computer vision vs stereo computer vision applications for computer vision applications range from tasks such as industrial machine vision systems which say inspect bottles speeding by on production line to research into artificial intelligence and computers or robots that can comprehend the world around them the computer vision and machine vision fields have significant overlap computer vision covers the core technology of automated image analysis which is used in many fields machine vision usually refers to process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications in many computer vision applications the computers are pre programmed to solve particular task but methods based on learning are now becoming increasingly common examples of applications of computer vision include systems for controlling processes an industrial robot navigation by an autonomous vehicle or mobile robot detecting events for visual surveillance or people counting organizing information for indexing databases of images and image sequences modeling objects or environments medical image analysis or topographical modeling interaction as the input to device for computer human interaction and automatic inspection in manufacturing applications darpa visual media reasoning concept video one of the most prominent application fields is medical computer vision or medical image processing this area is characterized by the extraction of information from image data for the purpose of making medical diagnosis of patient generally image data is in the form of microscopy images ray images angiography images ultrasonic images and tomography images an example of information which can be extracted from such image data is detection of tumours or other malign changes it can also be measurements of organ dimensions blood flow etc this application area also supports medical research by providing new information about the structure of the brain or about the quality of medical treatments applications of computer vision in the medical area also includes enhancement of images that are interpreted by humans for example ultrasonic images or ray images to reduce the influence of noise second application area in computer vision is in industry sometimes called machine vision where information is extracted for the purpose of supporting manufacturing process one example is quality control where details or final products are being automatically inspected in order to find defects another example is measurement of position and orientation of details to be picked up by robot arm machine vision is also heavily used in agricultural process to remove undesirable food stuff from bulk material process called optical sorting military applications are probably one of the largest areas for computer vision the obvious examples are detection of enemy soldiers or vehicles and missile guidance more advanced systems for missile guidance send the missile to an area rather than specific target and target selection is made when the missile reaches the area based on locally acquired image data modern military concepts such as battlefield awareness imply that various sensors including image sensors provide rich set of information about combat scene which can be used to support strategic decisions in this case automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability artist concept of rover on mars an example of an unmanned land based vehicle notice the stereo cameras mounted on top of the rover one of the newer application areas is autonomous vehicles which include submersibles land based vehicles small robots with wheels cars or trucks aerial vehicles and unmanned aerial vehicles uav the level of autonomy ranges from fully autonomous unmanned vehicles to vehicles where computer vision based systems support driver or pilot in various situations fully autonomous vehicles typically use computer vision for navigation for knowing where it is or for producing map of its environment slam and for detecting obstacles it can also be used for detecting certain task specific events uav looking for forest fires examples of supporting systems are obstacle warning systems in cars and systems for autonomous landing of aircraft several car manufacturers have demonstrated systems for autonomous driving of cars but this technology has still not reached level where it can be put on the market there are ample examples of military autonomous vehicles ranging from advanced missiles to uavs for recon missions or missile guidance space exploration is already being made with autonomous vehicles using computer vision nasa mars exploration rover and esa exomars rover other application areas include support of visual effects creation for cinema and broadcast camera tracking matchmoving surveillance typical tasks of computer vision each of the application areas described above employ range of computer vision tasks more or less well defined measurement problems or processing problems which can be solved using variety of methods some examples of typical computer vision tasks are presented below recognition the classical problem in computer vision image processing and machine vision is that of determining whether or not the image data contains some specific object feature or activity different varieties of the recognition problem are described in the literature object recognition also called object classification one or several pre specified or learned objects or object classes can be recognized usually together with their positions in the image or poses in the scene google goggles or likethat provide stand alone programs that illustrate this function identification an individual instance of an object is recognized examples include identification of specific person face or fingerprint identification of handwritten digits or identification of specific vehicle detection the image data are scanned for specific condition examples include detection of possible abnormal cells or tissues in medical images or detection of vehicle in an automatic road toll system detection based on relatively simple and fast computations is sometimes used for finding smaller regions of interesting image data which can be further analyzed by more computationally demanding techniques to produce correct interpretation currently the best algorithms for such tasks are based on convolutional neural networks an illustration of their capabilities is given by the imagenet large scale visual recognition challenge this is benchmark in object classification and detection with millions of images and hundreds of object classes performance of convolutional neural networks on the imagenet tests is now close to that of humans the best algorithms still struggle with objects that are small or thin such as small ant on stem of flower or person holding quill in their hand they also have trouble with images that have been distorted with filters an increasingly common phenomenon with modern digital cameras by contrast those kinds of images rarely trouble humans humans however tend to have trouble with other issues for example they are not good at classifying objects into fine grained classes such as the particular breed of dog or species of bird whereas convolutional neural networks handle this with ease several specialized tasks based on recognition exist such as content based image retrieval finding all images in larger set of images which have specific content the content can be specified in different ways for example in terms of similarity relative target image give me all images similar to image or in terms of high level search criteria given as text input give me all images which contains many houses are taken during winter and have no cars in them computer vision for people counter purposes in public places malls shopping centres pose estimation estimating the position or orientation of specific object relative to the camera an example application for this technique would be assisting robot arm in retrieving objects from conveyor belt in an assembly line situation or picking parts from bin optical character recognition ocr identifying characters in images of printed or handwritten text usually with view to encoding the text in format more amenable to editing or indexing ascii code reading reading of codes such as data matrix and qr codes facial recognition shape recognition technology srt in people counter systems differentiating human beings head and shoulder patterns from objects motion analysis several tasks relate to motion estimation where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the scene or even of the camera that produces the images examples of such tasks are egomotion determining the rigid motion rotation and translation of the camera from an image sequence produced by the camera tracking following the movements of usually smaller set of interest points or objects vehicles or humans in the image sequence optical flow to determine for each point in the image how that point is moving relative to the image plane its apparent motion this motion is result both of how the corresponding point is moving in the scene and how the camera is moving relative to the scene scene reconstruction given one or typically more images of scene or video scene reconstruction aims at computing model of the scene in the simplest case the model can be set of points more sophisticated methods produce complete surface model the advent of imaging not requiring motion or scanning and related processing algorithms is enabling rapid advances in this field grid based sensing can be used to acquire images from multiple angles algorithms are now available to stitch multiple images together into point clouds and models image restoration the aim of image restoration is the removal of noise sensor noise motion blur etc from images the simplest possible approach for noise removal is various types of filters such as low pass filters or median filters more sophisticated methods assume model of how the local image structures look like model which distinguishes them from the noise by first analysing the image data in terms of the local image structures such as lines or edges and then controlling the filtering based on local information from the analysis step better level of noise removal is usually obtained compared to the simpler approaches an example in this field is inpainting computer vision system methods the organization of computer vision system is highly application dependent some systems are stand alone applications which solve specific measurement or detection problem while others constitute sub system of larger design which for example also contains sub systems for control of mechanical actuators planning information databases man machine interfaces etc the specific implementation of computer vision system also depends on if its functionality is pre specified or if some part of it can be learned or modified during operation many functions are unique to the application there are however typical functions which are found in many computer vision systems image acquisition digital image is produced by one or several image sensors which besides various types of light sensitive cameras include range sensors tomography devices radar ultra sonic cameras etc depending on the type of sensor the resulting image data is an ordinary image volume or an image sequence the pixel values typically correspond to light intensity in one or several spectral bands gray images or colour images but can also be related to various physical measures such as depth absorption or reflectance of sonic or electromagnetic waves or nuclear magnetic resonance pre processing before computer vision method can be applied to image data in order to extract some specific piece of information it is usually necessary to process the data in order to assure that it satisfies certain assumptions implied by the method examples are re sampling in order to assure that the image coordinate system is correct noise reduction in order to assure that sensor noise does not introduce false information contrast enhancement to assure that relevant information can be detected scale space representation to enhance image structures at locally appropriate scales feature extraction image features at various levels of complexity are extracted from the image data typical examples of such features are lines edges and ridges localized interest points such as corners blobs or points more complex features may be related to texture shape or motion detection segmentation at some point in the processing decision is made about which image points or regions of the image are relevant for further processing examples are selection of specific set of interest points segmentation of one or multiple image regions which contain specific object of interest high level processing at this step the input is typically small set of data for example set of points or an image region which is assumed to contain specific object the remaining processing deals with for example verification that the data satisfy model based and application specific assumptions estimation of application specific parameters such as object pose or object size image detected object into different categories image and combining two different views of the same object decision making making the final decision required for the application for example pass fail on automatic inspection applications match no match in recognition applications flag for further human review in medical military security and recognition applications computer vision hardware there are many kinds of computer vision systems nevertheless all of them contain these basic elements power source at least one image acquisition device camera ccd etc processor as well as control and communication cables or some kind of wireless interconnection mechanism in addition practical vision system contains software as well as display in order to monitor the system vision systems for inner spaces as most industrial ones contain an illumination system and may be placed in controlled environment furthermore completed system includes many accessories like camera supports cables and connectors see also ai effect applications of artificial intelligence machine vision glossary artificial neural networks deep learning lists list of computer vision topics list of emerging technologies outline of artificial intelligence references further reading external links usc iris computer vision conference list computer vision papers on the web complete list of papers of the most relevant computer vision conferences computer vision online news source code datasets and job offers related to computer vision keith price annotated computer vision bibliography cvonline bob fisher compendium of computer vision british machine vision association supporting computer vision research within the uk via the bmvc and miua conferences annals of the bmva open source journal bmva summer school and one day meetings", "Hardware architecture": "an projected diagram of the nighthawk an conducts live exercise bombing run using gbu laser guided bombs in engineering hardware architecture refers to the identification of system physical components and their this description often called hardware design model allows hardware designers to understand how their components fit into system architecture and provides to software component designers important information needed for software development and integration clear definition of hardware architecture allows the various traditional engineering disciplines electrical and mechanical engineering to work more effectively together to develop and manufacture new machines devices and components hardware is also an expression used within the computer engineering industry to explicitly distinguish the electronic computer hardware from the software that runs on it but hardware within the automation and software engineering disciplines need not simply be computer of some sort modern automobile runs vastly more software than the apollo spacecraft also modern aircraft cannot function without running tens of millions of computer instructions embedded and distributed throughout the aircraft and resident in both standard computer hardware and in specialized hardward components such as ic wired logic gates analog and hybrid devices and other digital components the need to effectively model how separate physical components combine to form complex systems is important over wide range of applications including computers personal digital assistants pdas cell phones surgical instrumentation satellites and submarines hardware architecture is the representation of an engineered or to be engineered electronic or hardware system and the process and discipline for effectively implementing the design for such system it is generally part of larger integrated system encompassing information software and device prototyping it is representation because it is used to convey information about the related elements comprising hardware system the relationships among those elements and the rules governing those relationships electric multi turn actuator with controls it is process because sequence of steps is prescribed to produce or change the architecture and or design from that architecture of hardware system within set of constraints it is discipline because body of knowledge is used to inform practitioners as to the most effective way to design the system within set of constraints hardware architecture is primarily concerned with the internal electrical and more rarely the mechanical interfaces among the system components or subsystems and the interface between the system and its external environment especially the devices operated by or the electronic displays viewed by user this latter special interface is known as the computer human interface aka human computer interface or hci formerly called the man machine interface integrated circuit ic designers are driving current technologies into innovative approaches for new products hence multiple layers of active devices are being proposed as single chip opening up opportunities for disruptive microelectronic optoelectronic and new hardware implementation background hardware architecture example which is integrated as handheld medical device for diabetes monitoring boat layout with detailed equipment hardware specification and functionality prior to the advent of digital computers the electronics and other engineering disciplines used the terms system and hardware as they are still commonly used today however with the arrival of digital computers on the scene and the development of software engineering as separate discipline it was often necessary to distinguish among engineered hardware artifacts software artifacts and the combined artifacts programmable hardware artifact or machine that lacks its software program is impotent even as software artifact or program is equally impotent unless it can be used to alter the sequential states of suitable hardware machine however hardware machine and its software program can be designed to perform an almost illimitable number of abstract and physical tasks within the computer and software engineering disciplines and often other engineering disciplines such as communications then the terms hardware software and system came to distinguish between the hardware that runs software program the software and the hardware device complete with its program the hardware engineer or architect deals more or less exclusively with the hardware device the software engineer or architect deals more or less exclusively with the software program and the systems engineer or systems architect is responsible for seeing that the software program is capable of properly running within the hardware device and that the system composed of the two entities is capable of properly interacting with its external environment especially the user and performing its intended function hardware architecture then is an abstract representation of an electronic or an device capable of running fixed or changeable program hardware architecture generally includes some form of analog digital or hybrid electronic computer along with electronic and mechanical sensors and actuators hardware design may be viewed as partitioning scheme or algorithm which considers all of the system present and foreseeable requirements and arranges the necessary hardware components into workable set of cleanly bounded subsystems with no more parts than are required that is it is partitioning scheme that is exclusive inclusive and exhaustive major purpose of the partitioning is to arrange the elements in the hardware subsystems so that there is minimum of electrical connections and electronic communications needed among them in both software and hardware good subsystem tends to be seen as meaningful object moreover clear allocation of user requirements to the architecture hardware and software provides an effective basis for validation tests of the user requirements in the as built system see also computer aided manufacturing cam electronic design automation eda elmer fem solver finite element analysis hardware architect integrated circuit ic system on chip soc very large scale integration vlsi vhsic hardware description language vhdl technology cad tcad open cascade technology asic open source hardware references", "Natural language processing": "natural language processing nlp is field of computer science artificial intelligence and computational linguistics concerned with the interactions between computers and human natural languages as such nlp is related to the area of human computer interaction many challenges in nlp involve natural language understanding that is enabling computers to derive meaning from human or natural language input and others involve natural language generation an automated online assistant providing customer service on web page an example of an application where natural language processing is major component history the history of nlp generally starts in the although work can be found from earlier periods in alan turing published an article titled computing machinery and intelligence which proposed what is now called the turing test as criterion of intelligence the georgetown experiment in involved fully automatic translation of more than sixty russian sentences into english the authors claimed that within three or five years machine translation would be solved problem however real progress was much slower and after the alpac report in which found that ten year long research had failed to fulfill the expectations funding for machine translation was dramatically reduced little further research in machine translation was conducted until the late when the first statistical machine translation systems were developed some notably successful nlp systems developed in the were shrdlu natural language system working in restricted blocks worlds with restricted vocabularies and eliza simulation of rogerian psychotherapist written by joseph weizenbaum between to using almost no information about human thought or emotion eliza sometimes provided startlingly human like interaction when the patient exceeded the very small knowledge base eliza might provide generic response for example responding to my head hurts with why do you say your head hurts during the many programmers began to write conceptual ontologies which structured real world information into computer understandable data examples are margie schank sam cullingford pam wilensky talespin meehan qualm lehnert politics carbonell and plot units lehnert during this time many chatterbots were written including parry racter and jabberwacky up to the most nlp systems were based on complex sets of hand written rules starting in the late however there was revolution in nlp with the introduction of machine learning algorithms for language processing this was due to both the steady increase in computational power see moore law and the gradual lessening of the dominance of chomskyan theories of linguistics grammar whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine learning approach to language processing some of the earliest used machine learning algorithms such as decision trees produced systems of hard if then rules similar to existing hand written rules however part of speech tagging introduced the use of hidden markov models to nlp and increasingly research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to the features making up the input data the cache language models upon which many speech recognition systems now rely are examples of such statistical models such models are generally more robust when given unfamiliar input especially input that contains errors as is very common for real world data and produce more reliable results when integrated into larger system comprising multiple subtasks many of the notable early successes occurred in the field of machine translation due especially to work at ibm research where successively more complicated statistical models were developed these systems were able to take advantage of existing multilingual textual corpora that had been produced by the parliament of canada and the european union as result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government however most other systems depended on corpora specifically developed for the tasks implemented by these systems which was and often continues to be major limitation in the success of these systems as result great deal of research has gone into methods of more effectively learning from limited amounts of data recent research has increasingly focused on unsupervised and semi supervised learning algorithms such algorithms are able to learn from data that has not been hand annotated with the desired answers or using combination of annotated and non annotated data generally this task is much more difficult than supervised learning and typically produces less accurate results for given amount of input data however there is an enormous amount of non annotated data available including among other things the entire content of the world wide web which can often make up for the inferior results nlp using machine learning modern nlp algorithms are based on machine learning especially statistical machine learning the paradigm of machine learning is different from that of most prior attempts at language processing prior implementations of language processing tasks typically involved the direct hand coding of large sets of rules the machine learning paradigm calls instead for using general learning algorithms often although not always grounded in statistical inference to automatically learn such rules through the analysis of large corpora of typical real world examples corpus plural corpora is set of documents or sometimes individual sentences that have been hand annotated with the correct values to be learned many different classes of machine learning algorithms have been applied to nlp tasks these algorithms take as input large set of features that are generated from the input data some of the earliest used algorithms such as decision trees produced systems of hard if then rules similar to the systems of hand written rules that were then common increasingly however research has focused on statistical models which make soft probabilistic decisions based on attaching real valued weights to each input feature such models have the advantage that they can express the relative certainty of many different possible answers rather than only one producing more reliable results when such model is included as component of larger system systems based on machine learning algorithms have many advantages over hand produced rules the learning procedures used during machine learning automatically focus on the most common cases whereas when writing rules by hand it is often not obvious at all where the effort should be directed automatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input containing words or structures that have not been seen before and to erroneous input with misspelled words or words accidentally omitted generally handling such input gracefully with hand written rules or more generally creating systems of hand written rules that make soft decisions is extremely difficult error prone and time consuming systems based on automatically learning the rules can be made more accurate simply by supplying more input data however systems based on hand written rules can only be made more accurate by increasing the complexity of the rules which is much more difficult task in particular there is limit to the complexity of systems based on hand crafted rules beyond which the systems become more and more unmanageable however creating more data to input to machine learning systems simply requires corresponding increase in the number of man hours worked generally without significant increases in the complexity of the annotation process the subfield of nlp devoted to learning approaches is known as natural language learning nll and its conference conll and peak body signll are sponsored by acl recognizing also their links with computational linguistics and language acquisition when the aim of computational language learning research is to understand more about human language acquisition or nll overlaps into the related field of computational major tasks in nlp the following is list of some of the most commonly researched tasks in nlp note that some of these tasks have direct real world applications while others more commonly serve as subtasks that are used to aid in solving larger tasks what distinguishes these tasks from other potential and actual nlp tasks is not only the volume of research devoted to them but the fact that for each one there is typically well defined problem setting standard metric for evaluating the task standard corpora on which the task can be evaluated and competitions devoted to the specific task automatic summarization produce readable summary of chunk of text often used to provide summaries of text of known type such as articles in the financial section of newspaper coreference resolution given sentence or larger chunk of text determine which words mentions refer to the same objects entities anaphora resolution is specific example of this task and is specifically concerned with matching up pronouns with the nouns or names that they refer to the more general task of coreference resolution also includes identifying so called bridging relationships involving referring expressions for example in sentence such as he entered john house through the front door the front door is referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of john house rather than of some other structure that might also be referred to discourse analysis this rubric includes number of related tasks one task is identifying the discourse structure of connected text the nature of the discourse relationships between sentences elaboration explanation contrast another possible task is recognizing and classifying the speech acts in chunk of text yes no question content question statement assertion etc machine translation automatically translate text from one human language to another this is one of the most difficult problems and is member of class of problems colloquially termed ai complete requiring all of the different types of knowledge that humans possess grammar semantics facts about the real world etc in order to solve properly morphological segmentation separate words into individual morphemes and identify the class of the morphemes the difficulty of this task depends greatly on the complexity of the morphology the structure of words of the language being considered english has fairly simple morphology especially inflectional morphology and thus it is often possible to ignore this task entirely and simply model all possible forms of word open opens opened opening as separate words in languages such as turkish or manipuri highly agglutinated indian language however such an approach is not possible as each dictionary entry has thousands of possible word forms named entity recognition ner given stream of text determine which items in the text map to proper names such as people or places and what the type of each such name is person location organization note that although capitalization can aid in recognizing named entities in languages such as english this information cannot aid in determining the type of named entity and in any case is often inaccurate or insufficient for example the first word of sentence is also capitalized and named entities often span several words only some of which are capitalized furthermore many other languages in non western scripts chinese or arabic do not have any capitalization at all and even languages with capitalization may not consistently use it to distinguish names for example german capitalizes all nouns regardless of whether they refer to names and french and spanish do not capitalize names that serve as adjectives natural language generation convert information from computer databases into readable human language natural language understanding convert chunks of text into more formal representations such as first order logic structures that are easier for computer programs to manipulate natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from natural language expression which usually takes the form of organized notations of natural languages concepts introduction and creation of language metamodel and ontology are efficient however empirical solutions an explicit formalization of natural languages semantics without confusions with implicit assumptions such as closed world assumption cwa vs open world assumption or subjective yes no vs objective true false is expected for the construction of basis of semantics formalization optical character recognition ocr given an image representing printed text determine the corresponding text part of speech tagging given sentence determine the part of speech for each word many words especially common ones can serve as multiple parts of speech for example book can be noun the book on the table or verb to book flight set can be noun verb or adjective and out can be any of at least five different parts of speech some languages have more such ambiguity than others languages with little inflectional morphology such as english are particularly prone to such ambiguity chinese is prone to such ambiguity because it is tonal language during verbalization such inflection is not readily conveyed via the entities employed within the orthography to convey intended meaning parsing determine the parse tree grammatical analysis of given sentence the grammar for natural languages is ambiguous and typical sentences have multiple possible analyses in fact perhaps surprisingly for typical sentence there may be thousands of potential parses most of which will seem completely nonsensical to human question answering given human language question determine its answer typical questions have specific right answer such as what is the capital of canada but sometimes open ended questions are also considered such as what is the meaning of life recent works have looked at even more complex questions relationship extraction given chunk of text identify the relationships among named entities who is married to whom sentence breaking also known as sentence boundary disambiguation given chunk of text find the sentence boundaries sentence boundaries are often marked by periods or other punctuation marks but these same characters can serve other purposes marking abbreviations sentiment analysis extract subjective information usually from set of documents often using online reviews to determine polarity about specific objects it is especially useful for identifying trends of public opinion in the social media for the purpose of marketing speech recognition given sound clip of person or people speaking determine the textual representation of the speech this is the opposite of text to speech and is one of the extremely difficult problems colloquially termed ai complete see above in natural speech there are hardly any pauses between successive words and thus speech segmentation is necessary subtask of speech recognition see below note also that in most spoken languages the sounds representing successive letters blend into each other in process termed coarticulation so the conversion of the analog signal to discrete characters can be very difficult process speech segmentation given sound clip of person or people speaking separate it into words subtask of speech recognition and typically grouped with it topic segmentation and recognition given chunk of text separate it into segments each of which is devoted to topic and identify the topic of the segment word segmentation separate chunk of continuous text into separate words for language like english this is fairly trivial since words are usually separated by spaces however some written languages like chinese japanese and thai do not mark word boundaries in such fashion and in those languages text segmentation is significant task requiring knowledge of the vocabulary and morphology of words in the language word sense disambiguation many words have more than one meaning we have to select the meaning which makes the most sense in context for this problem we are typically given list of words and associated word senses from dictionary or from an online resource such as wordnet in some cases sets of related tasks are grouped into subfields of nlp that are often considered separately from nlp as whole examples include information retrieval ir this is concerned with storing searching and retrieving information it is separate field within computer science closer to databases but ir relies on some nlp methods for example stemming some current research and applications seek to bridge the gap between ir and nlp information extraction ie this is concerned in general with the extraction of semantic information from text this covers tasks such as named entity recognition coreference resolution relationship extraction etc speech processing this covers speech recognition text to speech and related tasks other tasks include native language identification stemming text simplification text to speech text proofing natural language search query expansion automated essay scoring truecasing statistical nlp statistical natural language processing uses stochastic probabilistic and statistical methods to resolve some of the difficulties discussed above especially those which arise because longer sentences are highly ambiguous when processed with realistic grammars yielding thousands or millions of possible analyses methods for disambiguation often involve the use of corpora and markov models one among the first models of statistical natural language understanding was introduced in by roberto pieraccini esther levin and chin hui lee from bell laboratories nlp comprises all quantitative approaches to automated language processing including probabilistic modeling information theory and linear algebra the technology for statistical nlp comes mainly from machine learning and data mining both of which are fields of artificial intelligence that involve learning from data evaluation of natural language processing objectives the goal of nlp evaluation is to measure one or more qualities of an algorithm or system in order to determine whether or to what extent the system answers the goals of its designers or meets the needs of its users research in nlp evaluation has received considerable attention because the definition of proper evaluation criteria is one way to specify precisely an nlp problem going thus beyond the vagueness of tasks defined only as language understanding or language generation precise set of evaluation criteria which includes mainly evaluation data and evaluation metrics enables several teams to compare their solutions to given nlp problem short history of evaluation in nlp the first evaluation campaign on written texts seems to be campaign dedicated to message understanding in pallet then the parseval geig project compared phrase structure grammars black series of campaigns within tipster project were realized on tasks like summarization translation and searching hirschman in in germany the morpholympics compared german taggers then the senseval romanseval campaigns were conducted with the objectives of semantic disambiguation in the sparkle campaign compared syntactic parsers in four different languages english french german and italian in france the grace project compared set of taggers for french in adda in during the technolangue easy project parsers for french were compared large scale evaluation of dependency parsers were performed in the context of the conll shared tasks in and in italy the evalita campaign was conducted in and to compare various nlp and speech tools for italian evalita web site in france within the anr passage project end of parsers for french were compared passage web site different types of evaluation depending on the evaluation procedures number of distinctions are traditionally made in nlp evaluation intrinsic vs extrinsic evaluation intrinsic evaluation considers an isolated nlp system and characterizes its performance mainly with respect to gold standard result pre defined by the evaluators extrinsic evaluation also called evaluation in use considers the nlp system in more complex setting either as an embedded system or serving precise function for human user the extrinsic performance of the system is then characterized in terms of its utility with respect to the overall task of the complex system or the human user for example consider syntactic parser that is based on the output of some new part of speech pos tagger an intrinsic evaluation would run the pos tagger on some labelled data and compare the system output of the pos tagger to the gold standard correct output an extrinsic evaluation would run the parser with some other pos tagger and then with the new pos tagger and compare the parsing accuracy black box vs glass box evaluation black box evaluation requires one to run an nlp system on given data set and to measure number of parameters related to the quality of the process speed reliability resource consumption and most importantly to the quality of the result the accuracy of data annotation or the fidelity of translation glass box evaluation looks at the design of the system the algorithms that are implemented the linguistic resources it uses vocabulary size etc given the complexity of nlp problems it is often difficult to predict performance only on the basis of glass box evaluation but this type of evaluation is more informative with respect to error analysis or future developments of system automatic vs manual evaluation in many cases automatic procedures can be defined to evaluate an nlp system by comparing its output with the gold standard or desired one although the cost of producing the gold standard can be quite high automatic evaluation can be repeated as often as needed without much additional costs on the same input data however for many nlp problems the definition of gold standard is complex task and can prove impossible when inter annotator agreement is insufficient manual evaluation is performed by human judges which are instructed to estimate the quality of system or most often of sample of its output based on number of criteria although thanks to their linguistic competence human judges can be considered as the reference for number of language processing tasks there is also considerable variation across their ratings this is why automatic evaluation is sometimes referred to as objective evaluation while the human kind appears to be more subjective standardization in nlp an iso subcommittee is working in order to ease between lexical resources and nlp programs the subcommittee is part of iso tc and is called iso tc sc some iso standards are already published but most of them are under construction mainly on lexicon representation see lmf annotation and data category registry the future of nlp nlp research is gradually shifting from lexical semantics to compositional semantics and further on narrative understanding human level natural language processing however is an ai complete problem that is it is equivalent to solving the central artificial intelligence problem making computers as intelligent as people or strong ai nlp future is therefore tied closely to the development of ai in general see also list of natural language processing toolkits biomedical text mining compound term processing computer assisted reviewing controlled natural language deep linguistic processing foreign language reading aid foreign language writing aid language technology latent semantic indexing lre map natural language programming reification linguistics spoken dialogue system telligent systems search references further reading steven bird ewan klein and edward loper natural language processing with python reilly media isbn daniel jurafsky and james martin speech and language processing nd edition pearson prentice hall isbn christopher manning prabhakar raghavan and hinrich sch\u00fctze introduction to information retrieval cambridge university press isbn official html and pdf versions available without charge christopher manning and hinrich sch\u00fctze foundations of statistical natural language processing the mit press isbn david powers and christopher turk machine learning of natural language springer verlag isbn", "Information security": "information security sometimes shortened to infosec is the practice of defending information from unauthorized access use disclosure disruption modification perusal inspection recording or destruction it is general term that can be used regardless of the form the data may take electronic physical overview it security sometimes referred to as computer security information technology security is information security applied to technology most often some form of computer system it is worthwhile to note that computer does not necessarily mean home desktop computer is any device with processor and some memory such devices can range from non networked standalone devices as simple as calculators to networked mobile computing devices such as smartphones and tablet computers it security specialists are almost always found in any major enterprise establishment due to the nature and value of the data within larger businesses they are responsible for keeping all of the technology within the company secure from malicious cyber attacks that often attempt to breach into critical private information or gain control of the internal systems information assurance the act of ensuring that data is not lost when critical issues arise these issues include but are not limited to natural disasters computer server malfunction physical theft or any other instance where data has the potential of being lost since most information is stored on computers in our modern era information assurance is typically dealt with by it security specialists one of the most common methods of providing information assurance is to have an off site backup of the data in case one of the mentioned issues arise threats computer system threats come in many different forms some of the most common threats today are software attacks theft of intellectual property identity theft theft of equipment or information sabotage and information extortion most people have experienced software attacks of some sort viruses worms phishing attacks and trojan horses are few common examples of software attacks the theft of intellectual property has also been an extensive issue for many businesses in the it field intellectual property is the ownership of property usually consisting of some form of protection theft of software is probably the most common in it businesses today identity theft is the attempt to act as someone else usually to obtain that person personal information or to take advantage of their access to vital information theft of equipment or information is becoming more prevalent today due to the fact that most devices today are mobile cell phones are prone to theft and have also become far more desirable as the amount of data capacity increases sabotage usually consists of the destruction of an organization website in an attempt to cause loss of confidence to its customers information extortion consists of theft of company property or information as an attempt to receive payment in exchange for returning the information or property back to its owner there are many ways to help protect yourself from some of these attacks but one of the most functional precautions is user carefulness governments military corporations financial institutions hospitals and private businesses amass great deal of confidential information about their employees customers products research and financial status most of this information is now collected processed and stored on electronic computers and transmitted across networks to other computers should confidential information about business customers or finances or new product line fall into the hands of competitor or black hat hacker business and its customers could suffer widespread irreparable financial loss as well as damage to the company reputation protecting confidential information is business requirement and in many cases also an ethical and legal requirement hence key concern for organizations today is to derive the optimal information security investment the renowned gordon loeb model actually provides powerful mathematical economic approach for addressing this critical concern for the individual information security has significant effect on privacy which is viewed very differently in different cultures the field of information security has grown and evolved significantly in recent years there are many ways of gaining entry into the field as career it offers many areas for specialization including securing network and allied infrastructure securing applications and databases security testing information systems auditing business continuity planning and digital forensics history since the early days of communication diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering julius caesar is credited with the invention of the caesar cipher which was created in order to prevent his secret messages from being read should message fall into the wrong hands but for the most part protection was achieved through the application of procedural handling controls sensitive information was marked up to indicate that it should be protected and transported by trusted persons guarded and stored in secure environment or strong box as postal services expanded governments created official organizations to intercept decipher read and reseal letters the uk secret office and deciphering branch in in the mid th century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity the british government codified this to some extent with the publication of the official secrets act in by the time of the first world war multi tier classification systems were used to communicate information to and from various fronts which encouraged greater use of code making and breaking sections in diplomatic and military headquarters in the united kingdom this led to the creation of the government code and cypher school in encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information the volume of information shared by the allied countries during the second world war necessitated formal alignment of classification systems and procedural controls an arcane range of markings evolved to indicate who could handle documents usually officers rather than men and where they should be stored as increasingly complex safes and storage facilities were developed procedures evolved to ensure documents were destroyed properly and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war the end of the th century and early years of the st century saw rapid advancements in computing hardware and software and data encryption the availability of smaller more powerful and less expensive computing equipment made electronic data processing within the reach of small business and the home user these computers quickly became interconnected through the internet the rapid growth and widespread use of electronic data processing and electronic business conducted through the internet along with numerous occurrences of international terrorism fueled the need for better methods of protecting the computers and the information they store process and transmit the academic disciplines of computer security and information assurance emerged along with numerous professional organizations all sharing the common goals of ensuring the security and reliability of information systems definitions information security attributes or qualities confidentiality integrity and availability cia information systems are composed in three main portions hardware software and communications with the purpose to help identify and apply information security industry standards as mechanisms of protection and prevention at three levels or layers physical personal and organizational essentially procedures or policies are implemented to tell people administrators users and operators how to use products to ensure information security within the organizations the definitions of infosec suggested in different sources are summarized below adopted from preservation of confidentiality integrity and availability of information note in addition other properties such as authenticity accountability non repudiation and reliability can also be involved iso iec the protection of information and information systems from unauthorized access use disclosure disruption modification or destruction in order to provide confidentiality integrity and availability cnss ensures that only authorized users confidentiality have access to accurate and complete information integrity when required availability isaca information security is the process of protecting the intellectual property of an organisation pipkin information security is risk management discipline whose job is to manage the cost of information risk to the business mcdermott and geer well informed sense of assurance that information risks and controls are in balance anderson information security is the protection of information and minimises the risk of exposing information to unauthorised parties venter and eloff information security is area of study and professional activity which is concerned with the development and implementation of security mechanisms of all available types technical organisational human oriented and legal in order to keep information in all its locations within and outside the organisation perimeter and consequently information systems where information is created processed stored transmitted and destroyed free from threats threats to information and information systems may be categorised and corresponding security goal may be defined for each category of threats set of security goals identified as result of threat analysis should be revised periodically to ensure its adequacy and conformance with the evolving environment the currently relevant set of security goals may include confidentiality integrity availability privacy authenticity trustworthiness non repudiation accountability and auditability cherdantseva and hilton profession information security is stable and growing profession information security professionals are very stable in their employment more than percent had no change in employer or employment in the past year and the number of professionals is projected to continuously grow more than percent annually from to basic principles key concepts the cia triad of confidentiality integrity and availability is at the heart of information security the members of the classic infosec triad confidentiality integrity and availability are interchangeably referred to in the literature as security attributes properties security goals fundamental aspects information criteria critical information characteristics and basic building blocks there is continuous debate about extending this classic trio other principles such as accountability have sometimes been proposed for addition it has been pointed out that issues such as non repudiation do not fit well within the three core concepts in and revised in the oecd guidelines for the security of information systems and networks proposed the nine generally accepted principles awareness responsibility response ethics democracy risk assessment security design and implementation security management and reassessment building upon those in the nist engineering principles for information technology security proposed principles from each of these derived guidelines and practices in donn parker proposed an alternative model for the classic cia triad that he called the six atomic elements of information the elements are confidentiality possession integrity authenticity availability and utility the merits of the parkerian hexad are subject of debate amongst security professionals in based on thorough analysis of information assurance and security ias literature the ias octave was proposed as an extension of the cia triad the ias octave includes confidentiality integrity availability accountability auditability authenticity trustworthiness non repudiation and privacy the completeness and accuracy of the ias octave was evaluated via series of interviews with ias academics and experts the ias octave is one of the dimensions of reference model of information assurance and security rmias which summarizes the ias knowledge in one all encompassing model confidentiality in information security confidentiality is the property that information is not made available or disclosed to unauthorized individuals entities or processes excerpt iso integrity in information security data integrity means maintaining and assuring the accuracy and completeness of data over its entire life cycle this means that data cannot be modified in an unauthorized or undetected manner this is not the same thing as referential integrity in databases although it can be viewed as special case of consistency as understood in the classic acid model of transaction processing information security systems typically provide message integrity in addition to data confidentiality availability for any information system to serve its purpose the information must be available when it is needed this means that the computing systems used to store and process the information the security controls used to protect it and the communication channels used to access it must be functioning correctly high availability systems aim to remain available at all times preventing service disruptions due to power outages hardware failures and system upgrades ensuring availability also involves preventing denial of service attacks such as flood of incoming messages to the target system essentially forcing it to shut down non repudiation in law non repudiation implies one intention to fulfill their obligations to contract it also implies that one party of transaction cannot deny having received transaction nor can the other party deny having sent transaction note this is also regarded as part of integrity it is important to note that while technology such as cryptographic systems can assist in non repudiation efforts the concept is at its core legal concept transcending the realm of technology it is not for instance sufficient to show that the message matches digital signature signed with the sender private key and thus only the sender could have sent the message and nobody else could have altered it in transit the alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed or allege or prove that his signing key has been compromised the fault for these violations may or may not lie with the sender himself and such assertions may or may not relieve the sender of liability but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity and thus prevents repudiation risk management the certified information systems auditor cisa review manual provides the following definition of risk management risk management is the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives and deciding what countermeasures if any to take in reducing risk to an acceptable level based on the value of the information resource to the organization there are two things in this definition that may need some clarification first the process of risk management is an ongoing iterative process it must be repeated indefinitely the business environment is constantly changing and new threats and vulnerabilities emerge every day second the choice of countermeasures controls used to manage risks must strike balance between productivity cost effectiveness of the countermeasure and the value of the informational asset being protected risk analysis and risk evaluation processes have their limitations since when security incidents occur they emerge in context and their rarity and even their uniqueness give rise to unpredictable threats the analysis of these phenomena which are characterized by breakdowns surprises and side effects requires theoretical approach which is able to examine and interpret subjectively the detail of each incident risk is the likelihood that something bad will happen that causes harm to an informational asset or the loss of the asset vulnerability is weakness that could be used to endanger or cause harm to an informational asset threat is anything man made or act of nature that has the potential to cause harm the likelihood that threat will use vulnerability to cause harm creates risk when threat does use vulnerability to inflict harm it has an impact in the context of information security the impact is loss of availability integrity and confidentiality and possibly other losses lost income loss of life loss of real property it should be pointed out that it is not possible to identify all risks nor is it possible to eliminate all risk the remaining risk is called residual risk risk assessment is carried out by team of people who have knowledge of specific areas of the business membership of the team may vary over time as different parts of the business are assessed the assessment may use subjective qualitative analysis based on informed opinion or where reliable dollar figures and historical information is available the analysis may use quantitative analysis the research has shown that the most vulnerable point in most information systems is the human user operator designer or other human the iso iec code of practice for information security management recommends the following be examined during risk assessment security policy organization of information security asset management human resources security physical and environmental security communications and operations management access control information systems acquisition development and maintenance information security incident management business continuity management and regulatory compliance in broad terms the risk management process consists of identification of assets and estimating their value include people buildings hardware software data electronic print other supplies conduct threat assessment include acts of nature acts of war accidents malicious acts originating from inside or outside the organization conduct vulnerability assessment and for each vulnerability calculate the probability that it will be exploited evaluate policies procedures standards training physical security quality control technical security calculate the impact that each threat would have on each asset use qualitative analysis or quantitative analysis identify select and implement appropriate controls provide proportional response consider productivity cost effectiveness and value of the asset evaluate the effectiveness of the control measures ensure the controls provide the required cost effective protection without discernible loss of productivity for any given risk management can choose to accept the risk based upon the relative low value of the asset the relative low frequency of occurrence and the relative low impact on the business or leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk in some cases the risk can be transferred to another business by buying insurance or outsourcing to another business the reality of some risks may be disputed in such cases leadership may choose to deny the risk controls selecting proper controls and implementing those will initially help an organization to bring down risk to acceptable levels control selection should follow and should be based on the risk assessment controls can vary in nature but fundamentally they are ways of protecting the confidentiality integrity or availability of information iso iec has defined controls in different areas but this is not exhaustive organizations can implement additional controls according to requirement of the organization iso has cut down the number of controls to from the technical standard of information security in place is abnt nbr iso iec administrative administrative controls also called procedural controls consist of approved written policies procedures standards and guidelines administrative controls form the framework for running the business and managing people they inform people on how the business is to be run and how day to day operations are to be conducted laws and regulations created by government bodies are also type of administrative control because they inform the business some industry sectors have policies procedures standards and guidelines that must be followed the payment card industry data security standard pci dss required by visa and mastercard is such an example other examples of administrative controls include the corporate security policy password policy hiring policies and disciplinary policies administrative controls form the basis for the selection and implementation of logical and physical controls logical and physical controls are manifestations of administrative controls administrative controls are of paramount importance logical logical controls also called technical controls use software and data to monitor and control access to information and computing systems for example passwords network and host based firewalls network intrusion detection systems access control lists and data encryption are logical controls an important logical control that is frequently overlooked is the principle of least privilege the principle of least privilege requires that an individual program or system process is not granted any more access privileges than are necessary to perform the task blatant example of the failure to adhere to the principle of least privilege is logging into windows as user administrator to read email and surf the web violations of this principle can also occur when an individual collects additional access privileges over time this happens when employees job duties change or they are promoted to new position or they transfer to another department the access privileges required by their new duties are frequently added onto their already existing access privileges which may no longer be necessary or appropriate physical physical controls monitor and control the environment of the work place and computing facilities they also monitor and control access to and from such facilities for example doors locks heating and air conditioning smoke and fire alarms fire suppression systems cameras barricades fencing security guards cable locks etc separating the network and workplace into functional areas are also physical controls an important physical control that is frequently overlooked is the separation of duties separation of duties ensures that an individual can not complete critical task by himself for example an employee who submits request for reimbursement should not also be able to authorize payment or print the check an applications programmer should not also be the server administrator or the database administrator these roles and must be separated from one another defense in depth the onion model of defense in depth information security must protect information throughout the life span of the information from the initial creation of the information on through to the final disposal of the information the information must be protected while in motion and while at rest during its lifetime information may pass through many different information processing systems and through many different parts of information processing systems there are many different ways the information and information systems can be threatened to fully protect the information during its lifetime each component of the information processing system must have its own protection mechanisms the building up layering on and overlapping of security measures is called defense in depth the strength of any system is no greater than its weakest link using defense in depth strategy should one defensive measure fail there are other defensive measures in place that continue to provide protection recall the earlier discussion about administrative controls logical controls and physical controls the three types of controls can be used to form the basis upon which to build defense in depth strategy with this approach defense in depth can be conceptualized as three distinct layers or planes laid one on top of the other additional insight into defense in depth can be gained by thinking of it as forming the layers of an onion with data at the core of the onion people the next outer layer of the onion and network security host based security and application security forming the outermost layers of the onion both perspectives are equally valid and each provides valuable insight into the implementation of good defense in depth strategy security classification for information an important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information not all information is equal and so not all information requires the same degree of protection this requires information to be assigned security classification the first step in information classification is to identify member of senior management as the owner of the particular information to be classified next develop classification policy the policy should describe the different classification labels define the criteria for information to be assigned particular label and list the required security controls for each classification some factors that influence which classification information should be assigned include how much value that information has to the organization how old the information is and whether or not the information has become obsolete laws and other regulatory requirements are also important considerations when classifying information the business model for information security enables security professionals to examine security from systems perspective creating an environment where security can be managed holistically allowing actual risks to be addressed the type of information security classification labels selected and used will depend on the nature of the organization with examples being in the business sector labels such as public sensitive private confidential in the government sector labels such as unclassified unofficial protected confidential secret top secret and their non english equivalents in cross sectoral formations the traffic light protocol which consists of white green amber and red all employees in the organization as well as business partners must be trained on the classification schema and understand the required security controls and handling procedures for each classification the classification of particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures access control access to protected information must be restricted to people who are authorized to access the information the computer programs and in many cases the computers that process the information must also be authorized this requires that mechanisms be in place to control the access to protected information the sophistication of the access control mechanisms should be in parity with the value of the information being protected the more sensitive or valuable the information the stronger the control mechanisms need to be the foundation on which access control mechanisms are built start with identification and authentication access control is generally considered in three steps identification authentication and authorization identification identification is an assertion of who someone is or what something is if person makes the statement hello my name is john doe they are making claim of who they are however their claim may or may not be true before john doe can be granted access to protected information it will be necessary to verify that the person claiming to be john doe really is john doe typically the claim is in the form of username by entering that username you are claiming am the person the username belongs to authentication authentication is the act of verifying claim of identity when john doe goes into bank to make withdrawal he tells the bank teller he is john doe claim of identity the bank teller asks to see photo id so he hands the teller his driver license the bank teller checks the license to make sure it has john doe printed on it and compares the photograph on the license against the person claiming to be john doe if the photo and name match the person then the teller has authenticated that john doe is who he claimed to be similarly by entering the correct password the user is providing evidence that they are the person the username belongs to there are three different types of information that can be used for authentication something you know things such as pin password or your mother maiden name something you have driver license or magnetic swipe card something you are biometrics including palm prints fingerprints voice prints and retina eye scans strong authentication requires providing more than one type of authentication information two factor authentication the username is the most common form of identification on computer systems today and the password is the most common form of authentication usernames and passwords have served their purpose but in our modern world they are no longer adequate usernames and passwords are slowly being replaced with more sophisticated authentication mechanisms authorization after person program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform run view create delete or change this is called authorization authorization to access information and other computing services begins with administrative policies and procedures the policies prescribe what information and computing services can be accessed by whom and under what conditions the access control mechanisms are then configured to enforce these policies different computing systems are equipped with different kinds of access control mechanisms some may even offer choice of different access control mechanisms the access control mechanism system offers will be based upon one of three approaches to access control or it may be derived from combination of the three approaches the non discretionary approach consolidates all access control under centralized administration the access to information and other resources is usually based on the individuals function role in the organization or the tasks the individual must perform the discretionary approach gives the creator or owner of the information resource the ability to control access to those resources in the mandatory access control approach access is granted or denied basing upon the security classification assigned to the information resource examples of common access control mechanisms in use today include role based access control available in many advanced database management systems simple file permissions provided in the unix and windows operating systems group policy objects provided in windows network systems kerberos radius tacacs and the simple access lists used in many firewalls and routers to be effective policies and other security controls must be enforceable and upheld effective policies ensure that people are held accountable for their actions all failed and successful authentication attempts must be logged and all access to information must leave some type of audit trail also need to know principle needs to be in affect when talking about access control need to know principle gives access rights to person to perform their job functions this principle is used in the government when dealing with difference clearances even though two employees in different departments have top secret clearance they must have need to know in order for information to be exchanged within the need to know principle network administrators grant the employee least amount privileges to prevent employees access and doing more than what they are supposed to need to know helps to enforce the confidentiality integrity availability triad need to know directly impacts the confidential area of the triad cryptography information security uses cryptography to transform usable information into form that renders it unusable by anyone other than an authorized user this process is called encryption information that has been encrypted rendered unusable can be transformed back into its original usable form by an authorized user who possesses the cryptographic key through the process of decryption cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit either electronically or physically and while information is in storage cryptography provides information security with other useful applications as well including improved authentication methods message digests digital signatures non repudiation and encrypted network communications older less secure applications such as telnet and ftp are slowly being replaced with more secure applications such as ssh that use encrypted network communications wireless communications can be encrypted using protocols such as wpa wpa or the older and less secure wep wired communications such as itu hn are secured using aes for encryption and for authentication and key exchange software applications such as gnupg or pgp can be used to encrypt data files and email cryptography can introduce security problems when it is not implemented correctly cryptographic solutions need to be implemented using industry accepted solutions that have undergone rigorous peer review by independent experts in cryptography the length and strength of the encryption key is also an important consideration key that is weak or too short will produce weak encryption the keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information they must be protected from unauthorized disclosure and destruction and they must be available when needed public key infrastructure pki solutions address many of the problems that surround key management process the terms reasonable and prudent person due care and due diligence have been used in the fields of finance securities and law for many years in recent years these terms have found their way into the fields of computing and information security federal sentencing guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems in the business world stockholders customers business partners and governments have the expectation that corporate officers will run the business in accordance with accepted business practices and in compliance with laws and other regulatory requirements this is often described as the reasonable and prudent person rule prudent person takes due care to ensure that everything necessary is done to operate the business by sound business principles and in legal ethical manner prudent person is also diligent mindful attentive and ongoing in their due care of the business in the field of information security harris offers the following definitions of due care and due diligence due care are steps that are taken to show that company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company its resources and employees and continual activities that make sure the protection mechanisms are continually maintained and operational attention should be made to two important points in these definitions first in due care steps are taken to show this means that the steps can be verified measured or even produce tangible artifacts second in due diligence there are continual activities this means that people are actually doing things to monitor and maintain the protection mechanisms and these activities are ongoing security governance the software engineering institute at carnegie mellon university in publication titled governing for enterprise security ges defines characteristics of effective security governance these include an enterprise wide issue leaders are accountable viewed as business requirement risk based roles and segregation of duties defined addressed and enforced in policy adequate resources committed staff aware and trained development life cycle requirement planned managed measurable and measured reviewed and audited incident response plans to paragraphs non technical that discuss selecting team members define roles and lines of authority define security incident define reportable incident training detection classification escalation containment eradication documentation change management change management is formal process for directing and controlling alterations to the information processing environment this includes alterations to desktop computers the network servers and software the objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the processing environment as changes are made it is not the objective of change management to prevent or hinder necessary changes from being implemented any change to the information processing environment introduces an element of risk even apparently simple changes can have unexpected effects one of management many is the management of risk change management is tool for managing the risks introduced by changes to the information processing environment part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented not every change needs to be managed some kinds of changes are part of the everyday routine of information processing and adhere to predefined procedure which reduces the overall level of risk to the processing environment creating new user account or deploying new desktop computer are examples of changes that do not generally require change management however relocating user file shares or upgrading the email server pose much higher level of risk to the processing environment and are not normal everyday activity the critical first steps in change management are defining change and communicating that definition and defining the scope of the change system change management is usually overseen by change review board composed of representatives from key business areas security networking systems administrators database administration applications development desktop support and the help desk the tasks of the change review board can be facilitated with the use of automated work flow application the responsibility of the change review board is to ensure the organizations documented change management procedures are followed the change management process is as follows requested anyone can request change the person making the change request may or may not be the same person that performs the analysis or implements the change when request for change is received it may undergo preliminary review to determine if the requested change is compatible with the organizations business model and practices and to determine the amount of resources needed to implement the change approved management runs the business and controls the allocation of resources therefore management must approve requests for changes and assign priority for every change management might choose to reject change request if the change is not compatible with the business model industry standards or best practices management might also choose to reject change request if the change requires more resources than can be allocated for the change planned planning change involves discovering the scope and impact of the proposed change analyzing the complexity of the change allocation of resources and developing testing and documenting both implementation and backout plans need to define the criteria on which decision to back out will be made tested every change must be tested in safe test environment which closely reflects the actual production environment before the change is applied to the production environment the backout plan must also be tested scheduled part of the change review board responsibility is to assist in the scheduling of changes by reviewing the proposed implementation date for potential conflicts with other scheduled changes or critical business activities communicated once change has been scheduled it must be communicated the communication is to give others the opportunity to remind the change review board about other changes or critical business activities that might have been overlooked when scheduling the change the communication also serves to make the help desk and users aware that change is about to occur another responsibility of the change review board is to ensure that scheduled changes have been properly communicated to those who will be affected by the change or otherwise have an interest in the change implemented at the appointed date and time the changes must be implemented part of the planning process was to develop an implementation plan testing plan and back out plan if the implementation of the change should fail or the post implementation testing fails or other drop dead criteria have been met the back out plan should be implemented documented all changes must be documented the documentation includes the initial request for change its approval the priority assigned to it the implementation testing and back out plans the results of the change review board critique the date time the change was implemented who implemented it and whether the change was implemented successfully failed or postponed post change review the change review board should hold post implementation review of changes it is particularly important to review failed and backed out changes the review board should try to understand the problems that were encountered and look for areas for improvement change management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment good change management procedures improve the overall quality and success of changes as they are implemented this is accomplished through planning peer review documentation and communication iso iec the visible ops handbook implementing itil in practical and auditable steps full book summary and information technology infrastructure library all provide valuable guidance on implementing an efficient and effective change management program information security business continuity business continuity is the mechanism by which an organization continues to operate its critical business units during planned or unplanned disruptions that affect normal business operations by invoking planned and managed procedures not only is business continuity simply about the business but it also an it system and process today disasters or disruptions to business are reality whether the disaster is natural or man made it affects normal life and so business therefore planning is important the planning is merely getting better prepared to face it knowing fully well that the best plans may fail planning helps to reduce cost of recovery operational overheads and most importantly sail through some smaller ones effortlessly for businesses to create effective plans they need to focus upon the following key questions most of these are common knowledge and anyone can do bcp should disaster strike what are the first few things that should do should call people to find if they are ok or call up the bank to figure out my money is safe this is emergency response emergency response services help take the first hit when the disaster strikes and if the disaster is serious enough the emergency response teams need to quickly get crisis management team in place what parts of my business should recover first the one that brings me most money or the one where spend the most or the one that will ensure shall be able to get sustained future growth the identified sections are the critical business units there is no magic bullet here no one answer satisfies all businesses need to find answers that meet business requirements how soon should target to recover my critical business units in bcp technical jargon this is called recovery time objective or rto this objective will define what costs the business will need to spend to recover from disruption for example it is cheaper to recover business in day than in hour what all do need to recover the business it machinery records food water people so many aspects to dwell upon the cost factor becomes clearer now business leaders need to drive business continuity hold on my it manager spent last month and created drp disaster recovery plan whatever happened to that drp is about continuing an it system and is one of the sections of comprehensive business continuity plan look below for more on this and where do recover my business from will the business center give me space to work or would it be flooded by many people queuing up for the same reasons that am but once do recover from the disaster and work in reduced production capacity since my main operational sites are unavailable how long can this go on how long can do without my original sites systems people this defines the amount of business resilience business may have now that know how to recover my business how do make sure my plan works most bcp pundits would recommend testing the plan at least once year reviewing it for adequacy and rewriting or updating the plans either annually or when businesses change disaster recovery planning while business continuity plan bcp takes broad approach to dealing with organizational wide effects of disaster disaster recovery plan drp which is subset of the business continuity plan is instead focused on taking the necessary steps to resume normal business operations as quickly as possible disaster recovery plan is executed immediately after the disaster occurs and details what steps are to be taken in order to recover critical information technology infrastructure disaster recovery planning includes establishing planning group performing risk assessment establishing priorities developing recovery strategies preparing inventories and documentation of the plan developing verification criteria and procedure and lastly implementing the plan laws and regulations privacy international privacy rankinggreen protections and safeguardsred endemic surveillance societies below is partial listing of european united kingdom canadian and us governmental laws and regulations that have or will have significant effect on data processing and information security important industry sector regulations have also been included when they have significant impact on information security uk data protection act makes new provisions for the regulation of the processing of information relating to individuals including the obtaining holding use or disclosure of such information the european union data protection directive eudpd requires that all eu member must adopt national regulations to standardize the protection of data privacy for citizens throughout the eu the computer misuse act is an act of the uk parliament making computer crime hacking criminal offence the act has become model upon which several other countries including canada and the republic of ireland have drawn inspiration when subsequently drafting their own information security laws eu data retention laws requires internet service providers and phone companies to keep data on every electronic message sent and phone call made for between six months and two years the family educational rights and privacy act ferpa cfr part is us federal law that protects the privacy of student education records the law applies to all schools that receive funds under an applicable program of the department of education generally schools must have written permission from the parent or eligible student in order to release any information from student education record federal financial institutions examination council ffiec security guidelines for auditors specifies requirements for online banking security health insurance portability and accountability act hipaa of requires the adoption of national standards for electronic health care transactions and national identifiers for providers health insurance plans and employers and it requires health care providers insurance providers and employers to safeguard the security and privacy of health data gramm leach bliley act of glba also known as the financial services modernization act of protects the privacy and security of private financial information that financial institutions collect hold and process sarbanes oxley act of sox section of the act requires publicly traded companies to assess the effectiveness of their internal controls for financial reporting in annual reports they submit at the end of each fiscal year chief information officers are responsible for the security accuracy and the reliability of the systems that manage and report the financial data the act also requires publicly traded companies to engage independent auditors who must attest to and report on the validity of their assessments payment card industry data security standard pci dss establishes comprehensive requirements for enhancing payment account data security it was developed by the founding payment brands of the pci security standards council including american express discover financial services jcb mastercard worldwide and visa international to help facilitate the broad adoption of consistent data security measures on global basis the pci dss is multifaceted security standard that includes requirements for security management policies procedures network architecture software design and other critical protective measures state security breach notification laws california and many others require businesses nonprofits and state institutions to notify consumers when unencrypted personal information may have been compromised lost or stolen personal information protection and electronics document act pipeda an act to support and promote electronic commerce by protecting personal information that is collected used or disclosed in certain circumstances by providing for the use of electronic means to communicate or record information or transactions and by amending the canada evidence act the statutory instruments act and the statute revision act hellenic authority for communication security and privacy adae law the greek law establishes and describes the minimum information security controls that should be deployed by every company which provides electronic communication networks and or services in greece in order to protect customers confidentiality these include both managerial and technical controls log records should be stored for two years hellenic authority for communication security and privacy adae law the latest greek law published by adae concentrates around the protection of the integrity and availability of the services and data offered by the greek companies the new law forces telcos and associated companies to build deploy and test appropriate business continuity plans and redundant infrastructures information security culture employee behavior has big impact to information security in organizations cultural concept can help different segments of the organization to concern about the information security within the organization exploring the relationship between organizational culture and information security culture provides the following definition of information security culture isc is the totality of patterns of behavior in an organization that contribute to the protection of information of all kinds information security culture needs to be improved continuously in information security culture from analysis to change authors commented it never ending process cycle of evaluation and change or maintenance to manage the information security culture five steps should be taken pre evaluation strategic planning operative planning implementation and post evaluation pre evaluation to identify the awareness of information security within employees and to analysis current security policy strategic planning to come up better awareness program we need to set clear targets clustering people is helpful to achieve it operative planning we can set good security culture based on internal communication management buy in and security awareness and training program implementation four stages should be used to implement the information security culture they are commitment of the management communication with organizational members courses for all organizational members and commitment of the employees sources of standards international organization for standardization iso is consortium of national standards institutes from countries coordinated through secretariat in geneva switzerland iso is the world largest developer of standards iso information technology security techniques framework for it security assurance iso iec information technology security techniques code of practice for information security management iso information technology service management and iso iec information technology security techniques information security management systems requirements are of particular interest to information security professionals the us national institute of standards and technology nist is non regulatory federal agency within the department of commerce the nist computer security division develops standards metrics tests and validation programs as well as publishes standards and guidelines to increase secure it planning implementation management and operation nist is also the custodian of the us federal information processing standard publications fips the internet society is professional membership society with more than organization and over individual members in over countries it provides leadership in addressing issues that confront the future of the internet and is the organization home for the groups responsible for internet infrastructure standards including the internet engineering task force ietf and the internet architecture board iab the isoc hosts the requests for comments rfcs which includes the official internet protocol standards and the rfc site security handbook the information security forum is global nonprofit organization of several hundred leading organizations in financial services manufacturing consumer goods government and other areas it undertakes research into information security practices and offers advice in its biannual standard of good practice and more detailed advisories for members the institute of information security professionals iisp is an independent non profit body governed by its members with the principal objective of advancing the professionalism of information security practitioners and thereby the professionalism of the industry as whole the institute developed the iisp skills framework this framework describes the range of competencies expected of information security and information assurance professionals in the effective performance of their roles it was developed through collaboration between both private and public sector organisations and world renowned academics and security leaders the german federal office for information security in german bundesamt f\u00fcr sicherheit in der bsi bsi standards to are set of recommendations including methods processes procedures approaches and measures relating to information security the bsi standard it grundschutz methodology describes how an information security management can be implemented and operated the standard includes very specific guide the it baseline protection catalogs also known as it grundschutz catalogs before the catalogs were formerly known as it baseline protection manual the catalogs are collection of documents useful for detecting and combating security relevant weak points in the it environment it cluster the collection encompasses as of september over pages with the introduction and catalogs the it grundschutz approach is aligned with to the iso iec family at the european standards institute catalog of information security indicators have been standardized by the industrial specification group isg isi scholars working in the field adam back annie anton brian lamacchia bruce schneier cynthia dwork dawn song deborah estrin gene spafford ian goldberg lawrence gordon martin loeb monica lam prosper mutorogodo joan feigenbaum jean camp lance cottrell lorrie cranor paul van oorschot peter gutmann peter landrock ross anderson stefan brands see also enterprise information security architecture identity based security information security audit information security management system information security policies information security standards information technology security audit it risk itil security management mobile security list of computer security certifications network security services privacy engineering privacy software privacy enhancing technologies security bug security information management security of information act security level management security service single sign on verification and validation further reading anderson it security professionals must evolve for changing market sc magazine october aceituno on information security paradigms issa journal september dhillon principles of information systems security text and cases john wiley sons easttom computer security fundamentals nd edition pearson press lambo iso iec the future of infosec certification issa journal november bibliography notes and references external links dod ia policy chart on the dod information assurance technology analysis center web site patterns practices security engineering explained open security architecture controls and patterns to secure it systems an introduction to information security iws information security chapter ross anderson book security engineering english translation of the greek regulation", "Operating system": "an operating system os is system software that manages computer hardware and software resources and provides common services for computer programs the operating system is component of the system software in computer system application programs usually require an operating system to function time sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time mass storage printing and other resources for hardware functions such as input and output and memory allocation the operating system acts as an intermediary between programs and the computer hardware although the application code is usually executed directly by the hardware and frequently makes system calls to an os function or be interrupted by it operating systems are found on many devices that contain computer from cellular phones and video game consoles to web servers and supercomputers examples of popular modern operating systems include android blackberry bsd chrome os ios linux os qnx steam os microsoft windows and variant windows phone and os the first nine of these examples share roots in unix popular hard real time operating systems include freertos micrium and vxworks types of operating systems single and multi tasking single tasking system can only run one program at time while multi tasking operating system allows more than one program to be running in concurrency this is achieved by time sharing dividing the available processor time between multiple processes which are each interrupted repeatedly in time slices by task scheduling subsystem of the operating system multi tasking may be characterized in preemptive and co operative types in preemptive multitasking the operating system slices the cpu time and dedicates slot to each of the programs unix like operating systems solaris linux as well as amigaos support preemptive multitasking cooperative multitasking is achieved by relying on each process to provide time to the other processes in defined manner bit versions of microsoft windows used cooperative multi tasking bit versions of both windows nt and win used preemptive multi tasking single and multi user single user operating systems have no facilities to distinguish users but may allow multiple programs to run in tandem multi user operating system extends the basic concept of multi tasking with facilities that identify processes and resources such as disk space belonging to multiple users and the system permits multiple users to interact with the system at the same time time sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time mass storage printing and other resources to multiple users distributed distributed operating system manages group of distinct computers and makes them appear to be single computer the development of networked computers that could be linked and communicate with each other gave rise to distributed computing distributed computations are carried out on more than one machine when computers in group work in cooperation they form distributed system templated in an os distributed and cloud computing context templating refers to creating single virtual machine image as guest operating system then saving it as tool for multiple running virtual machines gagne the technique is used both in virtualization and cloud computing management and is common in large server warehouses embedded embedded operating systems are designed to be used in embedded computer systems they are designed to operate on small machines like pdas with less autonomy they are able to operate with limited number of resources they are very compact and extremely efficient by design windows ce and minix are some examples of embedded operating systems real time real time operating system is an operating system that guarantees to process events or data within certain short amount of time real time operating system may be single or multi tasking but when multitasking it uses specialized scheduling algorithms so that deterministic nature of behavior is achieved an event driven system switches between tasks based on their priorities or external events while time sharing operating systems switch tasks based on clock interrupts library library operating system is one in which the services that typical operating system provides such as networking are provided in the form of libraries these libraries are composed with the application and configuration code to construct unikernels which are specialised single address space machine images that can be deployed to cloud or embedded environments history early computers were built to perform series of single tasks like calculator basic operating system features were developed in the such as resident monitor functions that could automatically run different programs in succession to speed up processing operating systems did not exist in their modern and more complex forms until the early hardware features were added that enabled use of runtime libraries interrupts and parallel processing when personal computers became popular in the operating systems were made for them similar in concept to those used on larger computers in the the earliest electronic digital systems had no operating systems electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plug boards these were special purpose systems that for example generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards after programmable general purpose computers were invented machine languages consisting of strings of the binary digits and on punched paper tape were introduced that sped up the programming process stern os was used on most ibm mainframe computers beginning in including computers used by the apollo program in the early computer could execute only one program at time each user had sole use of the computer for limited period of time and would arrive at scheduled time with program and data on punched paper cards and or punched tape the program would be loaded into the machine and the machine would be set to work until the program completed or crashed programs could generally be debugged via front panel using toggle switches and panel lights it is said that alan turing was master of this on the early manchester mark machine and he was already deriving the primitive conception of an operating system from the principles of the universal turing machine later machines came with libraries of programs which would be linked to user program to assist in operations such as input and output and generating computer code from human readable symbolic code this was the genesis of the modern day operating system however machines still ran single job at time at cambridge university in england the job queue was at one time washing line from which tapes were hung with different colored clothes pegs to indicate job priority an improvement was the atlas supervisor introduced with the manchester atlas commissioned in considered by many to be the first recognisable modern operating system brinch hansen described it as the most significant breakthrough in the history of operating systems mainframes through the many major features were pioneered in the field of operating systems including batch processing input output interrupt buffering multitasking spooling runtime libraries link loading and programs for sorting records in files these features were included or not included in application software at the option of application programmers rather than in separate operating system used by all applications in the share operating system was released as an integrated utility for the ibm and later in the and mainframes although it was quickly supplanted by ibsys ibjob on the and during the ibm os introduced the concept of single os spanning an entire product line which was crucial for the success of the system machines ibm current mainframe operating systems are distant descendants of this original system and applications written for os can still be run on modern machines os also pioneered the concept that the operating system keeps track of all of the system resources that are used including program and data space allocation in main memory and file space in secondary storage and file locking during update when the process is terminated for any reason all of these resources are re claimed by the operating system the alternative cp system for the started whole line of ibm operating systems focused on the concept of virtual machines other operating systems used on ibm series mainframes included systems developed by ibm cos compatibility operating system dos disk operating system tss time sharing system tos tape operating system bos basic operating system and acp airline control program as well as few non ibm systems mts michigan terminal system music multi user system for interactive computing and orvyl stanford timesharing system control data corporation developed the scope operating system in the for batch processing in cooperation with the university of minnesota the kronos and later the nos operating systems were developed during the which supported simultaneous batch and timesharing use like many commercial timesharing systems its interface was an extension of the dartmouth basic operating systems one of the pioneering efforts in timesharing and programming languages in the late control data and the university of illinois developed the plato operating system which used plasma panel displays and long distance time sharing networks plato was remarkably innovative for its time featuring real time chat and multi user graphical games in burroughs corporation introduced the with the mcp master control program operating system the was stack machine designed to exclusively support high level languages with no machine language or assembler and indeed the mcp was the first os to be written exclusively in high level language espol dialect of algol mcp also introduced many other ground breaking innovations such as being the first commercial implementation of virtual memory during development of the as ibm made an approach to burroughs to licence mcp to run on the as hardware this proposal was declined by burroughs management to protect its existing hardware production mcp is still in use today in the unisys clearpath mcp line of computers univac the first commercial computer manufacturer produced series of exec operating systems like all early main frame systems this batch oriented system managed magnetic drums disks card readers and line printers in the univac produced the real time basic rtb system to support large scale time sharing also patterned after the dartmouth bc system general electric and mit developed general electric comprehensive operating supervisor gecos which introduced the concept of ringed security privilege levels after acquisition by honeywell it was renamed general comprehensive operating system gcos digital equipment corporation developed many operating systems for its various computer lines including tops and tops time sharing systems for the bit pdp class systems before the widespread use of unix tops was particularly popular system in universities and in the early arpanet community from the late through the late several hardware capabilities evolved that allowed similar or ported software to run on more than one system early systems had utilized to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in series in fact most after the except the and were microprogrammed implementations the enormous investment in software for these systems made since the caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware notable supported mainframe operating systems include burroughs mcp to unisys clearpath mcp present ibm os ibm system to ibm os present ibm cp ibm system to ibm vm univac exec univac to os unisys clearpath dorado present microcomputers pc dos was an early personal computer os that featured command line interface mac os by apple computer became the first widespread os to feature graphical user interface many of its features such as windows and icons would later become commonplace in guis the first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minis minimalistic operating systems were developed often loaded from rom and known as monitors one notable early disk operating system was cp which was supported on many early microcomputers and was closely imitated by microsoft ms dos which became widely popular as the operating system chosen for the ibm pc ibm version of it was called ibm dos or pc dos in the apple computer inc now apple inc abandoned its popular apple ii series of microcomputers to introduce the apple macintosh computer with an innovative graphical user interface gui to the mac os the introduction of the intel cpu chip with bit architecture and paging capabilities provided personal computers with the ability to run multitasking operating systems like those of earlier minicomputers and mainframes microsoft responded to this progress by hiring dave cutler who had developed the vms operating system for digital equipment corporation he would lead the development of the windows nt operating system which continues to serve as the basis for microsoft operating systems line steve jobs co founder of apple inc started next computer inc which developed the nextstep operating system nextstep would later be acquired by apple inc and used along with code from freebsd as the core of mac os the gnu project was started by activist and programmer richard stallman with the goal of creating complete free software replacement to the proprietary unix operating system while the project was highly successful in duplicating the functionality of various parts of unix development of the gnu hurd kernel proved to be unproductive in finnish computer science student linus torvalds with cooperation from volunteers collaborating over the internet released the first version of the linux kernel it was soon merged with the gnu user space components and system software to form complete operating system since then the combination of the two major components has usually been referred to as simply linux by the software industry naming convention that stallman and the free software foundation remain opposed to preferring the name gnu linux the berkeley software distribution known as bsd is the unix derivative distributed by the university of california berkeley starting in the freely distributed and ported to many minicomputers it eventually also gained following for use on pcs mainly as freebsd netbsd and openbsd examples of operating systems unix and unix like operating systems file unix history simple png px thumb evolution of unix systems default file unix history simple svg unix was originally written in assembly language ken thompson wrote mainly based on bcpl based on his experience in the multics project was replaced by and unix rewritten in developed into large complex family of inter related operating systems which have been influential in every modern operating system see history the unix like family is diverse group of operating systems with several major sub categories including system bsd and linux the name unix is trademark of the open group which licenses it for use with any operating system that has been shown to conform to their definitions unix like is commonly used to refer to the large set of operating systems which resemble the original unix unix like systems run on wide variety of computer architectures they are used heavily for servers in business as well as workstations in academic and engineering environments free unix variants such as linux and bsd are popular in these areas four operating systems are certified by the open group holder of the unix trademark as unix hp hp ux and ibm aix are both descendants of the original system unix and are designed to run only on their respective vendor hardware in contrast sun microsystems solaris operating system can run on multiple types of hardware including and sparc servers and pcs apple os replacement for apple earlier non unix mac os is hybrid kernel based bsd variant derived from nextstep mach and freebsd unix was sought by establishing the posix standard the posix standard can be applied to any operating system although it was originally created for various unix variants bsd and its descendants first server for the world wide web ran on nextstep based on bsd subgroup of the unix family is the berkeley software distribution family which includes freebsd netbsd and openbsd these operating systems are most commonly found on webservers although they can also function as personal computer os the internet owes much of its existence to bsd as many of the protocols now commonly used by computers to connect send and receive data over network were widely implemented and refined in bsd the world wide web was also first demonstrated on number of computers running an os based on bsd called nextstep bsd has its roots in unix in university of california berkeley installed its first unix system over time students and staff in the computer science department there began adding new programs to make things easier such as text editors when berkeley received new vax computers in with unix installed the school undergraduates modified unix even more in order to take advantage of the computer hardware possibilities the defense advanced research projects agency of the us department of defense took interest and decided to fund the project many schools corporations and government organizations took notice and started to use berkeley version of unix instead of the official one distributed by at steve jobs upon leaving apple inc in formed next inc company that manufactured high end computers running on variation of bsd called nextstep one of these computers was used by tim berners lee as the first webserver to create the world wide web developers like keith bostic encouraged the project to replace any non free code that originated with bell labs once this was done however at sued eventually after two years of legal disputes the bsd project came out ahead and spawned number of free derivatives such as freebsd and netbsd os the standard user interface of os os formerly mac os is line of open core graphical operating systems developed marketed and sold by apple inc the latest of which is pre loaded on all currently shipping macintosh computers os is the successor to the original mac os which had been apple primary operating system since unlike its predecessor os is unix operating system built on technology that had been developed at next through the second half of the and up until apple purchased the company in early the operating system was first released in as mac os server with desktop oriented version mac os cheetah following in march since then six more distinct client and server editions of os have been released until the two were merged in os lion releases of os through are named after big cats starting with mavericks os versions are named after inspirational places in california os yosemite the most recent version was announced and released on june at the wwdc prior to its merging with os the server edition os server was architecturally identical to its desktop counterpart and usually ran on apple line of macintosh server hardware os server included work group management and administration software tools that provide simplified access to key network services including mail transfer agent samba server an ldap server domain name server and others with mac os lion all server aspects of mac os server have been integrated into the client version and the product re branded as os dropping mac from the name the server tools are now offered as an application linux ubuntu desktop linux distribution android popular mobile operating system using modified version of the linux kernel the linux kernel originated in as side project of linus torvalds while university student in finland he posted information about his project on newsgroup for computer students and programmers and received support and assistance from volunteers who succeeded in creating complete and functional kernel linux is unix like but was developed without any unix code unlike bsd and its variants because of its open license model the linux kernel code is available for study and modification which resulted in its use on wide range of computing machinery from supercomputers to smart watches although estimates suggest that linux is used on only of all personal computers it has been widely adopted for use in servers and embedded systems such as cell phones linux has superseded unix on many platforms and is used on the ten most powerful supercomputers in the world the linux kernel is used in some popular distributions such as red hat debian ubuntu linux mint and google android google chromium os chromium is an operating system based on the linux kernel and designed by google since chromium os targets computer users who spend most of their time on the internet it is mainly web browser with limited ability to run local applications though it has built in file manager and media player instead it relies on internet applications or web apps used in the web browser to accomplish tasks such as word processing chromium os differs from chrome os in that chromium is open source and used primarily by developers whereas chrome os is the operating system shipped out in chromebooks microsoft windows microsoft windows is family of proprietary operating systems designed by microsoft corporation and primarily targeted to intel architecture based computers with an estimated percent total usage share on web connected computers the newest version is windows windows recently overtook windows xp as most used os microsoft windows originated in as an operating environment running on top of ms dos which was the standard operating system shipped on most intel architecture personal computers at the time in windows was released which only used ms dos as bootstrap for backwards compatibility win could run real mode ms dos and bits windows drivers windows me released in was the last version in the win family later versions have all been based on the windows nt kernel current client versions of windows run on ia and bit arm microprocessors in addition itanium is still supported in older server version windows server in the past windows nt supported additional architectures server editions of windows are widely used in recent years microsoft has expended significant capital in an effort to promote the use of windows as server operating system however windows usage on servers is not as widespread as on personal computers as windows competes against linux and bsd for server market share the first pc that used windows operating system was the ibm personal system other there have been many operating systems that were significant in their day but are no longer so such as amigaos os from ibm and microsoft mac os the non unix precursor to apple mac os beos xts risc os morphos haiku baremetal and freemint some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications openvms formerly from dec is still under active development by hewlett packard yet other operating systems are used almost exclusively in academia for operating systems education or to do research on operating system concepts typical example of system that fulfills both roles is minix while for example singularity is used purely for research other operating systems have failed to win significant market share but have introduced innovations that have influenced mainstream operating systems not least bell labs plan components the components of an operating system all exist in order to make the different parts of computer work together all user software needs to go through the operating system in order to use any of the hardware whether it be as simple as mouse or keyboard or as complex as an internet component kernel kernel connects the application software to the hardware of computer with the aid of the firmware and device drivers the kernel provides the most basic level of control over all of the computer hardware devices it manages memory access for programs in the ram it determines which programs get access to which hardware resources it sets up or resets the cpu operating states for optimal operation at all times and it organizes the data for long term non volatile storage with file systems on such media as disks tapes flash memory etc program execution the operating system provides an interface between an application program and the computer hardware so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system the operating system is also set of services which simplify development and execution of application programs executing an application program involves the creation of process by the operating system kernel which assigns memory space and other resources establishes priority for the process in multi tasking systems loads program binary code into memory and initiates execution of the application program which then interacts with the user and with hardware devices interrupts interrupts are central to operating systems as they provide an efficient way for the operating system to interact with and react to its environment the alternative having the operating system watch the various sources of input for events polling that require action can be found in older systems with very small stacks or bytes but is unusual in modern systems with large stacks interrupt based programming is directly supported by most modern cpus interrupts provide computer with way of automatically saving local register contexts and running specific code in response to events even very basic computers support hardware interrupts and allow the programmer to specify code which may be run when that event takes place when an interrupt is received the computer hardware automatically suspends whatever program is currently running saves its status and runs computer code previously associated with the interrupt this is analogous to placing bookmark in book in response to phone call in modern operating systems interrupts are handled by the operating system kernel interrupts may come from either the computer hardware or the running program when hardware device triggers an interrupt the operating system kernel decides how to deal with this event generally by running some processing code the amount of code being run depends on the priority of the interrupt for example person usually responds to smoke detector alarm before answering the phone the processing of hardware interrupts is task that is usually delegated to software called device driver which may be part of the operating system kernel part of another program or both device drivers may then relay information to running program by various means program may also trigger an interrupt to the operating system if program wishes to access hardware for example it may interrupt the operating system kernel which causes control to be passed back to the kernel the kernel then processes the request if program wishes additional resources or wishes to shed resources such as memory it triggers an interrupt to get the kernel attention modes privilege rings for the available in protected mode operating systems determine which processes run in each mode modern cpus support multiple modes of operation cpus with this capability use at least two modes protected mode and supervisor mode the supervisor mode is used by the operating system kernel for low level tasks that need unrestricted access to hardware such as controlling how memory is written and erased and communication with devices like graphics cards protected mode in contrast is used for almost everything else applications operate within protected mode and can only use hardware by communicating with the kernel which controls everything in supervisor mode cpus might have other modes similar to protected mode as well such as the virtual modes in order to emulate older processor types such as bit processors on bit one or bit processors on bit one when computer first starts up it is automatically running in supervisor mode the first few programs to run on the computer being the bios or efi bootloader and the operating system have unlimited access to hardware and this is required because by definition initializing protected environment can only be done outside of one however when the operating system passes control to another program it can place the cpu into protected mode in protected mode programs may have access to more limited set of the cpu instructions user program may leave protected mode only by triggering an interrupt causing control to be passed back to the kernel in this way the operating system can maintain exclusive control over things like access to hardware and memory the term protected mode resource generally refers to one or more cpu registers which contain information that the running program isn allowed to alter attempts to alter these resources generally causes switch to supervisor mode where the operating system can deal with the illegal operation the program was attempting for example by killing the program memory management among other things operating system kernel must be responsible for managing all system memory which is currently in use by programs this ensures that program does not interfere with memory already in use by another program since programs time share each program must have independent access to memory cooperative memory management used by many early operating systems assumes that all programs make voluntary use of the kernel memory manager and do not exceed their allocated memory this system of memory management is almost never seen any more since programs often contain bugs which can cause them to exceed their allocated memory if program fails it may cause memory used by one or more other programs to be affected or overwritten malicious programs or viruses may purposefully alter another program memory or may affect the operation of the operating system itself with cooperative memory management it takes only one misbehaved program to crash the system memory protection enables the kernel to limit process access to the computer memory various methods of memory protection exist including memory segmentation and paging all methods require some level of hardware support such as the mmu which doesn exist in all computers in both segmentation and paging certain protected mode registers specify to the cpu what memory address it should allow running program to access attempts to access other addresses trigger an interrupt which cause the cpu to re enter supervisor mode placing the kernel in charge this is called segmentation violation or seg for short and since it is both difficult to assign meaningful result to such an operation and because it is usually sign of misbehaving program the kernel generally resorts to terminating the offending program and reports the error windows versions through me had some level of memory protection but programs could easily circumvent the need to use it general protection fault would be produced indicating segmentation violation had occurred however the system would often crash anyway virtual memory many operating systems can trick programs into using memory scattered around the hard disk and ram as if it is one continuous chunk of memory called virtual memory the use of virtual memory addressing such as paging or segmentation means that the kernel can choose what memory each program may use at any given time allowing the operating system to use the same memory locations for multiple tasks if program tries to access memory that isn in its current range of accessible memory but nonetheless has been allocated to it the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory see section on memory management under unix this kind of interrupt is referred to as page fault when the kernel detects page fault it generally adjusts the virtual memory range of the program which triggered it granting it access to the memory requested this gives the kernel discretionary power over where particular application memory is stored or even whether or not it has actually been allocated yet in modern operating systems memory which is accessed less frequently can be temporarily stored on disk or other media to make that space available for use by other programs this is called swapping as an area of memory can be used by multiple programs and what that memory area contains can be swapped or exchanged on demand virtual memory provides the programmer or the user with the perception that there is much larger amount of ram in the computer than is really there multitasking multitasking refers to the running of multiple independent computer programs on the same computer giving the appearance that it is performing the tasks at the same time since most computers can do at most one or two things at one time this is generally done via time sharing which means that each program uses share of the computer time to execute an operating system kernel contains scheduling program which determines how much time each process spends executing and in which order execution control should be passed to programs control is passed to process by the kernel which allows the program access to the cpu and memory later control is returned to the kernel through some mechanism so that another program may be allowed to use the cpu this so called passing of control between the kernel and applications is called context switch an early model which governed the allocation of time to programs was called cooperative multitasking in this model when control is passed to program by the kernel it may execute for as long as it wants before explicitly returning control to the kernel this means that malicious or malfunctioning program may not only prevent any other programs from using the cpu but it can hang the entire system if it enters an infinite loop modern operating systems extend the concepts of application preemption to device drivers and kernel code so that the operating system has preemptive control over internal run times as well the philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the cpu this implies that all programs must be limited in how much time they are allowed to spend on the cpu without being interrupted to accomplish this modern operating system kernels make use of timed interrupt protected mode timer is set by the kernel which triggers return to supervisor mode after the specified time has elapsed see above sections on interrupts and dual mode operation on many single user operating systems cooperative multitasking is perfectly adequate as home computers generally run small number of well tested programs the amigaos is an exception having preemptive multitasking from its very first version windows nt was the first version of microsoft windows which enforced preemptive multitasking but it didn reach the home user market until windows xp since windows nt was targeted at professionals disk access and file systems file systems allow users and programs to organize and sort files on computer often through the use of directories or folders access to data stored on disks is central feature of all operating systems computers store data on disks using files which are structured in specific ways in order to allow for faster access higher reliability and to make better use out of the drive available space the specific way in which files are stored on disk is called file system and enables files to have names and attributes it also allows them to be stored in hierarchy of directories or folders arranged in directory tree early operating systems generally supported single type of disk drive and only one kind of file system early file systems were limited in their capacity speed and in the kinds of file names and directory structures they could use these limitations often reflected limitations in the operating systems they were designed for making it very difficult for an operating system to support more than one file system while many simpler operating systems support limited range of options for accessing storage systems operating systems like unix and linux support technology known as virtual file system or vfs an operating system such as unix supports wide array of storage devices regardless of their design or file systems allowing them to be accessed through common application programming interface api this makes it unnecessary for programs to have any knowledge about the device they are accessing vfs allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them through the use of specific device drivers and file system drivers connected storage device such as hard drive is accessed through device driver the device driver understands the specific language of the drive and is able to translate that language into standard language used by the operating system to access all disk drives on unix this is the language of block devices when the kernel has an appropriate device driver in place it can then access the contents of the disk drive in raw format which may contain one or more file systems file system driver is used to translate the commands used to access each specific file system into standard set of commands that the operating system can use to talk to all file systems programs can then deal with these file systems on the basis of filenames and directories folders contained within hierarchical structure they can create delete open and close files as well as gather various information about them including access permissions size free space and creation and modification dates various differences between file systems make supporting all file systems difficult allowed characters in file names case sensitivity and the presence of various kinds of file attributes makes the implementation of single interface for every file system daunting task operating systems tend to recommend using and so support natively file systems specifically designed for them for example ntfs in windows and ext and reiserfs in linux however in practice third party drivers are usually available to give support for the most widely used file systems in most general purpose operating systems for example ntfs is available in linux through ntfs and ext and reiserfs are available in windows through third party software support for file systems is highly varied among modern operating systems although there are several common file systems which almost all operating systems include support and drivers for operating systems vary on file system support and on the disk formats they may be installed on under windows each file system is usually limited in application to certain media for example cds must use iso or udf and as of windows vista ntfs is the only file system which the operating system can be installed on it is possible to install linux onto many types of file systems unlike other operating systems linux and unix allow any file system to be used regardless of the media it is stored in whether it is hard drive disc cd dvd usb flash drive or even contained within file located on another file system device drivers device driver is specific type of computer software developed to allow interaction with hardware devices typically this constitutes an interface for communicating with the device through the specific computer bus or communications subsystem that the hardware is connected to providing commands to and or receiving data from the device and on the other end the requisite interfaces to the operating system and software applications it is specialized hardware dependent computer program which is also operating system specific that enables another program typically an operating system or applications software package or computer program running under the operating system kernel to interact transparently with hardware device and usually provides the requisite interrupt handling necessary for any necessary asynchronous time dependent hardware interfacing needs the key design goal of device drivers is abstraction every model of hardware even within the same class of device is different newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently computers and their operating systems cannot be expected to know how to control every device both now and in the future to solve this problem operating systems essentially dictate how every type of device should be controlled the function of the device driver is then to translate these operating system mandated function calls into device specific calls in theory new device which is controlled in new manner should function correctly if suitable driver is available this new driver ensures that the device appears to operate as usual from the operating system point of view under versions of windows before vista and versions of linux before all driver execution was co operative meaning that if driver entered an infinite loop it would freeze the system more recent revisions of these operating systems incorporate kernel preemption where the kernel interrupts the driver to give it tasks and then separates itself from the process until it receives response from the device driver or gives it more tasks to do networking currently most operating systems support variety of networking protocols hardware and applications for using them this means that computers running dissimilar operating systems can participate in common network for sharing resources such as computing files printers and scanners using either wired or wireless connections networks can essentially allow computer operating system to access the resources of remote computer to support the same functions as it could if those resources were connected directly to the local computer this includes everything from simple communication to using networked file systems or even sharing another computer graphics or sound hardware some network services allow the resources of computer to be accessed transparently such as ssh which allows networked users direct access to computer command line interface client server networking allows program on computer called client to connect via network to another computer called server servers offer or host various services to other network computers and users these services are usually provided through ports or numbered access points beyond the server network address each port number is usually associated with maximum of one running program which is responsible for handling requests to that port daemon being user program can in turn access the local hardware resources of that computer by passing requests to the operating system kernel many operating systems support one or more vendor specific or open networking protocols as well for example sna on ibm systems decnet on systems from digital equipment corporation and microsoft specific protocols smb on windows specific protocols for specific tasks may also be supported such as nfs for file access protocols like esound or esd can be easily extended over the network to provide sound from local applications on remote system sound hardware security computer being secure depends on number of technologies working properly modern operating system provides access to number of resources which are available to software running on the system and to external devices like networks via the kernel the operating system must be capable of distinguishing between requests which should be allowed to be processed and others which should not be processed while some systems may simply distinguish between privileged and non privileged systems commonly have form of requester identity such as user name to establish identity there may be process of authentication often username must be quoted and each username may have password other methods of authentication such as magnetic cards or biometric data might be used instead in some cases especially connections from the network resources may be accessed with no authentication at all such as reading files over network share also covered by the concept of requester identity is authorization the particular services and resources accessible by the requester once logged into system are tied to either the requester user account or to the variously configured groups of users to which the requester belongs in addition to the allow or disallow model of security system with high level of security also offers auditing options these would allow tracking of requests for access to resources such as who has been reading this file internal security or security from an already running program is only possible if all possibly harmful requests must be carried out through interrupts to the operating system kernel if programs can directly access hardware and resources they cannot be secured external security involves request from outside the computer such as login at connected console or some kind of network connection external requests are often passed through device drivers to the operating system kernel where they can be passed onto applications or carried out directly security of operating systems has long been concern because of highly sensitive data held on computers both of commercial and military nature the united states government department of defense dod created the trusted computer system evaluation criteria tcsec which is standard that sets basic requirements for assessing the effectiveness of security this became of vital importance to operating system makers because the tcsec was used to evaluate classify and select trusted operating systems being considered for the processing storage and retrieval of sensitive or classified information network services include offerings such as file sharing print services email web sites and file transfer protocols ftp most of which can have compromised security at the front line of security are hardware devices known as firewalls or intrusion detection prevention systems at the operating system level there are number of software firewalls available as well as intrusion detection prevention systems most modern operating systems include software firewall which is enabled by default software firewall can be configured to allow or deny network traffic to or from service or application running on the operating system therefore one can install and be running an insecure service such as telnet or ftp and not have to be threatened by security breach because the firewall would deny all traffic trying to connect to the service on that port an alternative strategy and the only sandbox strategy available in systems that do not meet the popek and goldberg virtualization requirements is where the operating system is not running user programs as native code but instead either emulates processor or provides host for code based system such as java internal security is especially relevant for multi user systems it allows each user of the system to have private files that the other users cannot tamper with or read internal security is also vital if auditing is to be of any use since program can potentially bypass the operating system inclusive of bypassing auditing user interface bash command line each command is typed out after the prompt and then its output appears below working its way down the screen the current command prompt is at the bottom every computer that is to be operated by an individual requires user interface the user interface is usually referred to as shell and is essential if human interaction is to be supported the user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices such as keyboard mouse or credit card reader and requests operating system services to display prompts status messages and such on output hardware devices such as video monitor or printer the two most common forms of user interface have historically been the command line interface where computer commands are typed out line by line and the graphical user interface where visual environment most commonly wimp is present graphical user interfaces screenshot of the kde plasma desktop graphical user interface programs take the form of images on the screen and the files folders directories and applications take the form of icons and symbols mouse is used to navigate the computer most of the modern computer systems support graphical user interfaces gui and often include them in some computer systems such as the original implementation of mac os the gui is integrated into the kernel while technically graphical user interface is not an operating system service incorporating support for one into the operating system kernel can allow the gui to be more responsive by reducing the number of context switches required for the gui to perform its output functions other operating systems are modular separating the graphics subsystem from the kernel and the operating system in the unix vms and many others had operating systems that were built this way linux and mac os are also built this way modern releases of microsoft windows such as windows vista implement graphics subsystem that is mostly in user space however the graphics drawing routines of versions between windows nt and windows server exist mostly in kernel space windows had very little distinction between the interface and the kernel many computer operating systems allow the user to install or create any user interface they desire the window system in conjunction with gnome or kde plasma desktop is commonly found setup on most unix and unix like bsd linux solaris systems number of windows shell replacements have been released for microsoft windows which offer alternatives to the included windows shell but the shell itself cannot be separated from windows numerous unix based guis have existed over time most derived from competition among the various vendors of unix hp ibm sun led to much fragmentation though an effort to standardize in the to cose and cde failed for various reasons and were eventually eclipsed by the widespread adoption of gnome and desktop environment prior to free software based toolkits and desktop environments motif was the prevalent toolkit desktop combination and was the basis upon which cde was developed graphical user interfaces evolve over time for example windows has modified its user interface almost every time new major version of windows is released and the mac os gui changed dramatically with the introduction of mac os in real time operating systems real time operating system rtos is an operating system intended for applications with fixed deadlines real time computing such applications include some small embedded systems automobile engine controllers industrial robots spacecraft industrial control and some large scale computing systems an early example of large scale real time operating system was transaction processing facility developed by american airlines and ibm for the sabre airline reservations system embedded systems that have fixed deadlines use real time operating system such as vxworks pikeos ecos qnx montavista linux and rtlinux windows ce is real time operating system that shares similar apis to desktop windows but shares none of desktop windows codebase symbian os also has an rtos kernel eka starting with version some embedded systems use operating systems such as palm os bsd and linux although such operating systems do not support real time computing operating system development as hobby operating system development is one of the most complicated activities in which computing hobbyist may engage hobby operating system may be classified as one whose code has not been directly derived from an existing operating system and has few users and active developers in some cases hobby development is in support of homebrew computing device for example simple single board computer powered by microprocessor or development may be for an architecture already in widespread use operating system development may come from entirely new concepts or may commence by modeling an existing operating system in either case the hobbyist is his her own developer or may interact with small and sometimes unstructured group of individuals who have like interests examples of hobby operating system include reactos and syllable diversity of operating systems and portability application software is generally written for use on specific operating system and sometimes even for specific hardware when porting the application to run on another os the functionality required by that application may be implemented differently by that os the names of functions meaning of arguments etc requiring the application to be adapted changed or otherwise maintained unix was the first operating system not written in assembly language making it very portable to systems different from its native pdp this cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms like java or qt these abstractions have already borne the cost of adaptation to specific operating systems and their system libraries another approach is for operating system vendors to adopt standards for example posix and os abstraction layers provide commonalities that reduce porting costs market share worldwide device shipments by operating system operating system millions of units million of units android windows ios mac os blackberry others total source gartner in android was first currently not replicated by others in single year operating system ever to ship on billion devices becoming the most popular operating system by installed base see also comparison of operating systems mobile device mobile operating system hypervisor interruptible operating system list of important publications in operating systems list of operating systems glossary of operating systems terms microcontroller network operating system object oriented operating system operating system projects live cd system commander system image timeline of operating systems usage share of operating systems references further reading brien marakas management information systems mcgraw hill irwin external links multics history and the history of operating systems", "Artificial intelligence": "artificial intelligence ai is the intelligence exhibited by machines or software it is also the name of the academic field of study which studies how to create computers and computer software that are capable of intelligent behavior major ai researchers and textbooks define this field as the study and design of intelligent agents in which an intelligent agent is system that perceives its environment and takes actions that maximize its chances of success john mccarthy who coined the term in defines it as the science and engineering of making intelligent machines ai research is highly technical and specialized and is deeply divided into subfields that often fail to communicate with each other some of the division is due to social and cultural factors subfields have grown up around particular institutions and the work of individual researchers ai research is also divided by several technical issues some subfields focus on the solution of specific problems others focus on one of several possible approaches or on the use of particular tool or towards the accomplishment of particular applications the central problems or goals of ai research include reasoning knowledge planning learning natural language processing communication perception and the ability to move and manipulate objects general intelligence is still among the field long term goals currently popular approaches include statistical methods computational intelligence and traditional symbolic ai there are large number of tools used in ai including versions of search and mathematical optimization logic methods based on probability and economics and many others the ai field is in which number of sciences and professions converge including computer science mathematics psychology linguistics philosophy and neuroscience as well as other specialized fields such as artificial psychology the field was founded on the claim that central property of humans human intelligence the sapience of homo sapiens can be so precisely described that machine can be made to simulate it this raises philosophical issues about the nature of the mind and the ethics of creating artificial beings endowed with human like intelligence issues which have been addressed by myth fiction and philosophy since antiquity artificial intelligence has been the subject of tremendous optimism but has also suffered stunning setbacks today it has become an essential part of the technology industry providing the heavy lifting for many of the most challenging problems in computer science history thinking machines and artificial beings appear in greek myths such as talos of crete the bronze robot of hephaestus and pygmalion galatea human likenesses believed to have intelligence were built in every major civilization animated cult images were worshiped in egypt and greece and humanoid automatons were built by yan shi hero of alexandria and al jazari it was also widely believed that artificial beings had been created by j\u0101bir ibn hayy\u0101n judah loew and paracelsus by the th and th centuries artificial beings had become common feature in fiction as in mary shelley frankenstein or karel \u010dapek rossum universal robots pamela mccorduck argues that all of these are some examples of an ancient urge as she describes it to forge the gods stories of these creatures and their fates discuss many of the same hopes fears and ethical concerns that are presented by artificial intelligence mechanical or formal reasoning has been developed by philosophers and mathematicians since antiquity the study of logic led directly to the invention of the programmable digital electronic computer based on the work of mathematician alan turing and others turing theory of computation suggested that machine by shuffling symbols as simple as and could simulate any conceivable act of mathematical deduction this along with concurrent discoveries in neurology information theory and cybernetics inspired small group of researchers to begin to seriously consider the possibility of building an electronic brain the field of ai research was founded at conference on the campus of dartmouth college in the summer of the attendees including john mccarthy marvin minsky allen newell arthur samuel and herbert simon became the leaders of ai research for many decades they and their students wrote programs that were to most people simply astonishing computers were winning at checkers solving word problems in algebra proving logical theorems and speaking english by the middle of the research in the was heavily funded by the department of defense and laboratories had been established around the world ai founders were profoundly optimistic about the future of the new field herbert simon predicted that machines will be capable within twenty years of doing any work man can do and marvin minsky agreed writing that within generation the problem of creating artificial intelligence will substantially be solved they had failed to recognize the difficulty of some of the problems they faced in in response to the criticism of sir james lighthill and ongoing pressure from the us congress to fund more productive projects both the and british governments cut off all undirected exploratory research in ai the next few years would later be called an ai winter period when funding for ai projects was hard to find in the early ai research was revived by the commercial success of expert systems form of ai program that simulated the knowledge and analytical skills of one or more human experts by the market for ai had reached over billion dollars at the same time japan fifth generation computer project inspired the and british governments to restore funding for academic research in the field however beginning with the collapse of the lisp machine market in ai once again fell into disrepute and second longer lasting ai winter began in the and early st century ai achieved its greatest successes albeit somewhat behind the scenes artificial intelligence is used for logistics data mining medical diagnosis and many other areas throughout the technology industry the success was due to several factors the increasing computational power of computers see moore law greater emphasis on solving specific subproblems the creation of new ties between ai and other fields working on similar problems and new commitment by researchers to solid mathematical methods and rigorous scientific standards on may deep blue became the first computer chess playing system to beat reigning world chess champion garry kasparov in february in jeopardy quiz show exhibition match ibm question answering system watson defeated the two greatest jeopardy champions brad rutter and ken jennings by significant margin the kinect which provides body motion interface for the xbox and the xbox one uses algorithms that emerged from lengthy ai research as do intelligent personal assistants in smartphones research goals the general problem of simulating or creating intelligence has been broken down into number of specific sub problems these consist of particular traits or capabilities that researchers would like an intelligent system to display the traits described below have received the most attention deduction reasoning problem solving early ai researchers developed algorithms that imitated the step by step reasoning that humans use when they solve puzzles or make logical deductions by the late and ai research had also developed highly successful methods for dealing with uncertain or incomplete information employing concepts from probability and economics for difficult problems most of these algorithms can require enormous computational resources most experience combinatorial explosion the amount of memory or computer time required becomes astronomical when the problem goes beyond certain size the search for more efficient problem solving algorithms is high priority for ai research human beings solve most of their problems using fast intuitive judgements rather than the conscious step by step deduction that early ai research was able to model ai has made some progress at imitating this kind of sub symbolic problem solving embodied agent approaches emphasize the importance of sensorimotor skills to higher reasoning neural net research attempts to simulate the structures inside the brain that give rise to this skill statistical approaches to ai mimic the probabilistic nature of the human ability to guess knowledge representation an ontology represents knowledge as set of concepts within domain and the relationships between those concepts knowledge representation and knowledge engineering are central to ai research many of the problems machines are expected to solve will require extensive knowledge about the world among the things that ai needs to represent are objects properties categories and relations between objects situations events states and time causes and effects knowledge about knowledge what we know about what other people know and many other less well researched domains representation of what exists is an ontology the set of objects relations concepts and so on that the machine knows about the most general are called upper ontologies which attempt to provide foundation for all other knowledge among the most difficult problems in knowledge representation are default reasoning and the qualification problem many of the things people know take the form of working assumptions for example if bird comes up in conversation people typically picture an animal that is fist sized sings and flies none of these things are true about all birds john mccarthy identified this problem in as the qualification problem for any commonsense rule that ai researchers care to represent there tend to be huge number of exceptions almost nothing is simply true or false in the way that abstract logic requires ai research has explored number of solutions to this problem the breadth of commonsense knowledge the number of atomic facts that the average person knows is astronomical research projects that attempt to build complete knowledge base of commonsense knowledge cyc require enormous amounts of laborious ontological engineering they must be built by hand one complicated concept at time major goal is to have the computer understand enough concepts to be able to learn by reading from sources like the internet and thus be able to add to its own ontology the subsymbolic form of some commonsense knowledge much of what people know is not represented as facts or statements that they could express verbally for example chess master will avoid particular chess position because it feels too exposed or an art critic can take one look at statue and instantly realize that it is fake these are intuitions or tendencies that are represented in the brain non consciously and sub symbolically knowledge like this informs supports and provides context for symbolic conscious knowledge as with the related problem of sub symbolic reasoning it is hoped that situated ai computational intelligence or statistical ai will provide ways to represent this kind of knowledge planning hierarchical control system is form of control system in which set of devices and governing software is arranged in hierarchy intelligent agents must be able to set goals and achieve them they need way to visualize the future they must have representation of the state of the world and be able to make predictions about how their actions will change it and be able to make choices that maximize the utility or value of the available choices in classical planning problems the agent can assume that it is the only thing acting on the world and it can be certain what the consequences of its actions may be however if the agent is not the only actor it must periodically ascertain whether the world matches its predictions and it must change its plan as this becomes necessary requiring the agent to reason under uncertainty multi agent planning uses the cooperation and competition of many agents to achieve given goal emergent behavior such as this is used by evolutionary algorithms and swarm intelligence learning machine learning is the study of computer algorithms that improve automatically through experience and has been central to ai research since the field inception unsupervised learning is the ability to find patterns in stream of input supervised learning includes both classification and numerical regression classification is used to determine what category something belongs in after seeing number of examples of things from several categories regression is the attempt to produce function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change in reinforcement learning the agent is rewarded for good responses and punished for bad ones the agent uses this sequence of rewards and punishments to form strategy for operating in its problem space these three types of learning can be analyzed in terms of decision theory using concepts like utility the mathematical analysis of machine learning algorithms and their performance is branch of theoretical computer science known as computational learning theory within developmental robotics developmental learning approaches were elaborated for lifelong cumulative acquisition of repertoires of novel skills by robot through autonomous self exploration and social interaction with human teachers and using guidance mechanisms such as active learning maturation motor synergies and imitation natural language processing communication parse tree represents the syntactic structure of sentence according to some formal grammar natural language processing gives machines the ability to read and understand the languages that humans speak sufficiently powerful natural language processing system would enable natural language user interfaces and the acquisition of knowledge directly from human written sources such as newswire texts some straightforward applications of natural language processing include information retrieval or text mining question answering and machine translation common method of processing and extracting meaning from natural language is through semantic indexing increases in processing speeds and the drop in the cost of data storage makes indexing large volumes of abstractions of the user input much more efficient perception machine perception is the ability to use input from sensors such as cameras microphones tactile sensors sonar and others more exotic to deduce aspects of the world computer vision is the ability to analyze visual input few selected subproblems are speech recognition facial recognition and object recognition motion and manipulation the field of robotics is closely related to ai intelligence is required for robots to be able to handle such tasks as object manipulation and navigation with sub problems of localization knowing where you are or finding out where other things are mapping learning what is around you building map of the environment and motion planning figuring out how to get there or path planning going from one point in space to another point which may involve compliant motion where the robot moves while maintaining physical contact with an object long term goals among the long term goals in the research pertaining to artificial intelligence are social intelligence creativity and general intelligence social intelligence kismet robot with rudimentary social skills affective computing is the study and development of systems and devices that can recognize interpret process and simulate human affects it is an field spanning computer sciences psychology and cognitive science while the origins of the field may be traced as far back as to early philosophical inquiries into emotion the more modern branch of computer science originated with rosalind picard paper on affective computing motivation for the research is the ability to simulate empathy the machine should interpret the emotional state of humans and adapt its behaviour to them giving an appropriate response for those emotions emotion and social skills play two roles for an intelligent agent first it must be able to predict the actions of others by understanding their motives and emotional states this involves elements of game theory decision theory as well as the ability to model human emotions and the perceptual skills to detect emotions also in an effort to facilitate human computer interaction an intelligent machine might want to be able to display emotions even if it does not actually experience them itself in order to appear sensitive to the emotional dynamics of human interaction creativity sub field of ai addresses creativity both theoretically from philosophical and psychological perspective and practically via specific implementations of systems that generate outputs that can be considered creative or systems that identify and assess creativity related areas of computational research are artificial intuition and artificial thinking general intelligence many researchers think that their work will eventually be incorporated into machine with general intelligence known as strong ai combining all the skills above and exceeding human abilities at most or all of them few believe that anthropomorphic features like artificial consciousness or an artificial brain may be required for such project many of the problems above may require general intelligence to be considered solved for example even straightforward specific task like machine translation requires that the machine read and write in both languages nlp follow the author argument reason know what is being talked about knowledge and faithfully reproduce the author intention social intelligence problem like machine translation is considered ai complete in order to solve this particular problem you must solve all the problems approaches there is no established unifying theory or paradigm that guides ai research researchers disagree about many issues few of the most long standing questions that have remained unanswered are these should artificial intelligence simulate natural intelligence by studying psychology or neurology or is human biology as irrelevant to ai research as bird biology is to aeronautical engineering can intelligent behavior be described using simple elegant principles such as logic or optimization or does it necessarily require solving large number of completely unrelated problems can intelligence be reproduced using high level symbols similar to words and ideas or does it require sub symbolic processing john haugeland who coined the term gofai good old fashioned artificial intelligence also proposed that ai should more properly be referred to as synthetic intelligence term which has since been adopted by some non gofai researchers cybernetics and brain simulation in the and number of researchers explored the connection between neurology information theory and cybernetics some of them built machines that used electronic networks to exhibit rudimentary intelligence such as grey walter turtles and the johns hopkins beast many of these researchers gathered for meetings of the teleological society at princeton university and the ratio club in england by this approach was largely abandoned although elements of it would be revived in the symbolic when access to digital computers became possible in the middle ai research began to explore the possibility that human intelligence could be reduced to symbol manipulation the research was centered in three institutions carnegie mellon university stanford and mit and each one developed its own style of research john haugeland named these approaches to ai good old fashioned ai or gofai during the symbolic approaches had achieved great success at simulating high level thinking in small demonstration programs approaches based on cybernetics or neural networks were abandoned or pushed into the background researchers in the and the were convinced that symbolic approaches would eventually succeed in creating machine with artificial general intelligence and considered this the goal of their field cognitive simulation economist herbert simon and allen newell studied human problem solving skills and attempted to formalize them and their work laid the foundations of the field of artificial intelligence as well as cognitive science operations research and management science their research team used the results of psychological experiments to develop programs that simulated the techniques that people used to solve problems this tradition centered at carnegie mellon university would eventually culminate in the development of the soar architecture in the middle logic based unlike newell and simon john mccarthy felt that machines did not need to simulate human thought but should instead try to find the essence of abstract reasoning and problem solving regardless of whether people used the same algorithms his laboratory at stanford sail focused on using formal logic to solve wide variety of problems including knowledge representation planning and learning logic was also the focus of the work at the university of edinburgh and elsewhere in europe which led to the development of the programming language prolog and the science of logic programming anti logic or scruffy researchers at mit such as marvin minsky and seymour papert found that solving difficult problems in vision and natural language processing required ad hoc solutions they argued that there was no simple and general principle like logic that would capture all the aspects of intelligent behavior roger schank described their anti logic approaches as scruffy as opposed to the neat paradigms at cmu and stanford commonsense knowledge bases such as doug lenat cyc are an example of scruffy ai since they must be built by hand one complicated concept at time knowledge based when computers with large memories became available around researchers from all three traditions began to build knowledge into ai applications this knowledge revolution led to the development and deployment of expert systems introduced by edward feigenbaum the first truly successful form of ai software the knowledge revolution was also driven by the realization that enormous amounts of knowledge would be required by many simple ai applications sub symbolic by the progress in symbolic ai seemed to stall and many believed that symbolic systems would never be able to imitate all the processes of human cognition especially perception robotics learning and pattern recognition number of researchers began to look into sub symbolic approaches to specific ai problems bottom up embodied situated behavior based or nouvelle ai researchers from the related field of robotics such as rodney brooks rejected symbolic ai and focused on the basic engineering problems that would allow robots to move and survive their work revived the non symbolic viewpoint of the early cybernetics researchers of the and reintroduced the use of control theory in ai this coincided with the development of the embodied mind thesis in the related field of cognitive science the idea that aspects of the body such as movement perception and visualization are required for higher intelligence computational intelligence and soft computing interest in neural networks and connectionism was revived by david rumelhart and others in the middle neural networks are an example of soft computing they are solutions to problems which cannot be solved with complete logical certainty and where an approximate solution is often enough other soft computing approaches to ai include fuzzy systems evolutionary computation and many statistical tools the application of soft computing to ai is studied collectively by the emerging discipline of computational intelligence statistical in the ai researchers developed sophisticated mathematical tools to solve specific subproblems these tools are truly scientific in the sense that their results are both measurable and verifiable and they have been responsible for many of ai recent successes the shared mathematical language has also permitted high level of collaboration with more established fields like mathematics economics or operations research stuart russell and peter norvig describe this movement as nothing less than revolution and the victory of the neats critics argue that these techniques with few exceptions are too focused on particular problems and have failed to address the long term goal of general intelligence there is an ongoing debate about the relevance and validity of statistical approaches in ai exemplified in part by exchanges between peter norvig and noam chomsky integrating the approaches intelligent agent paradigm an intelligent agent is system that perceives its environment and takes actions which maximize its chances of success the simplest intelligent agents are programs that solve specific problems more complicated agents include human beings and organizations of human beings such as firms the paradigm gives researchers license to study isolated problems and find solutions that are both verifiable and useful without agreeing on one single approach an agent that solves specific problem can use any approach that works some agents are symbolic and logical some are sub symbolic neural networks and others may use new approaches the paradigm also gives researchers common language to communicate with other fields such as decision theory and economics that also use concepts of abstract agents the intelligent agent paradigm became widely accepted during the agent architectures and cognitive architectures researchers have designed systems to build intelligent systems out of interacting intelligent agents in multi agent system system with both symbolic and sub symbolic components is hybrid intelligent system and the study of such systems is artificial intelligence systems integration hierarchical control system provides bridge between sub symbolic ai at its lowest reactive levels and traditional symbolic ai at its highest levels where relaxed time constraints permit planning and world modelling rodney brooks subsumption architecture was an early proposal for such hierarchical system tools in the course of years of research ai has developed large number of tools to solve the most difficult problems in computer science few of the most general of these methods are discussed below search and optimization many problems in ai can be solved in theory by intelligently searching through many possible solutions reasoning can be reduced to performing search for example logical proof can be viewed as searching for path that leads from premises to conclusions where each step is the application of an inference rule planning algorithms search through trees of goals and subgoals attempting to find path to target goal process called means ends analysis robotics algorithms for moving limbs and grasping objects use local searches in configuration space many learning algorithms use search algorithms based on optimization simple exhaustive searches are rarely sufficient for most real world problems the search space the number of places to search quickly grows to astronomical numbers the result is search that is too slow or never completes the solution for many problems is to use heuristics or rules of thumb that eliminate choices that are unlikely to lead to the goal called pruning the search tree heuristics supply the program with best guess for the path on which the solution lies heuristics limit the search for solutions into smaller sample size very different kind of search came to prominence in the based on the mathematical theory of optimization for many problems it is possible to begin the search with some form of guess and then refine the guess incrementally until no more refinements can be made these algorithms can be visualized as blind hill climbing we begin the search at random point on the landscape and then by jumps or steps we keep moving our guess uphill until we reach the top other optimization algorithms are simulated annealing beam search and random optimization evolutionary computation uses form of optimization search for example they may begin with population of organisms the guesses and then allow them to mutate and recombine selecting only the fittest to survive each generation refining the guesses forms of evolutionary computation include swarm intelligence algorithms such as ant colony or particle swarm optimization and evolutionary algorithms such as genetic algorithms gene expression programming and genetic programming logic logic is used for knowledge representation and problem solving but it can be applied to other problems as well for example the satplan algorithm uses logic for planning and inductive logic programming is method for learning several different forms of logic are used in ai research propositional or sentential logic is the logic of statements which can be true or false first order logic also allows the use of quantifiers and predicates and can express facts about objects their properties and their relations with each other fuzzy logic is version of first order logic which allows the truth of statement to be represented as value between and rather than simply true or false fuzzy systems can be used for uncertain reasoning and have been widely used in modern industrial and consumer product control systems subjective logic models uncertainty in different and more explicit manner than fuzzy logic given binomial opinion satisfies belief disbelief uncertainty within beta distribution by this method ignorance can be distinguished from probabilistic statements that an agent makes with high confidence default logics non monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem several extensions of logic have been designed to handle specific domains of knowledge such as description logics situation calculus event calculus and fluent calculus for representing events and time causal calculus belief calculus and modal logics probabilistic methods for uncertain reasoning many problems in ai in reasoning planning learning perception and robotics require the agent to operate with incomplete or uncertain information ai researchers have devised number of powerful tools to solve these problems using methods from probability theory and economics bayesian networks are very general tool that can be used for large number of problems reasoning using the bayesian inference algorithm learning using the expectation maximization algorithm planning using decision networks and perception using dynamic bayesian networks probabilistic algorithms can also be used for filtering prediction smoothing and finding explanations for streams of data helping perception systems to analyze processes that occur over time hidden markov models or kalman filters key concept from the science of economics is utility measure of how valuable something is to an intelligent agent precise mathematical tools have been developed that analyze how an agent can make choices and plan using decision theory decision analysis and information value theory these tools include models such as markov decision processes dynamic decision networks game theory and mechanism design classifiers and statistical learning methods the simplest ai applications can be divided into two types classifiers if shiny then diamond and controllers if shiny then pick up controllers do however also classify conditions before inferring actions and therefore classification forms central part of many ai systems classifiers are functions that use pattern matching to determine closest match they can be tuned according to examples making them very attractive for use in ai these examples are known as observations or patterns in supervised learning each pattern belongs to certain predefined class class can be seen as decision that has to be made all the observations combined with their class labels are known as data set when new observation is received that observation is classified based on previous experience classifier can be trained in various ways there are many statistical and machine learning approaches the most widely used classifiers are the neural network kernel methods such as the support vector machine nearest neighbor algorithm gaussian mixture model naive bayes classifier and decision tree the performance of these classifiers have been compared over wide range of tasks classifier performance depends greatly on the characteristics of the data to be classified there is no single classifier that works best on all given problems this is also referred to as the no free lunch theorem determining suitable classifier for given problem is still more an art than science neural networks neural network is an interconnected group of nodes akin to the vast network of neurons in the human brain the study of artificial neural networks began in the decade before the field of ai research was founded in the work of walter pitts and warren mccullough other important early researchers were frank rosenblatt who invented the perceptron and paul werbos who developed the backpropagation algorithm the main categories of networks are acyclic or feedforward neural networks where the signal passes in only one direction and recurrent neural networks which allow feedback among the most popular feedforward networks are perceptrons multi layer perceptrons and radial basis networks among recurrent networks the most famous is the hopfield net form of attractor network which was first described by john hopfield in neural networks can be applied to the problem of intelligent control for robotics or learning using such techniques as hebbian learning and competitive learning hierarchical temporal memory is an approach that models some of the structural and algorithmic properties of the neocortex the term deep learning gained traction in the mid after publication by geoffrey hinton and ruslan salakhutdinov showed how many layered feedforward neural network could be effectively pre trained one layer at time treating each layer in turn as an unsupervised restricted boltzmann machine then using supervised backpropagation for fine tuning control theory control theory the grandchild of cybernetics has many important applications especially in robotics languages ai researchers have developed several specialized languages for ai research including lisp and prolog evaluating progress in alan turing proposed general procedure to test the intelligence of an agent now known as the turing test this procedure allows almost all the major problems of artificial intelligence to be tested however it is very difficult challenge and at present all agents fail artificial intelligence can also be evaluated on specific problems such as small problems in chemistry hand writing recognition and game playing such tests have been termed subject matter expert turing tests smaller problems provide more achievable goals and there are an ever increasing number of positive results one classification for outcomes of an ai test is optimal it is not possible to perform better strong super human performs better than all humans super human performs better than most humans sub human performs worse than most humans for example performance at draughts checkers is optimal performance at chess is super human and nearing strong super human see computer chess computers versus human and performance at many everyday tasks such as recognizing face or crossing room without bumping into something is sub human quite different approach measures machine intelligence through tests which are developed from mathematical definitions of intelligence examples of these kinds of tests start in the late nineties devising intelligence tests using notions from kolmogorov complexity and data compression two major advantages of mathematical definitions are their applicability to nonhuman intelligences and their absence of requirement for human testers derivative of the turing test is the completely automated public turing test to tell computers and humans apart captcha as the name implies this helps to determine that user is an actual person and not computer posing as human in contrast to the standard turing test captcha administered by machine and targeted to human as opposed to being administered by human and targeted to machine computer asks user to complete simple test then generates grade for that test computers are unable to solve the problem so correct solutions are deemed to be the result of person taking the test common type of captcha is the test that requires the typing of distorted letters numbers or symbols that appear in an image undecipherable by computer applications an automated online assistant providing customer service on web page one of many very primitive applications of artificial intelligence artificial intelligence techniques are pervasive and are too numerous to list frequently when technique reaches mainstream use it is no longer considered artificial intelligence this phenomenon is described as the ai effect an area that artificial intelligence has contributed greatly to is intrusion detection competitions and prizes there are number of competitions and prizes to promote research in artificial intelligence the main areas promoted are general machine intelligence conversational behavior data mining robotic cars robot soccer and games platforms platform or computing platform is defined as some sort of hardware architecture or software framework including application frameworks that allows software to run as rodney brooks pointed out many years ago it is not just the artificial intelligence software that defines the ai features of the platform but rather the actual platform itself that affects the ai that results there needs to be work in ai problems on real world platforms rather than in isolation wide variety of platforms has allowed different aspects of ai to develop ranging from expert systems albeit pc based but still an entire real world system to various robot platforms such as the widely available roomba with open interface toys aibo the first robotic pet grew out of sony computer science laboratory csl famed engineer toshitada doi is credited as aibo original progenitor in he had started work on robots with artificial intelligence expert masahiro fujita at csl doi friend the artist hajime sorayama was enlisted to create the initial designs for the aibo body those designs are now part of the permanent collections of museum of modern art and the smithsonian institution with later versions of aibo being used in studies in carnegie mellon university in aibo was added into carnegie mellon university robot hall of fame philosophy and ethics there are three philosophical questions related to ai is artificial general intelligence possible can machine solve any problem that human being can solve using intelligence or are there hard limits to what machine can accomplish are intelligent machines dangerous how can we ensure that machines behave ethically and that they are used ethically can machine have mind consciousness and mental states in exactly the same sense that human beings do can it machine be sentient and thus deserve certain rights can machine intentionally cause harm the limits of artificial general intelligence can machine be intelligent can it think turing polite convention we need not decide if machine can think we need only decide if machine can act as intelligently as human being this approach to the philosophical problems associated with artificial intelligence forms the basis of the turing test the dartmouth proposal every aspect of learning or any other feature of intelligence can be so precisely described that machine can be made to simulate it this conjecture was printed in the proposal for the dartmouth conference of and represents the position of most working ai researchers newell and simon physical symbol system hypothesis physical symbol system has the necessary and sufficient means of general intelligent action newell and simon argue that intelligence consists of formal operations on symbols hubert dreyfus argued that on the contrary human expertise depends on unconscious instinct rather than conscious symbol manipulation and on having feel for the situation rather than explicit symbolic knowledge see dreyfus critique of ai g\u00f6delian arguments g\u00f6del himself john lucas in and roger penrose in more detailed argument from onwards argued that humans are not reducible to turing machines the detailed arguments are complex but in essence they derive from kurt g\u00f6del proof in his first incompleteness theorem that it is always possible to create statements that formal system could not prove human being however can with some thought see the truth of these g\u00f6del statements any turing program designed to search for these statements can have its methods reduced to formal system and so will always have g\u00f6del statement derivable from its program which it can never discover however if humans are indeed capable of understanding mathematical truth it doesn seem possible that we could be limited in the same way this is quite general result if accepted since it can be shown that hardware neural nets and computers based on random processes annealing approaches and quantum computers based on entangled qubits so long as they involve no new physics can all be reduced to turing machines all they do is reduce the complexity of the tasks not permit new types of problems to be solved roger penrose speculates that there may be new physics involved in our brain perhaps at the intersection of gravity and quantum mechanics at the planck scale this argument if accepted does not rule out the possibility of true artificial intelligence but means it has to be biological in basis or based on new physical principles the argument has been followed up by many counter arguments and then roger penrose has replied to those with counter counter examples and it is now an intricate complex debate for details see philosophy of artificial intelligence lucas penrose and g\u00f6del the artificial brain argument the brain can be simulated by machines and because brains are intelligent simulated brains must also be intelligent thus machines can be intelligent hans moravec ray kurzweil and others have argued that it is technologically feasible to copy the brain directly into hardware and software and that such simulation will be essentially identical to the original the ai effect machines are already intelligent but observers have failed to recognize it when deep blue beat gary kasparov in chess the machine was acting intelligently however onlookers commonly discount the behavior of an artificial intelligence program by arguing that it is not real intelligence after all thus real intelligence is whatever intelligent behavior people can do that machines still can not this is known as the ai effect ai is whatever hasn been done yet intelligent behaviour and machine ethics as minimum an ai system must be able to reproduce aspects of human intelligence this raises the issue of how ethically the machine should behave towards both humans and other ai agents this issue was addressed by wendell wallach in his book titled moral machines in which he introduced the concept of artificial moral agents ama for wallach amas have become part of the research landscape of artificial intelligence as guided by its two central questions which he identifies as does humanity want computers making moral decisions and can ro bots really be moral for wallach the question is not centered on the issue of whether machines can demonstrate the equivalent of moral behavior in contrast to the constraints which society may place on the development of amas machine ethics the field of machine ethics is concerned with giving machines ethical principles or procedure for discovering way to resolve the ethical dilemmas they might encounter enabling them to function in an ethically responsible manner through their own ethical decision making the field was delineated in the aaai fall symposium on machine ethics past research concerning the relationship between technology and ethics has largely focused on responsible and irresponsible use of technology by human beings with few people being interested in how human beings ought to treat machines in all cases only human beings have engaged in ethical reasoning the time has come for adding an ethical dimension to at least some machines recognition of the ethical ramifications of behavior involving machines as well as recent and potential developments in machine autonomy necessitate this in contrast to computer hacking software property issues privacy issues and other topics normally ascribed to computer ethics machine ethics is concerned with the behavior of machines towards human users and other machines research in machine ethics is key to alleviating concerns with autonomous systems it could be argued that the notion of autonomous machines without such dimension is at the root of all fear concerning machine intelligence further investigation of machine ethics could enable the discovery of problems with current ethical theories advancing our thinking about ethics machine ethics is sometimes referred to as machine morality computational ethics or computational morality variety of perspectives of this nascent field can be found in the collected edition machine ethics that stems from the aaai fall symposium on machine ethics malevolent and friendly ai political scientist charles rubin believes that ai can be neither designed nor guaranteed to be benevolent he argues that any sufficiently advanced benevolence may be from malevolence humans should not assume machines or robots would treat us favorably because there is no priori reason to believe that they would be sympathetic to our system of morality which has evolved along with our particular biology which ais would not share hyper intelligent software may not necessarily decide to support the continued existence of mankind and would be extremely difficult to stop this topic has also recently begun to be discussed in academic publications as real source of risks to civilization humans and planet earth physicist stephen hawking microsoft founder bill gates and spacex founder elon musk have expressed concerns about the possibility that ai could evolve to the point that humans could not control it with hawking theorizing that this could spell the end of the human race one proposal to deal with this is to ensure that the first generally intelligent ai is friendly ai and will then be able to control subsequently developed ais some question whether this kind of check could really remain in place leading ai researcher rodney brooks writes think it is mistake to be worrying about us developing malevolent ai anytime in the next few hundred years think the worry stems from fundamental error in not distinguishing the difference between the very real recent advances in particular aspect of ai and the enormity and complexity of building sentient volitional intelligence devaluation of humanity joseph weizenbaum wrote that ai applications can not by definition successfully simulate genuine human empathy and that the use of ai technology in fields such as customer service or psychotherapy was deeply misguided weizenbaum was also bothered that ai researchers and some philosophers were willing to view the human mind as nothing more than computer program position now known as to weizenbaum these points suggest that ai research devalues human life decrease in demand for human labor martin ford author of the lights in the tunnel automation accelerating technology and the economy of the future and others argue that specialized artificial intelligence applications robotics and other forms of automation will ultimately result in significant unemployment as machines begin to match and exceed the capability of workers to perform most routine and repetitive jobs ford predicts that many knowledge based occupations and in particular entry level jobs will be increasingly susceptible to automation via expert systems machine learning and other ai enhanced applications ai based applications may also be used to amplify the capabilities of low wage offshore workers making it more feasible to outsource knowledge work machine consciousness sentience and mind if an ai system replicates all key aspects of human intelligence will that system also be sentient will it have mind which has conscious experiences this question is closely related to the philosophical problem as to the nature of human consciousness generally referred to as the hard problem of consciousness consciousness there are no objective criteria for knowing whether an intelligent agent is sentient that it has conscious experiences we assume that other people do because we do and they tell us that they do but this is only subjective determination the lack of any hard criteria is known as the hard problem in the theory of consciousness the problem applies not only to other people but to the higher animals and by extension to ai agents are human intelligence consciousness and mind products of information processing is the brain essentially computer is the idea that the human mind or the human brain or both is an information processing system and that thinking is form of computing ai or implementing machines with human intelligence was founded on the claim that central property of humans intelligence can be so precisely described that machine can be made to simulate it program can then be derived from this human computer and implemented into an artificial one to create efficient artificial intelligence this program would act upon set of outputs that result from set inputs of the internal memory of the computer that is the machine can only act with what it has implemented in it to start with long term goal for ai researchers is to provide machines with deep understanding of the many abilities of human being to replicate general intelligence or strong ai defined as machine surpassing human abilities to perform the skills implanted in it scary thought to many who fear losing control of such powerful machine obstacles for researchers are mainly time contstraints that is ai scientists cannot establish much of database for commonsense knowledge because it must be ontologically crafted into the machine which takes up tremendous amount of time to combat this ai research looks to have the machine able to understand enough concepts in order to add to its own ontology but how can it do this when machine ethics is primarily concerned with behavior of machines towards humans or other machines limiting the extent of developing ai in order to function like common human ai must also display the ability to solve subsymbolic commonsense knowledge tasks such as how artists can tell statues are fake or how chess masters don move certain spots to avoid exposure but by developing machines who can do it all ai research is faced with the difficulty of potentially putting lot of people out of work while on the economy side of things businesses would boom from efficiency thus forcing ai into bottleneck trying to developing self improving machines strong ai hypothesis searle strong ai hypothesis states that the appropriately programmed computer with the right inputs and outputs would thereby have mind in exactly the same sense human beings have minds john searle counters this assertion with his chinese room argument which asks us to look inside the computer and try to find where the mind might be robot rights mary shelley frankenstein considers key issue in the ethics of artificial intelligence if machine can be created that has intelligence could it also feel if it can feel does it have the same rights as human the idea also appears in modern science fiction such as the film artificial intelligence in which humanoid machines have the ability to feel emotions this issue now known as robot rights is currently being considered by for example california institute for the future although many critics believe that the discussion is premature the subject is profoundly discussed in the documentary film plug pray are there limits to how intelligent machines or human machine hybrids can be or superhuman intelligence is hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind may also refer to the form or degree of intelligence possessed by such an agent technological singularity if research into strong ai produced sufficiently intelligent software it might be able to reprogram and improve itself the improved software would be even better at improving itself leading to recursive self improvement the new intelligence could thus increase exponentially and dramatically surpass humans science fiction writer vernor vinge named this scenario singularity technological singularity is when accelerating progress in technologies will cause runaway effect wherein artificial intelligence will exceed human intellectual capacity and control thus radically changing or even ending civilization because the capabilities of such an intelligence may be impossible to comprehend the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable ray kurzweil has used moore law which describes the relentless exponential improvement in digital technology to calculate that desktop computers will have the same processing power as human brains by the year and predicts that the singularity will occur in transhumanism robot designer hans moravec cyberneticist kevin warwick and inventor ray kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either this idea called transhumanism which has roots in aldous huxley and robert ettinger has been illustrated in fiction as well for example in the manga ghost in the shell and the science fiction series dune in the artist hajime sorayama sexy robots series were painted and published in japan depicting the actual organic human form with lifelike muscular metallic skins and later the gynoids book followed that was used by or influenced movie makers including george lucas and other creatives sorayama never considered these organic robots to be real part of nature but always unnatural product of the human mind fantasy existing in the mind even when realized in actual form edward fredkin argues that artificial intelligence is the next stage in evolution an idea first proposed by samuel butler darwin among the machines and expanded upon by george dyson in his book of the same name in in fiction the implications of artificial intelligence have been persistent theme in science fiction early stories typically revolved around intelligent robots the word robot itself was coined by karel \u010dapek in his play the title standing for rossum universal robots later the sf writer isaac asimov developed the three laws of robotics which he subsequently explored in long series of robot stories these laws have since gained some traction in genuine ai research other influential fictional intelligences include hal the computer in charge of the spaceship in space odyssey released as both film and book in and written by arthur clarke since then ai has become firmly rooted in popular culture see also ai takeover artificial intelligence journal artificial intelligence video games artificial stupidity nick bostrom computer go effective altruism existential risk existential risk of artificial general intelligence future of humanity institute human cognome project list of artificial intelligence projects list of artificial intelligence researchers list of emerging technologies list of important artificial intelligence publications list of machine learning algorithms list of scientific journals machine ethics machine learning never ending language learning our final invention outline of artificial intelligence outline of human intelligence philosophy of mind simulated reality symbolic artificial intelligence notes references ai textbooks history of ai other sources bibtex in cited by presidential address to the association for the advancement of artificial intelligence later published as further reading techcast article series john sagi framing consciousness boden margaret mind as machine oxford university press johnston john the allure of machinic life cybernetics artificial life and the new ai mit press myers courtney boyd ed the ai report forbes june sun bookman eds computational architectures integrating neural and symbolic processes kluwer academic publishers needham ma external links what is ai an introduction to artificial intelligence by ai founder john mccarthy the handbook of artificial intelligence volume by avron barr and edward feigenbaum stanford university aitopics large directory of links and other resources maintained by the association for the advancement of artificial intelligence the leading organization of academic ai researchers", "Programming language": "source code of simple computer program written in the programming language which will output the hello world message when compiled and run programming language is formal constructed language designed to communicate instructions to machine particularly computer programming languages can be used to create programs to control the behavior of machine or to express algorithms the earliest programming languages preceded the invention of the digital computer and were used to direct the behavior of machines such as jacquard looms and player pianos thousands of different programming languages have been created mainly in the computer field and many more still are being created every year many programming languages require computation to be specified in an imperative form as sequence of operations to perform while other languages use other forms of program specification such as the declarative form the desired result is specified not how to achieve it the description of programming language is usually split into the two components of syntax form and semantics meaning some languages are defined by specification document for example the programming language is specified by an iso standard while other languages such as perl have dominant implementation that is treated as reference definitions programming language is notation for writing programs which are specifications of computation or algorithm some but not all authors restrict the term programming language to those languages that can express all possible algorithms traits often considered important for what constitutes programming language include function and target computer programming language is language used to write computer programs which involve computer performing some kind of computation or algorithm and possibly control external devices such as printers disk drives robots and so on for example postscript programs are frequently created by another program to control computer printer or display more generally programming language may describe computation on some possibly abstract machine it is generally accepted that complete specification for programming language includes description possibly idealized of machine or processor for that language in most practical contexts programming language involves computer consequently programming languages are usually defined and studied this way programming languages differ from natural languages in that natural languages are only used for interaction between people while programming languages also allow humans to communicate instructions to machines abstractions programming languages usually contain abstractions for defining and manipulating data structures or controlling the flow of execution the practical necessity that programming language support adequate abstractions is expressed by the abstraction principle this principle is sometimes formulated as recommendation to the programmer to make proper use of such abstractions expressive power the theory of computation classifies languages by the computations they are capable of expressing all turing complete languages can implement the same set of algorithms ansi iso sql and charity are examples of languages that are not turing complete yet often called programming languages markup languages like xml html or troff which define structured data are not usually considered programming languages programming languages may however share the syntax with markup languages if computational semantics is defined xslt for example is turing complete xml dialect moreover latex which is mostly used for structuring documents also contains turing complete subset the term computer language is sometimes used interchangeably with programming language however the usage of both terms varies among authors including the exact scope of each one usage describes programming languages as subset of computer languages in this vein languages used in computing that have different goal than expressing computer programs are generically designated computer languages for instance markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming another usage regards programming languages as theoretical constructs for programming abstract machines and computer languages as the subset thereof that runs on physical computers which have finite hardware resources john reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution he also argues that textual and even graphical input formats that affect the behavior of computer are programming languages despite the fact they are commonly not turing complete and remarks that ignorance of programming language concepts is the reason for many flaws in input formats history early developments the first programming languages designed to communicate instructions to computer were written in the an early high level programming language to be designed for computer was plankalk\u00fcl developed for the german by konrad zuse between and however it was not implemented until and john mauchly short code proposed in was one of the first high level languages ever developed for an electronic computer unlike machine code short code statements represented mathematical expressions in understandable form however the program had to be translated into machine code every time it ran making the process much slower than running the equivalent machine code the manchester mark ran programs written in autocode from at the university of manchester alick glennie developed autocode in the early programming language it used compiler to automatically convert the language into machine code the first code and compiler was developed in for the mark computer at the university of manchester and is considered to be the first compiled high level programming language the second autocode was developed for the mark by brooker in and was called the mark autocode brooker also developed an autocode for the ferranti mercury in the in conjunction with the university of manchester the version for the edsac was devised by hartley of university of cambridge mathematical laboratory in known as edsac autocode it was straight development from mercury autocode adapted for local circumstances and was noted for its object code optimisation and source language diagnostics which were advanced for the time contemporary but separate thread of development atlas autocode was developed for the university of manchester atlas machine another early programming language was devised by grace hopper in the us called flow matic it was developed for the univac at remington rand during the period from until hopper found that business data processing customers were uncomfortable with mathematical notation and in early she and her team wrote specification for an english programming language and implemented prototype the flow matic compiler became publicly available in early and was substantially complete in flow matic was major influence in the design of cobol since only it and its direct descendent aimaco were in actual use at the time the language fortran was developed at ibm in the mid and became the first widely used high level general purpose programming language refinement the period from the to the late brought the development of the major language paradigms now in use apl introduced array programming and influenced functional programming algol refined both structured procedural programming and the discipline of language specification the revised report on the algorithmic language algol became model for how later language specifications were written in the simula was the first language designed to support object oriented programming in the mid smalltalk followed with the first purely object oriented language was developed between and as system programming language and remains popular prolog designed in was the first logic programming language in ml built polymorphic type system on top of lisp pioneering statically typed functional programming languages each of these languages spawned descendants and most modern programming languages count at least one of them in their ancestry the and also saw considerable debate over the merits of structured programming and whether programming languages should be designed to support it edsger dijkstra in famous letter published in the communications of the acm argued that goto statements should be eliminated from all higher level programming languages consolidation and growth selection of textbooks that teach programming in languages both popular and obscure these are only few of the thousands of programming languages and dialects that have been designed in history the were years of relative consolidation combined object oriented and systems programming the united states government standardized ada systems programming language derived from pascal and intended for use by defense contractors in japan and elsewhere vast sums were spent investigating so called fifth generation languages that incorporated logic programming constructs the functional languages community moved to standardize ml and lisp rather than inventing new paradigms all of these movements elaborated upon the ideas invented in the previous decade one important trend in language design for programming large scale systems during the was an increased focus on the use of modules or large scale organizational units of code modula ada and ml all developed notable module systems in the which were often wedded to generic programming constructs the rapid growth of the internet in the mid created opportunities for new languages perl originally unix scripting tool first released in became common in dynamic websites java came to be used for server side programming and bytecode virtual machines became popular again in commercial settings with their promise of write once run anywhere ucsd pascal had been popular for time in the early these developments were not fundamentally novel rather they were refinements to existing languages and paradigms and largely based on the family of programming languages programming language evolution continues in both industry and research current directions include security and reliability verification new kinds of modularity mixins delegates aspects and database integration such as microsoft linq the gls are examples of languages which are domain specific such as sql which manipulates and returns sets of data rather than the scalar values which are canonical to most programming languages perl for example with its here document can hold multiple gl programs as well as multiple javascript programs in part of its own perl code and use variable interpolation in the here document to support multi language programming elements all programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them like the addition of two numbers or the selection of an item from collection these primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively syntax parse tree of python code with inset tokenization syntax highlighting is often used to aid programmers in recognizing elements of source code the language above is python programming language surface form is known as its syntax most programming languages are purely textual they use sequences of text including words numbers and punctuation much like written natural languages on the other hand there are some programming languages which are more graphical in nature using visual relationships between symbols to specify program the syntax of language describes the possible combinations of symbols that form syntactically correct program the meaning given to combination of symbols is handled by semantics either formal or hard coded in reference implementation since most languages are textual this article discusses textual syntax programming language syntax is usually defined using combination of regular expressions for lexical structure and backus naur form for grammatical structure below is simple grammar based on lisp expression atom list atom number symbol number symbol list expression this grammar specifies the following an expression is either an atom or list an atom is either number or symbol number is an unbroken sequence of one or more decimal digits optionally preceded by plus or minus sign symbol is letter followed by zero or more of any characters excluding whitespace and list is matched pair of parentheses with zero or more expressions inside it the following are examples of well formed token sequences in this grammar and not all syntactically correct programs are semantically correct many syntactically correct programs are nonetheless ill formed per the language rules and may depending on the language specification and the soundness of the implementation result in an error on translation or execution in some cases such programs may exhibit undefined behavior even when program is well defined within language it may still have meaning that is not intended by the person who wrote it using natural language as an example it may not be possible to assign meaning to grammatically correct sentence or the sentence may be false colorless green ideas sleep furiously is grammatically well formed but has no generally accepted meaning john is married bachelor is grammatically well formed but expresses meaning that cannot be true the following language fragment is syntactically correct but performs operations that are not semantically defined the operation has no meaning for value having complex type and im is not defined because the value of is the null pointer complex null complex abs_p sqrt im if the type declaration on the first line were omitted the program would trigger an error on compilation as the variable would not be defined but the program would still be syntactically correct since type declarations provide only semantic information the grammar needed to specify programming language can be classified by its position in the chomsky hierarchy the syntax of most programming languages can be specified using type grammar they are context free grammars some languages including perl and lisp contain constructs that allow execution during the parsing phase languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem and generally blur the distinction between parsing and execution in contrast to lisp macro system and perl begin blocks which may contain general computations macros are merely string replacements and do not require code execution semantics the term semantics refers to the meaning of languages as opposed to their form syntax static semantics the static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms for compiled languages static semantics essentially include those semantic rules that can be checked at compile time examples include checking that every identifier is declared before it is used in languages that require such declarations or that the labels on the arms of case statement are distinct many important restrictions of this type like checking that identifiers are used in the appropriate context not adding an integer to function name or that subroutine calls have the appropriate number and type of arguments can be enforced by defining them as rules in logic called type system other forms of static analyses like data flow analysis may also be part of static semantics newer programming languages like java and have definite assignment analysis form of data flow analysis as part of their static semantics dynamic semantics once data has been specified the machine must be instructed to perform operations on the data for example the semantics may define the strategy by which expressions are evaluated to values or the manner in which control structures conditionally execute statements the dynamic semantics also known as execution semantics of language defines how and when the various constructs of language should produce program behavior there are many ways of defining execution semantics natural language is often used to specify the execution semantics of languages commonly used in practice significant amount of academic research went into formal semantics of programming languages which allow execution semantics to be specified in formal manner results from this field of research have seen limited application to programming language design and implementation outside academia type system type system defines how programming language classifies values and expressions into types how it can manipulate those types and how they interact the goal of type system is to verify and usually enforce certain level of correctness in programs written in that language by detecting certain incorrect operations any decidable type system involves trade off while it rejects many incorrect programs it can also prohibit some correct albeit unusual programs in order to bypass this downside number of languages have type loopholes usually unchecked casts that may be used by the programmer to explicitly allow normally disallowed operation between different types in most typed languages the type system is used only to type check programs but number of languages usually functional ones infer types relieving the programmer from the need to write type annotations the formal design and study of type systems is known as type theory typed versus untyped languages language is typed if the specification of every operation defines types of data to which the operation is applicable with the implication that it is not applicable to other types for example the data represented by this text between the quotes is string and in many programming languages dividing number by string has no meaning and will be rejected by the compilers the invalid operation may be detected when the program is compiled static type checking and will be rejected by the compiler with compilation error message or it may be detected when the program is run dynamic type checking resulting in run time exception many languages allow function called an exception handler to be written to handle this exception and for example always return as the result special case of typed languages are the single type languages these are often scripting or markup languages such as rexx or sgml and have only one data type most commonly character strings which are used for both symbolic and numeric data in contrast an untyped language such as most assembly languages allows any operation to be performed on any data which are generally considered to be sequences of bits of various lengths high level languages which are untyped include bcpl tcl and some varieties of forth in practice while few languages are considered typed from the point of view of type theory verifying or rejecting all operations most modern languages offer degree of typing many production languages provide means to bypass or subvert the type system trading type safety for finer control over the program execution see casting static versus dynamic typing in static typing all expressions have their types determined prior to when the program is executed typically at compile time for example and are integer expressions they cannot be passed to function that expects string or stored in variable that is defined to hold dates statically typed languages can be either manifestly typed or type inferred in the first case the programmer must explicitly write types at certain textual positions for example at variable declarations in the second case the compiler infers the types of expressions and declarations based on context most mainstream statically typed languages such as and java are manifestly typed complete type inference has traditionally been associated with less mainstream languages such as haskell and ml however many manifestly typed languages support partial type inference for example java and both infer types in certain limited cases additionally some programming languages allow for some types to be automatically converted to other types for example an int can be used where the program expects float dynamic typing also called latent typing determines the type safety of operations at run time in other words types are associated with run time values rather than textual expressions as with type inferred languages dynamically typed languages do not require the programmer to write explicit type annotations on expressions among other things this may permit single variable to refer to values of different types at different points in the program execution however type errors cannot be automatically detected until piece of code is actually executed potentially making debugging more difficult lisp perl python javascript and ruby are dynamically typed weak and strong typing weak typing allows value of one type to be treated as another for example treating string as number this can occasionally be useful but it can also allow some kinds of program faults to go undetected at compile time and even at run time strong typing prevents the above an attempt to perform an operation on the wrong type of value raises an error strongly typed languages are often termed type safe or safe an alternative definition for weakly typed refers to languages such as perl and javascript which permit large number of implicit type conversions in javascript for example the expression implicitly converts to number and this conversion succeeds even if is null undefined an array or string of letters such implicit conversions are often useful but they can mask programming errors strong and static are now generally considered orthogonal concepts but usage in the literature differs some use the term strongly typed to mean strongly statically typed or even more confusingly to mean simply statically typed thus has been called both strongly typed and weakly statically typed it may seem odd to some professional programmers that could be weakly statically typed however notice that the use of the generic pointer the void pointer does allow for casting of pointers to other pointers without needing to do an explicit cast this is extremely similar to somehow casting an array of bytes to any kind of datatype in without using an explicit cast such as int or char standard library and run time system most programming languages have an associated core library sometimes known as the standard library especially if it is included as part of the published language standard which is conventionally made available by all implementations of the language core libraries typically include definitions for commonly used algorithms data structures and mechanisms for input and output the line between language and its core library differs from language to language in some cases the language designers may treat the library as separate entity from the language however language core library is often treated as part of the language by its users and some language specifications even require that this library be made available in all implementations indeed some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library for example in java string literal is defined as an instance of the java lang string class similarly in smalltalk an anonymous function expression block constructs an instance of the library blockcontext class conversely scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros and so the language designers do not even bother to say which portions of the language must be implemented as language constructs and which must be implemented as parts of library design and implementation programming languages share properties with natural languages related to their purpose as vehicles for communication having syntactic form separate from its semantics and showing language families of related languages branching one from another but as artificial constructs they also differ in fundamental ways from languages that have evolved through usage significant difference is that programming language can be fully described and studied in its entirety since it has precise and finite definition by contrast natural languages have changing meanings given by their users in different communities while constructed languages are also artificial languages designed from the ground up with specific purpose they lack the precise and complete semantic definition that programming language has many programming languages have been designed from scratch altered to meet new needs and combined with other languages many have eventually fallen into disuse although there have been attempts to design one universal programming language that serves all purposes all of them have failed to be generally accepted as filling this role the need for diverse programming languages arises from the diversity of contexts in which languages are used programs range from tiny scripts written by individual hobbyists to huge systems written by hundreds of programmers programmers range in expertise from novices who need simplicity above all else to experts who may be comfortable with considerable complexity programs must balance speed size and simplicity on systems ranging from to supercomputers programs may be written once and not change for generations or they may undergo continual modification programmers may simply differ in their tastes they may be accustomed to discussing problems and expressing them in particular language one common trend in the development of programming languages has been to add more ability to solve problems using higher level of abstraction the earliest programming languages were tied very closely to the underlying hardware of the computer as new programming languages have developed features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions because programmers are less tied to the complexity of the computer their programs can do more computing with less effort from the programmer this lets them write more functionality per time unit natural language programming has been proposed as way to eliminate the need for specialized language for programming however this goal remains distant and its benefits are open to debate edsger dijkstra took the position that the use of formal language is essential to prevent the introduction of meaningless constructs and dismissed natural language programming as foolish alan perlis was similarly dismissive of the idea hybrid approaches have been taken in structured english and sql language designers and users must construct number of artifacts that govern and enable the practice of programming the most important of these artifacts are the language specification and implementation specification the specification of programming language is an artifact that the language users and the implementors can use to agree upon whether piece of source code is valid program in that language and if so what its behavior shall be programming language specification can take several forms including the following an explicit definition of the syntax static semantics and execution semantics of the language while syntax is commonly specified using formal grammar semantic definitions may be written in natural language as in the language or formal semantics as in standard ml and scheme specifications description of the behavior of translator for the language the and fortran specifications the syntax and semantics of the language have to be inferred from this description which may be written in natural or formal language reference or model implementation sometimes written in the language being specified prolog or ansi rexx the syntax and semantics of the language are explicit in the behavior of the reference implementation implementation an implementation of programming language provides way to write programs in that language and execute them on one or more configurations of hardware and software there are broadly two approaches to programming language implementation compilation and interpretation it is generally possible to implement language using either technique the output of compiler may be executed by hardware or program called an interpreter in some implementations that make use of the interpreter approach there is no distinct boundary between compiling and interpreting for instance some implementations of basic compile and then execute the source line at time programs that are executed directly on the hardware usually run several orders of magnitude faster than those that are interpreted in software one technique for improving the performance of interpreted programs is just in time compilation here the virtual machine just before execution translates the blocks of bytecode which are going to be used to machine code for direct execution on the hardware proprietary languages although most of the most commonly used programming languages have fully open specifications and implementations many programming languages exist only as proprietary programming languages with the implementation available only from single vendor which may claim that such proprietary language is their intellectual property proprietary programming languages are commonly domain specific languages or internal scripting languages for single product some proprietary languages are used only internally within vendor while others are available to external users some programming languages exist on the border between proprietary and open for example oracle corporation asserts proprietary rights to some aspects of the java programming language and microsoft programming language which has open implementations of most parts of the system also has common language runtime clr as closed environment many proprietary languages are widely used in spite of their proprietary nature examples include matlab and vbscript some languages may make the transition from closed to open for example erlang was originally an ericsson internal programming language usage thousands of different programming languages have been created mainly in the computing field software is commonly built with programming languages or more programming languages differ from most other forms of human expression in that they require greater degree of precision and completeness when using natural language to communicate with other people human authors and speakers can be ambiguous and make small errors and still expect their intent to be understood however figuratively speaking computers do exactly what they are told to do and cannot understand what code the programmer intended to write the combination of the language definition program and the program inputs must fully specify the external behavior that occurs when the program is executed within the domain of control of that program on the other hand ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode which interleaves natural language with code written in programming language programming language provides structured mechanism for defining pieces of data and the operations or transformations that may be carried out automatically on that data programmer uses the abstractions present in the language to represent the concepts involved in computation these concepts are represented as collection of the simplest elements available called primitives programming is the process by which programmers combine these primitives to compose new programs or adapt existing ones to new uses or changing environment programs for computer might be executed in batch process without human interaction or user might type commands in an interactive session of an interpreter in this case the commands are simply programs whose execution is chained together when language can run its commands through an interpreter such as unix shell or other command line interface without compiling it is called scripting language measuring language usage it is difficult to determine which programming languages are most widely used and what usage means varies by context one language may occupy the greater number of programmer hours different one have more lines of code and third may consume the most cpu time some languages are very popular for particular kinds of applications for example cobol is still strong in the corporate data center often on large mainframes fortran in scientific and engineering applications ada in aerospace transportation military real time and embedded applications and in embedded applications and operating systems other languages are regularly used to write many different kinds of applications various methods of measuring language popularity each subject to different bias over what is measured have been proposed counting the number of job advertisements that mention the language the number of books sold that teach or describe the language estimates of the number of existing lines of code written in the language which may underestimate languages not often found in public searches counts of language references to the name of the language found using web search engine combining and averaging information from various internet sites langpop com claims that in the ten most popular programming languages are in descending order by overall popularity java php javascript python shell ruby objective and taxonomies there is no overarching classification scheme for programming languages given programming language does not usually have single ancestor language languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time ideas that originate in one language will diffuse throughout family of related languages and then leap suddenly across familial gaps to appear in an entirely different family the task is further complicated by the fact that languages can be classified along multiple axes for example java is both an object oriented language because it encourages object oriented organization and concurrent language because it contains built in constructs for running multiple threads in parallel python is an object oriented scripting language in broad strokes programming languages divide into programming paradigms and classification by intended domain of use with general purpose programming languages distinguished from domain specific programming languages traditionally programming languages have been regarded as describing computation in terms of imperative sentences issuing commands these are generally called imperative programming languages great deal of research in programming languages has been aimed at blurring the distinction between program as set of instructions and program as an assertion about the desired answer which is the main feature of declarative programming more refined paradigms include procedural programming object oriented programming functional programming and logic programming some languages are hybrids of paradigms or multi paradigmatic an assembly language is not so much paradigm as direct model of an underlying machine architecture by purpose programming languages might be considered general purpose system programming languages scripting languages domain specific languages or concurrent distributed languages or combination of these some general purpose languages were designed largely with educational goals programming language may also be classified by factors unrelated to programming paradigm for instance most programming languages use english language keywords while minority do not other languages may be classified as being deliberately esoteric or not see also comparison of programming languages basic instructions comparison of programming languages computer programming computer science and outline of computer science educational programming language invariant based programming lists of programming languages list of programming language researchers programming languages used in most popular websites literate programming dialect computing programming language theory pseudocode scientific language software engineering and list of software engineering topics references further reading raphael finkel advanced programming language design addison wesley daniel friedman mitchell wand christopher haynes essentials of programming languages the mit press maurizio gabbrielli and simone martini programming languages principles and paradigms springer david gelernter suresh jagannathan programming linguistics the mit press ellis horowitz ed programming languages grand tour rd ed ellis horowitz fundamentals of programming languages shriram krishnamurthi programming languages application and interpretation online publication bruce maclennan principles of programming languages design evaluation and implementation oxford university press john mitchell concepts in programming languages cambridge university press benjamin pierce types and programming languages the mit press terrence pratt and marvin zelkowitz programming languages design and implementation th ed prentice hall peter salus handbook of programming languages vols macmillan ravi sethi programming languages concepts and constructs nd ed addison wesley michael scott programming language pragmatics morgan kaufmann publishers robert sebesta concepts of programming languages th ed addison wesley franklyn turbak and david gifford with mark sheldon design concepts in programming languages the mit press peter van roy and seif haridi concepts techniques and models of computer programming the mit press david watt programming language concepts and paradigms prentice hall david watt and muffy thomas programming language syntax and semantics prentice hall david watt programming language processors prentice hall david watt programming language design concepts john wiley sons external links bottles of beer collection of implementations in many languages", "Distributed computing": "distributed computing is field of computer science that studies distributed systems distributed system is software system in which components located on networked computers communicate and coordinate their actions by passing messages the components interact with each other in order to achieve common goal three significant characteristics of distributed systems are concurrency of components lack of global clock and independent failure of components examples of distributed systems vary from soa based systems to massively multiplayer online games to peer to peer applications computer program that runs in distributed system is called distributed program and distributed programming is the process of writing such programs there are many alternatives for the message passing mechanism including rpc like connectors and message queues goal and challenge pursued by some computer scientists and practitioners in distributed systems is location transparency however this goal has fallen out of favour in industry as distributed systems are different from conventional non distributed systems and the differences such as network partitions partial system failures and partial upgrades cannot simply be papered over by attempts at transparency see cap theorem distributed computing also refers to the use of distributed systems to solve computational problems in distributed computing problem is divided into many tasks each of which is solved by one or more computers which communicate with each other by message passing introduction the word distributed in terms such as distributed system distributed programming and distributed algorithm originally referred to computer networks where individual computers were physically distributed within some geographical area the terms are nowadays used in much wider sense even referring to autonomous processes that run on the same physical computer and interact with each other by message passing while there is no single definition of distributed system the following defining properties are commonly used there are several autonomous computational entities each of which has its own local memory the entities communicate with each other by message passing in this article the computational entities are called computers or nodes distributed system may have common goal such as solving large computational problem alternatively each computer may have its own user with individual needs and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users other typical properties of distributed systems include the following the system has to tolerate failures in individual computers the structure of the system network topology network latency number of computers is not known in advance the system may consist of different kinds of computers and network links and the system may change during the execution of distributed program each computer has only limited incomplete view of the system each computer may know only one part of the input distributed system parallel system architecture client server system the client server architecture is way to provide service from central source there is single server that provides service and many clients that communicate with the server to consume its products in this architecture clients and servers have different jobs the server job is to respond to service requests from clients while client job is to use the data provided in response in order to perform some tasks peer to peer system the term peer to peer is used to describe distributed systems in which labour is divided among all the components of the system all the computers send and receive data and they all contribute some processing power and memory to distributed computation as distributed system increases in size its capacity of computational resources increases parallel and distributed computing distributed systems are groups of networked computers which have the same goal for their work the terms concurrent computing parallel computing and distributed computing have lot of overlap and no clear distinction exists between them the same system may be characterized both as parallel and distributed the processors in typical distributed system run concurrently in parallel parallel computing may be seen as particular tightly coupled form of distributed computing and distributed computing may be seen as loosely coupled form of parallel computing nevertheless it is possible to roughly classify concurrent systems as parallel or distributed using the following criteria in parallel computing all processors may have access to shared memory to exchange information between processors in distributed computing each processor has its own private memory distributed memory information is exchanged by passing messages between the processors the figure on the right illustrates the difference between distributed and parallel systems figure is schematic view of typical distributed system as usual the system is represented as network topology in which each node is computer and each line connecting the nodes is communication link figure shows the same distributed system in more detail each computer has its own local memory and information can be exchanged only by passing messages from one node to another by using the available communication links figure shows parallel system in which each processor has direct access to shared memory the situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems see the section theoretical foundations below for more detailed discussion nevertheless as rule of thumb high performance parallel computation in shared memory multiprocessor uses parallel algorithms while the coordination of large scale distributed system uses distributed algorithms history the use of concurrent processes that communicate by message passing has its roots in operating system architectures studied in the the first widespread distributed systems were local area networks such as ethernet which was invented in the arpanet the predecessor of the internet was introduced in the late and arpanet mail was invented in the early mail became the most successful application of arpanet and it is probably the earliest example of large scale distributed application in addition to arpanet and its successor the internet other early worldwide computer networks included usenet and fidonet from both of which were used to support distributed discussion systems the study of distributed computing became its own branch of computer science in the late and early the first conference in the field symposium on principles of distributed computing podc dates back to and its european counterpart international symposium on distributed computing disc was first held in applications reasons for using distributed systems and distributed computing may include the very nature of an application may require the use of communication network that connects several computers for example data produced in one physical location and required in another location there are many cases in which the use of single computer would be possible in principle but the use of distributed system is beneficial for practical reasons for example it may be more cost efficient to obtain the desired level of performance by using cluster of several low end computers in comparison with single high end computer distributed system can provide more reliability than non distributed system as there is no single point of failure moreover distributed system may be easier to expand and manage than monolithic uniprocessor system ghaemi et al define distributed query as query that selects data from databases located at multiple sites in network and offer as an sql example select ename dname from company emp company dept sales goods where deptno deptno examples examples of distributed systems and applications of distributed computing include the following networks telephone networks and cellular networks computer networks such as the internet wireless sensor networks routing algorithms network applications world wide web and peer to peer networks massively multiplayer online games and virtual reality communities distributed databases and distributed database management systems network file systems distributed information processing systems such as banking systems and airline reservation systems real time process control aircraft control systems industrial control systems parallel computation scientific computing including cluster computing and grid computing and various volunteer computing projects see the list of distributed computing projects distributed rendering in computer graphics theoretical foundations models many tasks that we would like to automate by using computer are of question answer type we would like to ask question and the computer should produce an answer in theoretical computer science such tasks are called computational problems formally computational problem consists of instances together with solution for each instance instances are questions that we can ask and solutions are desired answers to these questions theoretical computer science seeks to understand which computational problems can be solved by using computer computability theory and how efficiently computational complexity theory traditionally it is said that problem can be solved by using computer if we can design an algorithm that produces correct solution for any given instance such an algorithm can be implemented as computer program that runs on general purpose computer the program reads problem instance from input performs some computation and produces the solution as output formalisms such as random access machines or universal turing machines can be used as abstract models of sequential general purpose computer executing such an algorithm the field of concurrent and distributed computing studies similar questions in the case of either multiple computers or computer that executes network of interacting processes which computational problems can be solved in such network and how efficiently however it is not at all obvious what is meant by solving problem in the case of concurrent or distributed system for example what is the task of the algorithm designer and what is the concurrent or distributed equivalent of sequential general purpose computer the discussion below focuses on the case of multiple computers although many of the issues are the same for concurrent processes running on single computer three viewpoints are commonly used parallel algorithms in shared memory model all processors have access to shared memory the algorithm designer chooses the program executed by each processor one theoretical model is the parallel random access machines pram that are used however the classical pram model assumes synchronous access to the shared memory shared memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems model that is closer to the behavior of real world multiprocessor machines and takes into account the use of machine instructions such as compare and swap cas is that of asynchronous shared memory there is wide body of work on this model summary of which can be found in the literature parallel algorithms in message passing model the algorithm designer chooses the structure of the network as well as the program executed by each computer models such as boolean circuits and sorting networks are used boolean circuit can be seen as computer network each gate is computer that runs an extremely simple computer program similarly sorting network can be seen as computer network each comparator is computer distributed algorithms in message passing model the algorithm designer only chooses the computer program all computers run the same program the system must work correctly regardless of the structure of the network commonly used model is graph with one finite state machine per node in the case of distributed algorithms computational problems are typically related to graphs often the graph that describes the structure of the computer network is the problem instance this is illustrated in the following example an example consider the computational problem of finding coloring of given graph different fields might take the following approaches centralized algorithms the graph is encoded as string and the string is given as input to computer the computer program finds coloring of the graph encodes the coloring as string and outputs the result parallel algorithms again the graph is encoded as string however multiple computers can access the same string in parallel each computer might focus on one part of the graph and produce coloring for that part the main focus is on high performance computation that exploits the processing power of multiple computers in parallel distributed algorithms the graph is the structure of the computer network there is one computer for each node of and one communication link for each edge of initially each computer only knows about its immediate neighbors in the graph the computers must exchange messages with each other to discover more about the structure of each computer must produce its own color as output the main focus is on coordinating the operation of an arbitrary distributed system while the field of parallel algorithms has different focus than the field of distributed algorithms there is lot of interaction between the two fields for example the cole vishkin algorithm for graph coloring was originally presented as parallel algorithm but the same technique can also be used directly as distributed algorithm moreover parallel algorithm can be implemented either in parallel system using shared memory or in distributed system using message passing the traditional boundary between parallel and distributed algorithms choose suitable network vs run in any given network does not lie in the same place as the boundary between parallel and distributed systems shared memory vs message passing complexity measures in parallel algorithms yet another resource in addition to time and space is the number of computers indeed often there is trade off between the running time and the number of computers the problem can be solved faster if there are more computers running in parallel see speedup if decision problem can be solved in polylogarithmic time by using polynomial number of processors then the problem is said to be in the class nc the class nc can be defined equally well by using the pram formalism or boolean circuits pram machines can simulate boolean circuits efficiently and vice versa in the analysis of distributed algorithms more attention is usually paid on communication operations than computational steps perhaps the simplest model of distributed computing is synchronous system where all nodes operate in lockstep fashion during each communication round all nodes in parallel receive the latest messages from their neighbours perform arbitrary local computation and send new messages to their neighbours in such systems central complexity measure is the number of synchronous communication rounds required to complete the task this complexity measure is closely related to the diameter of the network let be the diameter of the network on the one hand any computable problem can be solved trivially in synchronous distributed system in approximately communication rounds simply gather all information in one location rounds solve the problem and inform each node about the solution rounds on the other hand if the running time of the algorithm is much smaller than communication rounds then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network in other words the nodes must make globally consistent decisions based on information that is available in their local neighbourhood many distributed algorithms are known with the running time much smaller than rounds and understanding which problems can be solved by such algorithms is one of the central research questions of the field other commonly used measures are the total number of bits transmitted in the network cf communication complexity other problems traditional computational problems take the perspective that we ask question computer or distributed system processes the question for while and then produces an answer and stops however there are also problems where we do not want the system to ever stop examples of such problems include the dining philosophers problem and other similar mutual exclusion problems in these problems the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur there are also fundamental challenges that are unique to distributed computing the first example is challenges that are related to fault tolerance examples of related problems include consensus problems byzantine fault tolerance and self stabilisation lot of research is also focused on understanding the asynchronous nature of distributed systems synchronizers can be used to run synchronous algorithms in asynchronous systems logical clocks provide causal happened before ordering of events clock synchronization algorithms provide globally consistent physical time stamps properties of distributed systems so far the focus has been on designing distributed system that solves given problem complementary research problem is studying the properties of given distributed system the halting problem is an analogous example from the field of centralised computation we are given computer program and the task is to decide whether it halts or runs forever the halting problem is undecidable in the general case and naturally understanding the behaviour of computer network is at least as hard as understanding the behaviour of one computer however there are many interesting special cases that are decidable in particular it is possible to reason about the behaviour of network of finite state machines one example is telling whether given network of interacting asynchronous and non deterministic finite state machines can reach deadlock this problem is pspace complete it is decidable but it is not likely that there is an efficient centralised parallel or distributed algorithm that solves the problem in the case of large networks coordinator election coordinator election sometimes called leader election is the process of designating single process as the organizer of some task distributed among several computers nodes before the task is begun all network nodes are either unaware which node will serve as the coordinator or leader of the task or unable to communicate with the current coordinator after coordinator election algorithm has been run however each node throughout the network recognizes particular unique node as the task coordinator the network nodes communicate among themselves in order to decide which of them will get into the coordinator state for that they need some method in order to break the symmetry among them for example if each node has unique and comparable identities then the nodes can compare their identities and decide that the node with the highest identity is the coordinator the definition of this problem is often attributed to lelann who formalized it as method to create new token in token ring network in which the token has been lost coordinator election algorithms are designed to be economical in terms of total bytes transmitted and time the algorithm suggested by gallager humblet and spira for general undirected graphs has had strong impact on the design of distributed algorithms in general and won the dijkstra prize for an influential paper in distributed computing many other algorithms were suggested for different kind of network graphs such as undirected rings unidirectional rings complete graphs grids directed euler graphs and others general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by korach kutten and moran in order to perform coordination distributed systems employ the concept of coordinators the coordinator election problem is to choose process from among group of processes on different processors in distributed system to act as the central coordinator several central coordinator election algorithms exist bully algorithm when using the bully algorithm any process sends message to the current coordinator if there is no response within given time limit the process tries to elect itself as leader chang and roberts algorithm the chang and roberts algorithm or ring algorithm is ring based election algorithm used to find process with the largest unique identification number architectures various hardware and software architectures are used for distributed computing at lower level it is necessary to interconnect multiple cpus with some sort of network regardless of whether that network is printed onto circuit board or made up of loosely coupled devices and cables at higher level it is necessary to interconnect processes running on those cpus with some sort of communication system distributed programming typically falls into one of several basic architectures or categories client server tier architecture tier architecture distributed objects loose coupling or tight coupling client server smart client code contacts the server for data then formats and displays it to the user input at the client is committed back to the server when it represents permanent change tier architecture three tier systems move the client intelligence to middle tier so that stateless clients can be used this simplifies application deployment most web applications are tier tier architecture tier refers typically to web applications which further forward their requests to other enterprise services this type of application is the one most responsible for the success of application servers highly coupled clustered refers typically to cluster of machines that closely work together running shared process in parallel the task is subdivided in parts that are made individually by each one and then put back together to make the final result peer to peer an architecture where there is no special machine or machines that provide service or manage the network resources instead all are uniformly divided among all machines known as peers peers can serve both as clients and servers space based refers to an infrastructure that creates the illusion virtualization of one single address space data are transparently replicated according to application needs decoupling in time space and reference is achieved another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes through various message passing protocols processes may communicate directly with one another typically in master slave relationship alternatively database centric architecture can enable distributed computing to be done without any form of direct inter process communication by utilizing shared database see also appscale boinc code mobility decentralized computing distributed algorithmic mechanism design distributed cache distributed operating system edsger dijkstra prize in distributed computing folding home inferno jungle computing layered queueing network library oriented architecture loa list of distributed computing conferences list of important publications in concurrent parallel and distributed computing parallel distributed processing parallel programming model plan from bell labs notes references books articles web sites further reading books isbn isbn java distributed computing by jim faber isbn articles conference papers rodr\u00edguez villagra and bar\u00e1n bionetics pp external links", "Algorithm": "flow chart of an algorithm euclid algorithm for calculating the greatest common divisor of two numbers and in locations named and the algorithm proceeds by successive subtractions in two loops if the test yields yes or true more accurately the number in location is greater than or equal to the number in location then the algorithm specifies meaning the number replaces the old similarly if then the process terminates when the contents of is yielding the in algorithm derived from scott symbols and drawing style from tausworthe in mathematics and computer science an algorithm is self contained step by step set of operations to be performed algorithms exist that perform calculation data processing and automated reasoning an algorithm is an effective method that can be expressed within finite amount of space and time and in well defined formal language for calculating function starting from an initial state and initial input perhaps empty the instructions describe computation that when executed proceeds through finite number of well defined successive states eventually producing output and terminating at final ending state the transition from one state to the next is not necessarily deterministic some algorithms known as randomized algorithms incorporate random input the concept of algorithm has existed for centuries however partial formalization of what would become the modern algorithm began with attempts to solve the the decision problem posed by david hilbert in subsequent formalizations were framed as attempts to define effective calculability or effective method those formalizations included the g\u00f6del herbrand kleene recursive functions of and alonzo church lambda calculus of emil post formulation of and alan turing turing machines of and giving formal definition of algorithms corresponding to the intuitive notion remains challenging problem word origin algorithm stems from the name of latin translation of book written by al khw\u0101rizm\u012b persian mathematician astronomer and geographer al khwarizmi wrote book titled on the calculation with hindu numerals in about ad and was principally responsible for spreading the indian system of numeration throughout the middle east and europe it was translated into latin as algoritmi de numero indorum in english al khwarizmi on the hindu art of reckoning the term algoritmi in the title of the book led to the term algorithm informal definition an informal definition could be set of rules that precisely defines sequence of operations which would include all computer programs including programs that do not perform numeric calculations generally program is only an algorithm if it stops eventually prototypical example of an algorithm is euclid algorithm to determine the maximum common divisor of two integers an example there are others is described by the flow chart above and as an example in later section offer an informal meaning of the word in the following quotation no human being can write fast enough or long enough or small enough smaller and smaller without limit you be trying to write on molecules on atoms on electrons to list all members of an enumerably infinite set by writing out their names one after another in some notation but humans can do something equally useful in the case of certain enumerably infinite sets they can give explicit instructions for determining the th member of the set for arbitrary finite such instructions are to be given quite explicitly in form in which they could be followed by computing machine or by human who is capable of carrying out only very elementary operations on symbols an enumerably infinite set is one whose elements can be put into one to one correspondence with the integers thus boolos and jeffrey are saying that an algorithm implies instructions for process that creates output integers from an arbitrary input integer or integers that in theory can be arbitrarily large thus an algorithm can be an algebraic equation such as two arbitrary input variables and that produce an output but various authors attempts to define the notion indicate that the word implies much more than this something on the order of for the addition example precise instructions in language understood by the computer for fast efficient good process that specifies the moves of the computer machine or human equipped with the necessary internally contained information and capabilities to find decode and then process arbitrary input integers symbols and symbols and and effectively produce in reasonable time output integer at specified place and in specified format the concept of algorithm is also used to define the notion of decidability that notion is central for explaining how formal systems come into being starting from small set of axioms and rules in logic the time that an algorithm requires to complete cannot be measured as it is not apparently related with our customary physical dimension from such uncertainties that characterize ongoing work stems the unavailability of definition of algorithm that suits both concrete in some sense and abstract usage of the term formalization algorithms are essential to the way computers process data many computer programs contain algorithms that detail the specific instructions computer should perform in specific order to carry out specified task such as calculating employees paychecks or printing students report cards thus an algorithm can be considered to be any sequence of operations that can be simulated by turing complete system authors who assert this thesis include minsky savage and gurevich minsky but we will also maintain with turing that any procedure which could naturally be called effective can in fact be realized by simple machine although this may seem extreme the arguments in its favor are hard to refute gurevich turing informal argument in favor of his thesis justifies stronger thesis every algorithm can be simulated by turing machine according to savage an algorithm is computational process defined by turing machine typically when an algorithm is associated with processing information data is read from an input source written to an output device and or stored for further processing stored data is regarded as part of the internal state of the entity performing the algorithm in practice the state is stored in one or more data structures for some such computational process the algorithm must be rigorously defined specified in the way it applies in all possible circumstances that could arise that is any conditional steps must be systematically dealt with case by case the criteria for each case must be clear and computable because an algorithm is precise list of precise steps the order of computation is always critical to the functioning of the algorithm instructions are usually assumed to be listed explicitly and are described as starting from the top and going down to the bottom an idea that is described more formally by flow of control so far this discussion of the formalization of an algorithm has assumed the premises of imperative programming this is the most common conception and it attempts to describe task in discrete mechanical means unique to this conception of formalized algorithms is the assignment operation setting the value of variable it derives from the intuition of memory as scratchpad there is an example below of such an assignment for some alternate conceptions of what constitutes an algorithm see functional programming and logic programming expressing algorithms algorithms can be expressed in many kinds of notation including natural languages pseudocode flowcharts drakon charts programming languages or control tables processed by interpreters natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for complex or technical algorithms pseudocode flowcharts drakon charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements programming languages are primarily intended for expressing algorithms in form that can be executed by computer but are often used as way to define or document algorithms there is wide variety of representations possible and one can express given turing machine program as sequence of machine tables see more at finite state machine state transition table and control table as flowcharts and drakon charts see more at state diagram or as form of rudimentary machine code or assembly code called sets of quadruples see more at turing machine representations of algorithms can be classed into three accepted levels of turing machine description high level description prose to describe an algorithm ignoring the implementation details at this level we do not need to mention how the machine manages its tape or head implementation description prose used to define the way the turing machine uses its head and the way that it stores data on its tape at this level we do not give details of states or transition function formal description most detailed lowest level gives the turing machine state table for an example of the simple algorithm add described in all three levels see algorithm examples implementation logical nand algorithm implemented electronically in chip most algorithms are intended to be implemented as computer programs however algorithms are also implemented by other means such as in biological neural network for example the human brain implementing arithmetic or an insect looking for food in an electrical circuit or in mechanical device computer algorithms b\u00f6hm jacopini structures the sequence rectangles descending the page the while do and the if then else the three structures are made of the primitive conditional goto if test true then goto step xxx diamond the unconditional goto rectangle various assignment operators rectangle and halt rectangle nesting of these structures inside assignment blocks result in complex diagrams cf tausworthe in computer systems an algorithm is basically an instance of logic written in software by software developers to be effective for the intended target computer to produce output from given input perhaps null an optimal algorithm even running in old hardware would produce faster results than non optimal higher time complexity algorithm for the same purpose running in more efficient hardware that is why the algorithms like computer hardware are considered technology elegant compact programs good fast programs the notion of simplicity and elegance appears informally in knuth and precisely in chaitin knuth we want good algorithms in some loosely defined aesthetic sense one criterion is the length of time taken to perform the algorithm other criteria are adaptability of the algorithm to computers its simplicity and elegance etc chaitin program is elegant by which mean that it the smallest possible program for producing the output that it does chaitin prefaces his definition with ll show you can prove that program is elegant such proof would solve the halting problem ibid algorithm versus function computable by an algorithm for given function multiple algorithms may exist this is true even without expanding the available instruction set available to the programmer rogers observes that it is important to distinguish between the notion of algorithm procedure and the notion of function computable by algorithm mapping yielded by procedure the same function may have several different algorithms unfortunately there may be tradeoff between goodness speed and elegance compactness an elegant program may take more steps to complete computation than one less elegant an example that uses euclid algorithm appears below computers and computors models of computation computer or human computor is restricted type of machine discrete deterministic mechanical device that blindly follows its instructions melzak and lambek primitive models reduced this notion to four elements discrete distinguishable locations ii discrete counters iii an agent and iv list of instructions that are effective relative to the capability of the agent minsky describes more congenial variation of lambek abacus model in his very simple bases for computability minsky machine proceeds sequentially through its five or six depending on how one counts instructions unless either conditional if then goto or an unconditional goto changes program flow out of sequence besides halt minsky machine includes three assignment replacement substitution operations zero the contents of location replaced by successor and decrement rarely must programmer write code with such limited instruction set but minsky shows as do melzak and lambek that his machine is turing complete with only four general types of instructions conditional goto unconditional goto assignment replacement substitution and halt simulation of an algorithm computer computor language knuth advises the reader that the best way to learn an algorithm is to try it immediately take pen and paper and work through an example but what about simulation or execution of the real thing the programmer must translate the algorithm into language that the simulator computer computor can effectively execute stone gives an example of this when computing the roots of quadratic equation the computor must know how to take square root if they don then for the algorithm to be effective it must provide set of rules for extracting square root this means that the programmer must know language that is effective relative to the target computing agent computer computor but what model should be used for the simulation van emde boas observes even if we base complexity theory on abstract instead of concrete machines arbitrariness of the choice of model remains it is at this point that the notion of simulation enters when speed is being measured the instruction set matters for example the subprogram in euclid algorithm to compute the remainder would execute much faster if the programmer had modulus division instruction available rather than just subtraction or worse just minsky decrement structured programming canonical structures per the church turing thesis any algorithm can be computed by model known to be turing complete and per minsky demonstrations turing completeness requires only four instruction types conditional goto unconditional goto assignment halt kemeny and kurtz observe that while undisciplined use of unconditional gotos and conditional if then gotos can result in spaghetti code programmer can write structured programs using these instructions on the other hand it is also possible and not too hard to write badly structured programs in structured language tausworthe augments the three b\u00f6hm jacopini canonical structures sequence if then else and while do with two more do while and case an additional benefit of structured program is that it lends itself to proofs of correctness using mathematical induction canonical flowchart symbols the graphical aide called flowchart offers way to describe and document an algorithm and computer program of one like program flow of minsky machine flowchart always starts at the top of page and proceeds down its primary symbols are only the directed arrow showing program flow the rectangle sequence goto the diamond if then else and the dot or tie the b\u00f6hm jacopini canonical structures are made of these primitive shapes sub structures can nest in rectangles but only if single exit occurs from the superstructure the symbols and their use to build the canonical structures are shown in the diagram examples algorithm example quicksort algorithm sorting an array of randomized values the red bars mark the pivot element at the start of the animation the element farthest to the right hand side is chosen as the pivot one of the simplest algorithms is to find the largest number in list of numbers of random order finding solution requires looking at every number in the list from this follows simple algorithm which can be stated in high level description english prose as high level description if there are no numbers in the set then there is no highest number assume the first number in the set is the largest number in the set for each remaining number in the set if this number is larger than the current largest number consider this number to be the largest number in the set when there are no numbers left in the set to iterate over consider the current largest number to be the largest number of the set quasi formal description written in prose but much closer to the high level language of computer program the following is the more formal coding of the algorithm in pseudocode or pidgin code input list of numbers output the largest number in the list if size return null largest for each item in do if item largest then largest item return largest euclid algorithm the example diagram of euclid algorithm from heath with more detail added euclid does not go beyond third measuring and gives no numerical examples nicomachus gives the example of and subtract the less from the greater is left then again subtract from this the same for this is possible is left subtract this from is left from which again subtract for this is possible is left but cannot be subtracted from heath comments that the last phrase is curious but the meaning of it is obvious enough as also the meaning of the phrase about ending at one and the same number heath euclid algorithm appears as proposition ii in book vii elementary number theory of his elements euclid poses the problem given two numbers not prime to one another to find their greatest common measure he defines number to be multitude composed of units counting number positive integer not including and to measure is to place shorter measuring length successively times along longer length until the remaining portion is less than the shorter length in modern words remainder being the quotient or remainder is the modulus the integer fractional part left over after the division for euclid method to succeed the starting lengths must satisfy two requirements the lengths must not be and ii the subtraction must be proper test must guarantee that the smaller of the two numbers is subtracted from the larger alternately the two can be equal so their subtraction yields euclid original proof adds third the two lengths are not prime to one another euclid stipulated this so that he could construct reductio ad absurdum proof that the two numbers common measure is in fact the greatest while nicomachus algorithm is the same as euclid when the numbers are prime to one another it yields the number for their common measure so to be precise the following is really nicomachus algorithm graphical expression on euclid algorithm using example with and computer language for euclid algorithm only few instruction types are required to execute euclid algorithm some logical tests conditional goto unconditional goto assignment replacement and subtraction location is symbolized by upper case letter etc the varying quantity number in location is written in lower case letter and usually associated with the location name for example location at the start might contain the number an inelegant program for euclid algorithm inelegant is translation of knuth version of the algorithm with subtraction based remainder loop replacing his use of division or modulus instruction derived from knuth depending on the two numbers inelegant may compute the in fewer steps than elegant the following algorithm is framed as knuth step version of euclid and nicomachus but rather than using division to find the remainder it uses successive subtractions of the shorter length from the remaining length until is less than the high level description shown in boldface is adapted from knuth input into two locations and put the numbers and that represent the two lengths input initialize make the remaining length equal to the starting initial input length ensure ensure the smaller of the two numbers is in and the larger in if then the contents of is the larger number so skip over the exchange steps and goto step else swap the contents of and this first step is redundant but is useful for later discussion find remainder until the remaining length in is less than the shorter length in repeatedly subtract the measuring number in from the remaining length in if then done measuring so goto else measure again remainder loop goto is the remainder either the last measure was exact and the remainder in is program can halt or ii the algorithm must continue the last measure left remainder in less than measuring number in if then done so goto step else continue to step interchange and the nut of euclid algorithm use remainder to measure what was previously smaller number serves as temporary location repeat the measuring process goto output done contains the greatest common divisor print done halt end stop an elegant program for euclid algorithm the following version of euclid algorithm requires only core instructions to do what are required to do by inelegant worse inelegant requires more types of instructions the flowchart of elegant can be found at the top of this article in the unstructured basic language the steps are numbered and the instruction let is the assignment instruction symbolized by rem euclid algorithm for greatest common divisor print type two integers greater than input if then goto if then goto let goto let goto print end how elegant works in place of an outer euclid loop elegant shifts back and forth between two co loops an loop that computes and loop that computes this works because when at last the minuend is less than or equal to the subtrahend difference minuend subtrahend the minuend can become the new measuring length and the subtrahend can become the new the length to be measured in other words the sense of the subtraction reverses testing the euclid algorithms does an algorithm do what its author wants it to do few test cases usually suffice to confirm core functionality one source uses and knuth suggested another interesting case is the two relatively prime numbers and but exceptional cases must be identified and tested will inelegant perform properly when ditto for elegant yes to all what happens when one number is zero both numbers are zero inelegant computes forever in all cases elegant computes forever when what happens if negative numbers are entered fractional numbers if the input numbers the domain of the function computed by the algorithm program is to include only positive integers including zero then the failures at zero indicate that the algorithm and the program that instantiates it is partial function rather than total function notable failure due to exceptions is the ariane rocket failure proof of program correctness by use of mathematical induction knuth demonstrates the application of mathematical induction to an extended version of euclid algorithm and he proposes general method applicable to proving the validity of any algorithm tausworthe proposes that measure of the complexity of program be the length of its correctness proof measuring and improving the euclid algorithms elegance compactness versus goodness speed with only core instructions elegant is the clear winner compared to inelegant at instructions however inelegant is faster it arrives at halt in fewer steps algorithm analysis indicates why this is the case elegant does two conditional tests in every subtraction loop whereas inelegant only does one as the algorithm usually requires many loop throughs on average much time is wasted doing test that is needed only after the remainder is computed can the algorithms be improved once the programmer judges program fit and effective that is it computes the function intended by its author then the question becomes can it be improved the compactness of inelegant can be improved by the elimination of steps but chaitin proved that compacting an algorithm cannot be automated by generalized algorithm rather it can only be done heuristically by exhaustive search examples to be found at busy beaver trial and error cleverness insight application of inductive reasoning etc observe that steps and are repeated in steps and comparison with elegant provides hint that these steps together with steps and can be eliminated this reduces the number of core instructions from to which makes it more elegant than elegant at steps the speed of elegant can be improved by moving the test outside of the two subtraction loops this change calls for the addition of instructions goto now elegant computes the example numbers faster whether for any given and this is always the case would require detailed analysis algorithmic analysis it is frequently important to know how much of particular resource such as time or storage is theoretically required for given algorithm methods have been developed for the analysis of algorithms to obtain such quantitative answers estimates for example the sorting algorithm above has time requirement of using the big notation with as the length of the list at all times the algorithm only needs to remember two values the largest number found so far and its current position in the input list therefore it is said to have space requirement of if the space required to store the input numbers is not counted or if it is counted different algorithms may complete the same task with different set of instructions in less or more time space or effort than others for example binary search algorithm usually outperforms brute force sequential search when used for table lookups on sorted lists formal versus empirical the analysis and study of algorithms is discipline of computer science and is often practiced abstractly without the use of specific programming language or implementation in this sense algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation usually pseudocode is used for analysis as it is the simplest and most general representation however ultimately most algorithms are usually implemented on particular hardware software platforms and their algorithmic efficiency is eventually put to the test using real code for the solution of one off problem the efficiency of particular algorithm may not have significant consequences unless is extremely large but for algorithms designed for fast interactive commercial or long life scientific usage it may be critical scaling from small to large frequently exposes inefficient algorithms that are otherwise benign empirical testing is useful because it may uncover unexpected interactions that affect performance benchmarks may be used to compare before after potential improvements to an algorithm after program optimization execution efficiency to illustrate the potential improvements possible even in well established algorithms recent significant innovation relating to fft algorithms used heavily in the field of image processing can decrease processing time up to times for applications like medical imaging in general speed improvements depend on special properties of the problem which are very common in practical applications speedups of this magnitude enable computing devices that make extensive use of image processing like digital cameras and medical equipment to consume less power classification there are various ways to classify algorithms each with its own merits by implementation one way to classify algorithms is by implementation means recursion or iteration recursive algorithm is one that invokes makes reference to itself repeatedly until certain condition also known as termination condition matches which is method common to functional programming iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems some problems are naturally suited for one implementation or the other for example towers of hanoi is well understood using recursive implementation every recursive version has an equivalent but possibly more or less complex iterative version and vice versa logical an algorithm may be viewed as controlled logical deduction this notion may be expressed as algorithm logic control the logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms this is the basis for the logic programming paradigm in pure logic programming languages the control component is fixed and algorithms are specified by supplying only the logic component the appeal of this approach is the elegant semantics change in the axioms has well defined change in the algorithm serial parallel or distributed algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at time those computers are sometimes called serial computers an algorithm designed for such an environment is called serial algorithm as opposed to parallel algorithms or distributed algorithms parallel algorithms take advantage of computer architectures where several processors can work on problem at the same time whereas distributed algorithms utilize multiple machines connected with network parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together the resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors some sorting algorithms can be parallelized efficiently but their communication overhead is expensive iterative algorithms are generally parallelizable some problems have no parallel algorithms and are called inherently serial problems deterministic or non deterministic deterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics exact or approximate while many algorithms reach an exact solution approximation algorithms seek an approximation that is close to the true solution approximation may use either deterministic or random strategy such algorithms have practical value for many hard problems quantum algorithm they run on realistic model of quantum computation the term is usually used for those algorithms which seem inherently quantum or use some essential feature of quantum computation such as quantum superposition or quantum entanglement by design paradigm another way of classifying algorithms is by their design methodology or paradigm there is certain number of paradigms each different from the other furthermore each of these categories include many different types of algorithms some common paradigms are brute force or exhaustive search this is the naive method of trying every possible solution to see which is best divide and conquer divide and conquer algorithm repeatedly reduces an instance of problem to one or more smaller instances of the same problem usually recursively until the instances are small enough to solve easily one such example of divide and conquer is merge sorting sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments simpler variant of divide and conquer is called decrease and conquer algorithm that solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms an example of decrease and conquer algorithm is the binary search algorithm search and enumeration many problems such as playing chess can be modeled as problems on graphs graph exploration algorithm specifies rules for moving around graph and is useful for such problems this category also includes search algorithms branch and bound enumeration and backtracking randomized algorithm such algorithms make some choices randomly or pseudo randomly they can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical see heuristic method below for some of these problems it is known that the fastest approximations must involve some randomness whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the versus np problem there are two large classes of such algorithms monte carlo algorithms return correct answer with high probability rp is the subclass of these that run in polynomial time las vegas algorithms always return the correct answer but their running time is only bound zpp reduction of complexity this technique involves solving difficult problem by transforming it into better known problem for which we have hopefully asymptotically optimal algorithms the goal is to find reducing algorithm whose complexity is not dominated by the resulting reduced algorithm for example one selection algorithm for finding the median in an unsorted list involves first sorting the list the expensive portion and then pulling out the middle element in the sorted list the cheap portion this technique is also known as transform and conquer optimization problems for optimization problems there is more specific classification of algorithms an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following linear programming when searching for optimal solutions to linear function bound to linear equality and inequality constrains the constrains of the problem can be used directly in producing the optimal solutions there are algorithms that can solve any problem in this category such as the popular simplex algorithm problems that can be solved with linear programming include the maximum flow problem for directed graphs if problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming linear programming algorithm can solve such problem if it can be proved that all restrictions for integer values are superficial the solutions satisfy these restrictions anyway in the general case specialized algorithm or an algorithm that finds approximate solutions is used depending on the difficulty of the problem dynamic programming when problem shows optimal substructures meaning the optimal solution to problem can be constructed from optimal solutions to subproblems and overlapping subproblems meaning the same subproblems are used to solve many different problem instances quicker approach called dynamic programming avoids recomputing solutions that have already been computed for example floyd warshall algorithm the shortest path to goal from vertex in weighted graph can be found by using the shortest path to the goal from all adjacent vertices dynamic programming and memoization go together the main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer whereas subproblems overlap in dynamic programming the difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls when subproblems are independent and there is no repetition memoization does not help hence dynamic programming is not solution for all complex problems by using memoization or maintaining table of subproblems already solved dynamic programming reduces the exponential nature of many problems to polynomial complexity the greedy method greedy algorithm is similar to dynamic programming algorithm in that it works by examining substructures in this case not of the problem but of given solution such algorithms start with some solution which may be given or have been constructed in some way and improve it by making small modifications for some problems they can find the optimal solution while for others they stop at local optima that is at solutions that cannot be improved by the algorithm but are not optimum the most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method huffman tree kruskal prim sollin are greedy algorithms that can solve this optimization problem the heuristic method in optimization problems heuristic algorithms can be used to find solution close to the optimal solution in cases where finding the optimal solution is impractical these algorithms work by getting closer and closer to the optimal solution as they progress in principle if run for an infinite amount of time they will find the optimal solution their merit is that they can find solution very close to the optimal solution in relatively short time such algorithms include local search tabu search simulated annealing and genetic algorithms some of them like simulated annealing are non deterministic algorithms while others like tabu search are deterministic when bound on the error of the non optimal solution is known the algorithm is further categorized as an approximation algorithm by field of study every field of science has its own problems and needs efficient algorithms related problems in one field are often studied together some example classes are search algorithms sorting algorithms merge algorithms numerical algorithms graph algorithms string algorithms computational geometric algorithms combinatorial algorithms medical algorithms machine learning cryptography data compression algorithms and parsing techniques fields tend to overlap with each other and algorithm advances in one field may improve those of other sometimes completely unrelated fields for example dynamic programming was invented for optimization of resource consumption in industry but is now used in solving broad range of problems in many fields by complexity algorithms can be classified by the amount of time they need to complete compared to their input size there is wide variety some algorithms complete in linear time relative to input size some do so in an exponential amount of time or even worse and some never halt additionally some problems may have multiple algorithms of differing complexity while other problems might have no algorithms or no known efficient algorithms there are also mappings from some problems to other problems owing to this it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them burgin uses generalized definition of algorithms that relaxes the common requirement that the output of the algorithm that computes function must be determined after finite number of steps he defines super recursive class of algorithms as class of algorithms in which it is possible to compute functions not computable by any turing machine burgin this is closely related to the study of methods of continuous algorithms the adjective continuous when applied to the word algorithm can mean an algorithm operating on data that represents continuous quantities even though this data is represented by discrete approximations such algorithms are studied in numerical analysis or an algorithm in the form of differential equation that operates continuously on the data running on an analog computer legal issues see also software patents for general overview of the patentability of software including computer implemented algorithms algorithms by themselves are not usually patentable in the united states claim consisting solely of simple manipulations of abstract concepts numbers or signals does not constitute processes uspto and hence algorithms are not patentable as in gottschalk benson however practical applications of algorithms are sometimes patentable for example in diamond diehr the application of simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable the patenting of software is highly controversial and there are highly criticized patents involving algorithms especially data compression algorithms such as unisys lzw patent additionally some cryptographic algorithms have export restrictions see export of cryptography etymology the word algorithm or algorism in some other writing versions comes from the name al khw\u0101rizm\u012b pronounced in classical arabic as al khwarithmi al khw\u0101rizm\u012b was persian mathematician astronomer geographer and scholar in the house of wisdom in baghdad whose name means the native of khwarezm city that was part of the greater iran during his era and now is in modern day uzbekistan about he wrote treatise in the arabic language which was translated into latin in the th century under the title algoritmi de numero indorum this title means algoritmi on the numbers of the indians where algoritmi was the translator latinization of al khwarizmi name al khwarizmi was the most widely read mathematician in europe in the late middle ages primarily through his other book the algebra in late medieval latin algorismus the corruption of his name simply meant the decimal number system that is still the meaning of modern english algorism in th century french the word form but not its meaning changed to algorithme english adopted the french very soon afterwards but it wasn until the late th century that algorithm took on the meaning that it has in modern english alternative etymology claims origin from the terms algebra in its late medieval sense of arabic arithmetics and arithmos the greek term for number thus literally meaning arabic numbers or arabic calculation algorithms of al kharizmi works are not meant in their modern sense but as type of repetitive calculus here is to mention that his fundamental work known as algebra was originally titled the compendious book on calculation by completion and balancing describing types of repetitive calculation and quadratic equations in that sense algorithms were known in europe long before al kharizmi the oldest algorithm known today is the euclidean algorithm see also extended euclidean algorithm before the coining of the term algorithm the greeks were calling them anthyphairesis literally meaning anti subtraction or reciprocal subtraction further reading here and here algorithms were known to the greeks centuries before euclid instead of the word algebra the greeks were using the term arithmetica \u1f00\u03c1\u03b9\u03b8\u03bc\u03b7\u03c4\u03b9\u03ba\u03ae in diophantus works the so called father of algebra see also diophantine equation and eudoxos history development of the notion of algorithm ancient greece algorithms were used in ancient greece two examples are the sieve of eratosthenes which was described in introduction to arithmetic by nicomachus and the euclidean algorithm which was first described in euclid elements bc origin the word algorithm comes from the name of the th century persian mathematician abu abdullah muhammad ibn musa al khwarizmi whose work built upon that of the th century indian mathematician brahmagupta the word algorism originally referred only to the rules of performing arithmetic using hindu arabic numerals but evolved via european latin translation of al khwarizmi name into algorithm by the th century the use of the word evolved to include all definite procedures for solving problems or performing tasks discrete and distinguishable symbols tally marks to keep track of their flocks their sacks of grain and their money the ancients used tallying accumulating stones or marks scratched on sticks or making discrete symbols in clay through the babylonian and egyptian use of marks and symbols eventually roman numerals and the abacus evolved dilson tally marks appear prominently in unary numeral system arithmetic used in turing machine and post turing machine computations manipulation of symbols as place holders for numbers algebra the work of the ancient greek geometers euclidean algorithm the indian mathematician brahmagupta and the persian mathematician al khwarizmi from whose name the terms algorism and algorithm are derived and western european mathematicians culminated in leibniz notion of the calculus ratiocinator ca mechanical contrivances with discrete states the clock bolter credits the invention of the weight driven clock as the key invention of europe in the middle ages in particular the verge escapement that provides us with the tick and tock of mechanical clock the accurate automatic machine led immediately to mechanical automata beginning in the th century and finally to computational machines the difference engine and analytical engines of charles babbage and countess ada lovelace mid th century lovelace is credited with the first creation of an algorithm intended for processing on computer babbage analytical engine the first device considered real turing complete computer instead of just calculator and is sometimes called history first programmer as result though full implementation of babbage second device would not be realized until decades after her lifetime logical machines stanley jevons logical abacus and logical machine the technical problem was to reduce boolean equations when presented in form similar to what are now known as karnaugh maps jevons describes first simple abacus of slips of wood furnished with pins contrived so that any part or class of the logical combinations can be picked out mechanically more recently however have reduced the system to completely mechanical form and have thus embodied the whole of the indirect process of inference in what may be called logical machine his machine came equipped with certain moveable wooden rods and at the foot are keys like those of piano etc with this machine he could analyze syllogism or any other simple logical argument this machine he displayed in before the fellows of the royal society another logician john venn however in his symbolic logic turned jaundiced eye to this effort have no high estimate myself of the interest or importance of what are sometimes called logical machines it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines see more at algorithm but not to be outdone he too presented plan somewhat analogous apprehend to prof jevon abacus and again corresponding to prof jevons logical machine the following contrivance may be described prefer to call it merely logical diagram machine but suppose that it could do very completely all that can be rationally expected of any logical machine jacquard loom hollerith punch cards telegraphy and telephony the relay bell and newell indicate that the jacquard loom precursor to hollerith cards punch cards and telephone switching technologies were the roots of tree leading to the development of the first computers by the mid th century the telegraph the precursor of the telephone was in use throughout the world its discrete and distinguishable encoding of letters as dots and dashes common sound by the late th century the ticker tape ca was in use as was the use of hollerith cards in the census then came the teleprinter ca with its punched paper use of baudot code on tape telephone switching networks of relays invented was behind the work of george stibitz the inventor of the digital adding device as he worked in bell laboratories he observed the burdensome use of mechanical calculators with gears he went home one evening in intending to test his idea when the tinkering was over stibitz had constructed binary adding device davis observes the particular importance of the relay with its two binary states open and closed it was only with the development beginning in the of calculators using electrical relays that machines were built having the scope babbage had envisioned mathematics during the th century up to the mid th century symbols and rules in rapid succession the mathematics of george boole gottlob frege and giuseppe peano reduced arithmetic to sequence of symbols manipulated by rules peano the principles of arithmetic presented by new method was the first attempt at an axiomatization of mathematics in symbolic language but heijenoort gives frege this kudos frege is perhaps the most important single work ever written in logic in which we see formula language that is lingua characterica language written with special symbols for pure thought that is free from rhetorical embellishments constructed from specific symbols that are manipulated according to definite rules the work of frege was further simplified and amplified by alfred north whitehead and bertrand russell in their principia mathematica the paradoxes at the same time number of disturbing paradoxes appeared in the literature in particular the burali forti paradox the russell paradox and the richard paradox the resultant considerations led to kurt g\u00f6del paper he specifically cites the paradox of the liar that completely reduces rules of recursion to numbers effective calculability in an effort to solve the defined precisely by hilbert in mathematicians first set about to define what was meant by an effective method or effective calculation or effective calculability calculation that would succeed in rapid succession the following appeared alonzo church stephen kleene and rosser calculus finely honed definition of general recursion from the work of g\u00f6del acting on suggestions of jacques herbrand cf g\u00f6del princeton lectures of and subsequent simplifications by kleene church proof that the was unsolvable emil post definition of effective calculability as worker mindlessly following list of instructions to move left or right through sequence of rooms and while there either mark or erase paper or observe the paper and make yes no decision about the next instruction alan turing proof of that the was unsolvable by use of his automatic machine in effect almost identical to post formulation barkley rosser definition of effective method in terms of machine kleene proposal of precursor to church thesis that he called thesis and few years later kleene renaming his thesis church thesis and proposing turing thesis emil post and alan turing here is remarkable coincidence of two men not knowing each other but describing process of men as computers working on computations and they yield virtually identical definitions emil post described the actions of computer human being as follows two concepts are involved that of symbol space in which the work leading from problem to answer is to be carried out and fixed unalterable set of directions his symbol space would be two way infinite sequence of spaces or boxes the problem solver or worker is to move and work in this symbol space being capable of being in and operating in but one box at time box is to admit of but two possible conditions being empty or unmarked and having single mark in it say vertical stroke one box is to be singled out and called the starting point specific problem is to be given in symbolic form by finite number of boxes input being marked with stroke likewise the answer output is to be given in symbolic form by such configuration of marked boxes set of directions applicable to general problem sets up deterministic process when applied to each specific problem this process terminates only when it comes to the direction of type stop see more at post turing machine alan turing statue at bletchley park alan turing work preceded that of stibitz it is unknown whether stibitz knew of the work of turing turing biographer believed that turing use of typewriter like model derived from youthful interest alan had dreamt of inventing typewriters as boy mrs turing had typewriter and he could well have begun by asking himself what was meant by calling typewriter mechanical given the prevalence of morse code and telegraphy ticker tape machines and teletypewriters we might conjecture that all were influences turing his model of computation is now called turing machine begins as did post with an analysis of human computer that he whittles down to simple set of basic motions and states of mind but he continues step further and creates machine as model of computation of numbers computing is normally done by writing certain symbols on paper we may suppose this paper is divided into squares like child arithmetic book assume then that the computation is carried out on one dimensional paper on tape divided into squares shall also suppose that the number of symbols which may be printed is finite the behaviour of the computer at any moment is determined by the symbols which he is observing and his state of mind at that moment we may suppose that there is bound to the number of symbols or squares which the computer can observe at one moment if he wishes to observe more he must use successive observations we will also suppose that the number of states of mind which need be taken into account is finite let us imagine that the operations performed by the computer to be split up into simple operations which are so elementary that it is not easy to imagine them further divided turing reduction yields the following the simple operations must therefore include changes of the symbol on one of the observed squares changes of one of the squares observed to another square within squares of one of the previously observed squares it may be that some of these change necessarily invoke change of state of mind the most general single operation must therefore be taken to be one of the following possible change of symbol together with possible change of state of mind possible change of observed squares together with possible change of state of mind we may now construct machine to do the work of this computer few years later turing expanded his analysis thesis definition with this forceful expression of it function is said to be effectively calculable if its values can be found by some purely mechanical process though it is fairly easy to get an intuitive grasp of this idea it is nevertheless desirable to have some more definite mathematical expressible definition he discusses the history of the definition pretty much as presented above with respect to g\u00f6del herbrand kleene church turing and post we may take this statement literally understanding by purely mechanical process one which could be carried out by machine it is possible to give mathematical description in certain normal form of the structures of these machines the development of these ideas leads to the author definition of computable function and to an identification of computability with effective calculability we shall use the expression computable function to mean function calculable by machine and we let effectively calculable refer to the intuitive idea without particular identification with any one of these definitions rosser and kleene barkley rosser defined an effective mathematical method in the following manner italicization added effective method is used here in the rather special sense of method each step of which is precisely determined and which is certain to produce the answer in finite number of steps with this special meaning three different precise definitions have been given to date his footnote see discussion immediately below the simplest of these to state due to post and turing says essentially that an effective method of solving certain sets of problems exists if one can build machine which will then solve any problem of the set with no human intervention beyond inserting the question and later reading the answer all three definitions are equivalent so it doesn matter which one is used moreover the fact that all three are equivalent is very strong argument for the correctness of any one rosser rosser footnote references the work of church and kleene and their definition of definability in particular church use of it in his an unsolvable problem of elementary number theory herbrand and g\u00f6del and their use of recursion in particular g\u00f6del use in his famous paper on formally undecidable propositions of principia mathematica and related systems and post and turing in their mechanism models of computation stephen kleene defined as his now famous thesis known as the church turing thesis but he did this in the following context boldface in original algorithmic theories in setting up complete algorithmic theory what we do is to describe procedure performable for each set of values of the independent variables which procedure necessarily terminates and in such manner that from the outcome we can read definite answer yes or no to the question is the predicate value true kleene history after number of efforts have been directed toward further refinement of the definition of algorithm and activity is on going because of issues surrounding in particular foundations of mathematics especially the church turing thesis and philosophy of mind especially arguments about artificial intelligence for more see algorithm see also abstract machine algorithm engineering algorithmic composition algorithmic synthesis algorithmic trading garbage in garbage out introduction to algorithms list of algorithm general topics list of important publications in theoretical computer science algorithms numerical mathematics consortium theory of computation computability theory computational complexity theory notes references bell gordon and newell allen computer structures readings and examples mcgraw hill book company new york isbn includes an excellent bibliography of references cf chapter turing machines where they discuss certain enumerable sets not effectively mechanically enumerable campagnolo moore and costa an analog of the subrecursive functions in proc of the th conference on real numbers and computers odense university pp reprinted in the undecidable ff the first expression of church thesis see in particular page the undecidable where he defines the notion of effective calculability in terms of an algorithm and he uses the word terminates etc reprinted in the undecidable ff church shows that the is unsolvable in about pages of text and pages of footnotes davis gives commentary before each article papers of g\u00f6del alonzo church turing rosser kleene and emil post are included those cited in the article are listed here by author name davis offers concise biographies of leibniz boole frege cantor hilbert g\u00f6del and turing with von neumann as the show stealing villain very brief bios of joseph marie jacquard babbage ada lovelace claude shannon howard aiken etc yuri gurevich sequential abstract state machines capture sequential algorithms acm transactions on computational logic vol no july pages includes bibliography of sources presented to the american mathematical society september reprinted in the undecidable ff kleene definition of general recursion known now as mu recursion was used by church in his paper an unsolvable problem of elementary number theory that proved the decision problem to be undecidable negative result reprinted in the undecidable ff kleene refined his definition of general recursion and proceeded in his chapter algorithmic theories to posit thesis he would later repeat this thesis in kleene and name it church thesis kleene the church thesis excellent accessible readable reference source for mathematical foundations kosovsky elements of mathematical logic and its application to the theory of subrecursive algorithms lsu publ leningrad markov theory of algorithms translated by jacques schorr kon and pst staff imprint moscow academy of sciences of the ussr jerusalem israel program for scientific translations available from the office of technical services dept of commerce washington description cm added in russian translation of works of the mathematical institute academy of sciences of the ussr original title teoriya algerifmov qa dartmouth college library dept of commerce office of technical services number ots minsky expands his idea of an algorithm an effective procedure in chapter computability effective procedures and algorithms infinite machines reprinted in the undecidable ff post defines simple algorithmic like process of man writing marks or erasing marks and going from box to box and eventually halting as he follows list of simple instructions this is cited by kleene as one source of his thesis the so called church turing thesis reprinted in the undecidable ff herein is rosser famous definition of effective method method each step of which is precisely predetermined and which is certain to produce the answer in finite number of steps machine which will then solve any problem of the set with no human intervention beyond inserting the question and later reading the answer the undecidable cf in particular the first chapter titled algorithms turing machines and programs his succinct informal definition any sequence of instructions that can be obeyed by robot is called an algorithm corrections ibid vol pp reprinted in the undecidable ff turing famous paper completed as master dissertation while at king college cambridge uk reprinted in the undecidable ff turing paper that defined the oracle was his phd thesis while at princeton usa united states patent and trademark office mathematical algorithms patentability manual of patent examining procedure mpep latest revision august secondary references isbn pbk isbn pbk rd edition isbn pbk isbn cf chapter the spirit of truth for history leading to and discussion of his proof further reading knuth donald selected papers on analysis of algorithms stanford california center for the study of language and information knuth donald selected papers on design of algorithms stanford california center for the study of language and information external links dictionary of algorithms and data structures national institute of standards and technology algorithms and data structures by dr nikolai bezroukov algorithm repositories the stony brook algorithm repository state university of new york at stony brook netlib repository university of tennessee and oak ridge national laboratory collected algorithms of the acm association for computing machinery the stanford graphbase stanford university combinatorica university of iowa and state university of new york at stony brook library of efficient datastructures and algorithms leda previously from max planck institut f\u00fcr informatik archive of interesting code semantic wiki to collect categorize and relate all algorithms and data structures lecture notes algorithms course materials jeff erickson university of illinois community algorithms on google", "Computational biology": "computational biology involves the development and application of data analytical and theoretical methods mathematical modeling and computational simulation techniques to the study of biological behavioral and social systems the field is broadly defined and includes foundations in computer science applied mathematics animation statistics biochemistry chemistry biophysics molecular biology genetics genomics ecology evolution anatomy neuroscience and visualization computational biology is different from biological computation which is subfield of computer science and computer engineering using bioengineering and biology to build computers but is similar to bioinformatics which is an science using computers to store and process biological data introduction computational biology sometimes referred to as bioinformatics is the science of using biological data to develop algorithms and relations among various biological systems prior to the advent of computational biology biologists were unable to have access to large amounts of data researchers were able to develop analytical methods for interpreting biological information but were unable to share them quickly among colleagues bioinformatics began to develop in the early it was considered the science of analyzing informatics processes of various biological systems at this time research in artificial intelligence was using network models of the human brain in order to generate new algorithms this use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets by information was being shared amongst researchers through the use of punch cards the amount of data being shared began to grow exponentially by the end of the this required the development of new computational methods in order to quickly analyze and interpret relevant information since the late computational biology has become an important part of developing emerging technologies for the field of biology the terms computational biology and evolutionary computation have similar name but are not to be confused unlike computational biology evolutionary computation is not concerned with modeling and analyzing biological data it instead creates algorithms based on the ideas of evolution across species sometimes referred to as genetic algorithms the research of this field can be applied to computational biology while evolutionary computation is not inherently part of computational biology computational evolutionary biology is subfield of it computational biology has been used to help sequence the human genome create accurate models of the human brain and assist in modeling biological systems subfields computational biomodeling computational biomodeling is field concerned with building computer models of biological systems computational biomodeling aims to develop and use visual simulations in order to assess the complexity of biological systems this is accomplished through the use of specialized algorithms and visualization software these models allow for prediction of how systems will react under different environments this is useful for determining if system is robust robust biological system is one that maintain their state and functions against external and internal perturbations which is essential for biological system to survive computational biomodeling generates large archive of such data allowing for analysis from multiple users while current techniques focus on small biological systems researchers are working on approaches that will allow for larger networks to be analyzed and modeled majority of researchers believe that this will be essential in developing modern medical approaches to creating new drugs and gene therapy computational genomics computational genetics partially sequenced genome computational genomics is field within genomics which studies the genomes of cells and organisms it is often referred to as computational and statistical genetics the human genome project is one example of computational genomics this project looks to sequence the entire human genome into set of data once fully implemented this could allow for doctors to analyze the genome of an individual patient this opens the possibility of personalized medicine prescribing treatments based on an individual pre existing genetic patterns this project has created many similar programs researchers are looking to sequence the genomes of animals plants bacteria and all other types of life one of the main tools used in comparing the genomes is homology homology is observing the same organ across species and seeing what different functions they have research suggests that between to of sequences genes can be identified this way in order to detect potential cures from genomes comparisons between genome sequences of related species and mrna sequences are drawn this method is not completely accurate however it may be necessary to include the genome of primate in order to improve current methods of unique gene therapy this field is still in development an untouched project in the development in computational genomics is analyzing intergenic regions studies show that roughly of the human genome consists of these regions there are no current methods for determining possible implications of these sequences computational genomics will look to expand research in this area and develop new numerical and computational approaches to sequencing these regions computational neuroscience computational neuroscience is the study of brain function in terms of the information processing properties of the structures that make up the nervous system it is subset of the field of neuroscience and looks to analyze brain data to create practical applications it looks to model the brain in order to examine specific types aspects of the neurological system various types of models of the brain include realistic brain models these models look to represent every aspect of the brain including as much detail at the cellular level as possible realistic models provide the most information about the brain but also have the largest margin for error more variables in brain model create the possibility for more error to occur these models do not account for parts of the cellular structure that scientists do not know about realistic brain models are the most computationally heavy and the most expensive to implement simplifying brain models these models look to limit the scope of model in order to assess specific physical property of the neurological system this allows for the intensive computational problems to be solved and reduces the amount of potential error from realistic brain model it is the work of computational neuroscientists to improve the algorithms and data structures currently used to increase the speed of such calculations computational pharmacology computational pharmacology from computational biology perspective is the study of the effects of genomic data to find links between specific genotypes and diseases and then screening drug data the pharmaceutical industry requires shift in methods to analyze drug data pharmacists were able to use microsoft excel to compare chemical and genomic data related to the effectiveness of drugs however the industry has reached what is referred to as the excel barricade this arises from the limited number of cells accessible on spreadsheet this development led to the need for computational pharmacology scientists and researcher develop computational methods to analyze these massive data sets this allows for an efficient comparison between the notable data points and provide for more accurate drugs to be developed analysts project that if major medications fail due to patents that computational biology will be necessary to replace current drugs on the market doctoral students in computational biology are being encouraged to pursue careers in industry rather than take post doctoral positions this is direct result of major pharmaceutical companies needing more qualified analysts of the large data sets required for producing new drugs computational evolutionary biology computational biology has assisted the field of evolutionary biology in many capacities this includes using dna data to evaluate the evolutionary change of species over time taking the results of computational genomics in order to evaluate the evolution of genetic disorders within species build models of evolutionary systems in order predict what types of changes will occur in the future one method of representing this subfield of computational biology is through the use of trees tree is data structure that splits nodes based on predefined rule this tree developed by hezinger king and warnow implements traversal of evolutionary information in less than polynomial time this is particularly quick method as opposed to some modern methods that take longer than time these trees have multiple applications to questions in computational evolutionary biology cancer computational biology cancer computational biology is field that aims to determine the future mutations in cancer through an algorithmic approach to analyzing data research in this field has led to the use of high throughput measurement high throughput measurement allows for the gathering of millions of data points using robotics and other sensing devices this data is collected from dna rna and other biological structures areas of focus include determining the characteristics of tumors analyzing molecules that are deterministic in causing cancer and understanding how the human genome relates to the causation of tumors and cancer software and tools computational biologists use wide range of software these range from command line programs to graphical and web based programs open source software open source software provides platform to develop computational biological methods specifically open source means that anybody can access software developed in research plos cites four main reasons for the use of open source software including reproducibility this allows for researchers to use the exact methods used to calculate the relations between biological data faster development developers and researchers do not have to reinvent existing code for minor tasks instead they can use pre existing programs to save time on the development and implementation of larger projects increased quality having input from multiple researchers studying the same topic provides layer of assurance that errors will not be in the code long term availability open source programs are not tied to any businesses or patents this allows for them to be posted to multiple web pages and ensure that they are available in the future conferences there are several large conferences that are concerned with computational biology some notable examples are intelligent systems for molecular biology ismb european conference on computational biology eccb and research in computational molecular biology recomb mit hosts list of upcoming computational biology conferences including ismb and recomb journals there are numerous journals dedicated to computational biology some notable examples include journal of computational biology and plos computational biology the plos computational biology journal is peer reviewed journal that has many notable research projects in the field of computational biology they provide reviews on software tutorials for open source software and display information on upcoming computational biology conferences plos computational biology is an open access journal the publication may be openly used provided the author is cited recently new open access journal computational molecular biology was launched related fields computational biology bioinformatics and mathematical biology are all approaches to the life sciences that draw from quantitative disciplines such as mathematics and information science the nih describes computational mathematical biology as the use of computational mathematical approaches to address theoretical and experimental questions in biology and by contrast bioinformatics as the application of information science to understand complex life sciences data specifically the nih defines while each field is distinct there may be significant overlap at their interface see also references external links bioinformatics org", "Modeling and Analysis of Real Time and Embedded systems": "modeling and analysis of real time and embedded systems also known as marte is the omg standard for modeling real time and embedded applications with uml description the uml modeling language has been extended by the omg consortium to support model driven development of real time and embedded application this extension has been defined via uml profile called marte modeling and analysis of real time and embedded systems it consists mainly of four parts core framework defining the basic concepts required to support real time and embedded domain first specialization refinement of this core package to support pure modeling of applications hardware and software platform modeling second specialization refinement of this core package to support quantitative analysis of uml models specially schedulability and performance analysis last part gathering all the marte annexes such as the one defining textual language for value specification within uml models and the one conflating the standard marte model libraries dedicated to rt system modeling the marte specification is publicly available on the omg web site currently two open source tools are available for system modeling using the marte profile modelio provides an open source modeling environment for designing high level uml models using the marte profile and also provides guidelines on the utilization of marte profile while an open source implementation based on eclipse of the marte profile is available in papyrus uml this latter is running within the eclipse uml plug in and within the open source tool for uml papyrus core the core part of marte is made of five chapters coreelements non functional properties this chapter specifies some notations to define various kinds of values related to physical quantities time mass energy time this chapter defines rich model of time that supports both the definition of physical and logical time properties it comes with companion language called ccsl defined as an annex generic resource modeling this chapter offers extensions required to model general platform for executing real time embedded applications allocation modeling finally this chapter defines notion of allocation to allocate application elements onto the execution platforms specific attention has been given to maintain compatibility with sysml allocation mechanism notes", "Bioinformatics": "map of the human chromosome from the national center for biotechnology information website bioinformatics is an field that develops methods and software tools for understanding biological data as an field of science bioinformatics combines computer science statistics mathematics and engineering to analyze and interpret biological data bioinformatics is both an umbrella term for the body of biological studies that use computer programming as part of their methodology as well as reference to specific analysis pipelines that are repeatedly used particularly in the fields of genetics and genomics common uses of bioinformatics include the identification of candidate genes and nucleotides snps often such identification is made with the aim of better understanding the genetic basis of disease unique adaptations desirable properties esp in agricultural species or differences between populations in less formal way bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences introduction bioinformatics has become an important part of many areas of biology in experimental molecular biology bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data in the field of genetics and genomics it aids in sequencing and annotating genomes and their observed mutations it plays role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data it also plays role in the analysis of gene and protein expression and regulation bioinformatics tools aid in the comparison of genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology at more integrative level it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology in structural biology it aids in the simulation and modeling of dna rna and protein structures as well as molecular interactions history historically the term bioinformatics did not mean what it means today paulien hogeweg and ben hesper coined it in to refer to the study of information processes in biotic systems this definition placed bioinformatics as field parallel to biophysics the study of physical processes in biological systems or biochemistry the study of chemical processes in biological systems sequences sequences of genetic material are frequently used in bioinformatics and are easier to manage using computers than manually computers became essential in molecular biology when protein sequences became available after frederick sanger determined the sequence of insulin in the early comparing multiple sequences manually turned out to be impractical pioneer in the field was margaret oakley dayhoff who has been hailed by david lipman director of the national center for biotechnology information as the mother and father of bioinformatics dayhoff compiled one of the first protein sequence databases initially published as books and pioneered methods of sequence alignment and molecular evolution another early contributor to bioinformatics was elvin kabat who pioneered biological sequence analysis in with his comprehensive volumes of antibody sequences released with tai te wu between and genomes as whole genome sequences became available again with the pioneering work of frederick sanger it became evident that computer assisted analysis would be insightful the first analysis of this type which had important input from cryptologists at the national security agency was applied to the nucleotide sequences of the bacteriophages ms and phix as proof of principle this work showed that standard methods of cryptology could reveal intrinsic features of the genetic code such as the codon length and the reading frame this work seems to have been ahead of its time it was rejected for publication by numerous standard journals and finally found home in the journal of theoretical biology the term bioinformatics was re discovered and used to refer to the creation of databases such as genbank in with public availability of data tools for their analysis were quickly developed and described in journals such as nucleic acids research which published specialized issues on bioinformatics tools as early as goals to study how normal cellular activities are altered in different disease states the biological data must be combined to form comprehensive picture of these activities therefore the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data this includes nucleotide and amino acid sequences protein domains and protein structures the actual process of analyzing and interpreting data is referred to as computational biology important sub disciplines within bioinformatics and computational biology include development and implementation of computer programs that enable efficient access to use and management of various types of information development of new algorithms mathematical formulas and statistical measures that assess relationships among members of large data sets for example there are methods to locate gene within sequence to predict protein structure and or function and to cluster protein sequences into families of related sequences the primary goal of bioinformatics is to increase the understanding of biological processes what sets it apart from other approaches however is its focus on developing and applying computationally intensive techniques to achieve this goal examples include pattern recognition data mining machine learning algorithms and visualization major research efforts in the field include sequence alignment gene finding genome assembly drug design drug discovery protein structure alignment protein structure prediction prediction of gene expression and protein protein interactions genome wide association studies and the modeling of evolution bioinformatics now entails the creation and advancement of databases algorithms computational and statistical techniques and theory to solve formal and practical problems arising from the management and analysis of biological data over the past few decades rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce tremendous amount of information related to molecular biology bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes approaches common activities in bioinformatics include mapping and analyzing dna and protein sequences aligning dna and protein sequences to compare them and creating and viewing models of protein structures there are two fundamental ways of modelling biological system living cell both coming under bioinformatic approaches static sequences proteins nucleic acids and peptides interaction data among the above entities including microarray data and networks of proteins metabolites dynamic structures proteins nucleic acids ligands including metabolites and drugs and peptides structures studied with bioinformatics tools are not considered static anymore and their dynamics is often the core of the structural studies systems biology comes under this category including reaction fluxes and variable concentrations of metabolites multi agent based modelling approaches capturing cellular events such as signalling transcription and reaction dynamics broad sub category under bioinformatics is structural bioinformatics relation to other fields bioinformatics is science field that is similar to but distinct from biological computation and computational biology biological computation uses bioengineering and biology to build biological computers whereas bioinformatics uses computation to better understand biology bioinformatics and computational biology have similar aims and approaches but they differ in scale bioinformatics organizes and analyzes basic biological data whereas computational biology builds theoretical models of biological systems just as mathematical biology does with mathematical models analyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory artificial intelligence soft computing data mining image processing and computer simulation the algorithms in turn depend on theoretical foundations such as discrete mathematics control theory system theory information theory and statistics sequence analysis the sequences of different genes or proteins may be aligned side by side to measure their similarity this alignment compares protein sequences containing wpp domains since the phage was sequenced in the dna sequences of thousands of organisms have been decoded and stored in databases this sequence information is analyzed to determine genes that encode proteins rna genes regulatory sequences structural motifs and repetitive sequences comparison of genes within species or between different species can show similarities between protein functions or relations between species the use of molecular systematics to construct phylogenetic trees with the growing amount of data it long ago became impractical to analyze dna sequences manually today computer programs such as blast are used daily to search sequences from more than organisms containing over billion nucleotides these programs can compensate for mutations exchanged deleted or inserted bases in the dna sequence to identify sequences that are related but not identical variant of this sequence alignment is used in the sequencing process itself the so called shotgun sequencing technique which was used for example by the institute for genomic research to sequence the first bacterial genome haemophilus influenzae does not produce entire chromosomes instead it generates the sequences of many thousands of small dna fragments ranging from to nucleotides long depending on the sequencing technology the ends of these fragments overlap and when aligned properly by genome assembly program can be used to reconstruct the complete genome shotgun sequencing yields sequence data quickly but the task of assembling the fragments can be quite complicated for larger genomes for genome as large as the human genome it may take many days of cpu time on large memory multiprocessor computers to assemble the fragments and the resulting assembly usually contains numerous gaps that must be filled in later shotgun sequencing is the method of choice for virtually all genomes sequenced today and genome assembly algorithms are critical area of bioinformatics research following the goals that the human genome project left to achieve after its closure in new project developed by the national human genome research institute in the appeared the so called encode project is collaborative data collection of the functional elements of the human genome that uses next generation dna sequencing technologies and genomic tiling arrays technologies able to generate automatically large amounts of data with lower research costs but with the same quality and viability in the project revealed that percent of the human genome is covered by primary rna transcripts produced by alternative splicing which could mean that the human genome consists of vast transcriptional networks another aspect of bioinformatics in sequence analysis is annotation this involves computational gene finding to search for protein coding genes rna genes and other functional sequences within genome not all of the nucleotides within genome are part of genes within the genomes of higher organisms large parts of the dna do not serve any obvious purpose this so called junk dna removed in process called splicing that take place in the nucleus after the transcription of the dna may however contain unrecognized functional elements recent software patented are now able to identify tissue specific alternative splicing events which will allow scientist to identify multiple gene products that come from the same transcripts in that way bioinformatics helps to bridge the gap between genome and proteome projects for example in the use of dna sequences for protein identification genome annotation in the context of genomics annotation is the process of marking the genes and other biological features in dna sequence this process needs to be automated because most genomes are too large to annotate by hand not to mention the desire to annotate as many genomes as possible as the rate of sequencing has ceased to pose bottleneck annotation is made possible by the fact that genes have recognisable start and stop regions although the exact sequence found in these regions can vary between genes the first genome annotation software system was designed in by owen white who was part of the team at the institute for genomic research that sequenced and analyzed the first genome of free living organism to be decoded the bacterium haemophilus influenzae white built software system to find the genes fragments of genomic sequence that encode proteins the transfer rnas and to make initial assignments of function to those genes most current genome annotation systems work similarly but the programs available for analysis of genomic dna such as the genemark program trained and used to find protein coding genes in haemophilus influenzae are constantly changing and improving computational evolutionary biology evolutionary biology is the study of the origin and descent of species as well as their change over time informatics has assisted evolutionary biologists by enabling researchers to trace the evolution of large number of organisms by measuring changes in their dna rather than through physical taxonomy or physiological observations alone more recently compare entire genomes which permits the study of more complex evolutionary events such as gene duplication horizontal gene transfer and the prediction of factors important in bacterial speciation build complex computational models of populations to predict the outcome of the system over time track and share information on an increasingly large number of species and organisms future work endeavours to reconstruct the now more complex tree of life the area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology but the two areas are not necessarily related comparative genomics the core of comparative genome analysis is the establishment of the correspondence between genes orthology analysis or other genomic features in different organisms it is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes multitude of evolutionary events acting at various organizational levels shape genome evolution at the lowest level point mutations affect individual nucleotides at higher level large chromosomal segments undergo duplication lateral transfer inversion transposition deletion and insertion ultimately whole genomes are involved in processes of hybridization and endosymbiosis often leading to rapid speciation the complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms who have recourse to spectra of algorithmic statistical and mathematical techniques ranging from exact heuristics fixed parameter and approximation algorithms for problems based on parsimony models to markov chain monte carlo algorithms for bayesian analysis of problems based on probabilistic models many of these studies are based on the homology detection and protein families computation pan genomics pan genomics is concept introduced in by tettelin and medini which eventually took root in bioinformatics pan genome is the complete gene repertoire of particular taxonomic group although initially applied to closely related strains of species it can be applied to larger context like genus phylum etc it is divided in two parts the core genome set of genes common to all the genomes under study these are often housekeeping genes vital for survival and the dispensable flexible genome set of genes not present in all but one or some genomes under study genetics of disease with the advent of next generation sequencing we are obtaining enough sequence data to map the genes of complex diseases such as infertility breast cancer or alzheimer disease genome wide association studies are essential to pinpoint the mutations for such complex diseases furthermore the possibility for genes to be used at prognosis diagnosis or treatment is one of the most essential applications many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis analysis of mutations in cancer in cancer the genomes of affected cells are rearranged in complex or even unpredictable ways massive sequencing efforts are used to identify previously unknown point mutations in variety of genes in cancer continue to produce specialized automated systems to manage the sheer volume of sequence data produced and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms new physical detection technologies are employed such as oligonucleotide microarrays to identify chromosomal gains and losses called comparative genomic hybridization and single nucleotide polymorphism arrays to detect known point mutations these detection methods simultaneously measure several hundred thousand sites throughout the genome and when used in high throughput to measure thousands of samples generate terabytes of data per experiment again the massive amounts and new types of data generate new opportunities for the data is often found to contain considerable variability or noise and thus hidden markov model and change point analysis methods are being developed to infer real copy number changes however with the breakthroughs that the next generation sequencing technology is providing to the field of bioinformatics cancer genomics may be drastically change this new methods and software allow to sequence in rapid and affordable way many cancer genomes this could mean more flexible process to classify types of cancer by analysis of cancer driven mutations in the genome furthermore individual tracking of patients during the progression of the disease may be possible in the future with the sequence of cancer samples another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors gene and protein expression analysis of gene expression the expression of many genes can be determined by measuring mrna levels with multiple techniques including microarrays expressed cdna sequence tag est sequencing serial analysis of gene expression sage tag sequencing massively parallel signature sequencing mpss rna seq also known as whole transcriptome shotgun sequencing wtss or various applications of multiplexed in situ hybridization all of these techniques are extremely noise prone and or subject to bias in the biological measurement and major research area in computational biology involves developing statistical tools to separate signal from noise in high throughput gene expression studies such studies are often used to determine the genes implicated in disorder one might compare microarray data from cancerous epithelial cells to data from non cancerous cells to determine the transcripts that are up regulated and down regulated in particular population of cancer cells analysis of protein expression protein microarrays and high throughput ht mass spectrometry ms can provide snapshot of the proteins present in biological sample bioinformatics is very much involved in making sense of protein microarray and ht ms data the former approach faces similar problems as with microarrays targeted at mrna the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases and the complicated statistical analysis of samples where multiple but incomplete peptides from each protein are detected analysis of regulation regulation is the complex orchestration of events starting with an extracellular signal such as hormone and leading to an increase or decrease in the activity of one or more proteins bioinformatics techniques have been applied to explore various steps in this process for example promoter analysis involves the identification and study of sequence motifs in the dna surrounding the coding region of gene these motifs influence the extent to which that region is transcribed into mrna expression data can be used to infer gene regulation one might compare microarray data from wide variety of states of an organism to form hypotheses about the genes involved in each state in single cell organism one might compare stages of the cell cycle along with various stress conditions heat shock starvation etc one can then apply clustering algorithms to that expression data to determine which genes are co expressed for example the upstream regions promoters of co expressed genes can be searched for over represented regulatory elements examples of clustering algorithms applied in gene clustering are means clustering self organizing maps soms hierarchical clustering and consensus clustering methods such as the bi copam the later namely bi copam has been actually proposed to address various issues specific to gene discovery problems such as consistent co expression of genes over multiple microarray datasets structural bioinformatics dimensional protein structures such as this one are common subjects in bioinformatic analyses protein structure prediction is another important application of bioinformatics the amino acid sequence of protein the so called primary structure can be easily determined from the sequence on the gene that codes for it in the vast majority of cases this primary structure uniquely determines structure in its native environment of course there are exceptions such as the bovine spongiform encephalopathy mad cow disease prion knowledge of this structure is vital in understanding the function of the protein structural information is usually classified as one of secondary tertiary and quaternary structure viable general solution to such predictions remains an open problem most efforts have so far been directed towards heuristics that work most of the time one of the key ideas in bioinformatics is the notion of homology in the genomic branch of bioinformatics homology is used to predict the function of gene if the sequence of gene whose function is known is homologous to the sequence of gene whose function is unknown one could infer that may share function in the structural branch of bioinformatics homology is used to determine which parts of protein are important in structure formation and interaction with other proteins in technique called homology modeling this information is used to predict the structure of protein once the structure of homologous protein is known this currently remains the only way to predict protein structures reliably one example of this is the similar protein homology between hemoglobin in humans and the hemoglobin in legumes leghemoglobin both serve the same purpose of transporting oxygen in the organism though both of these proteins have completely different amino acid sequences their protein structures are virtually identical which reflects their near identical purposes other techniques for predicting protein structure include protein threading and de novo from scratch physics based modeling network and systems biology network analysis seeks to understand the relationships within biological networks such as metabolic or protein protein interaction networks although biological networks can be constructed from single type of molecule or entity such as genes network biology often attempts to integrate many different data types such as proteins small molecules gene expression data and others which are all connected physically functionally or both systems biology involves the use of computer simulations of cellular subsystems such as the networks of metabolites and enzymes that comprise metabolism signal transduction pathways and gene regulatory networks to both analyze and visualize the complex connections of these cellular processes artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple artificial life forms molecular interaction networks interactions between proteins are frequently visualized and analyzed using networks this network is made up of protein protein interactions from treponema pallidum the causative agent of syphilis and other diseases tens of thousands of three dimensional protein structures have been determined by ray crystallography and protein nuclear magnetic resonance spectroscopy protein nmr and central question in structural bioinformatics is whether it is practical to predict possible protein protein interactions only based on these shapes without performing protein protein interaction experiments variety of methods have been developed to tackle the protein protein docking problem though it seems that there is still much work to be done in this field other interactions encountered in the field include protein ligand including drug and protein peptide molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms termed docking algorithms for studying molecular interactions others literature analysis the growth in the number of published literature makes it virtually impossible to read every paper resulting in disjointed sub fields of research literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources for example abbreviation recognition identify the long form and abbreviation of biological terms named entity recognition recognizing biological terms such as gene names protein protein interaction identify which proteins interact with which proteins from text the area of research draws from statistics and computational linguistics high throughput image analysis computational technologies are used to accelerate or fully automate the processing quantification and analysis of large amounts of high information content biomedical imagery modern image analysis systems augment an observer ability to make measurements from large or complex set of images by improving accuracy objectivity or speed fully developed analysis system may completely replace the observer although these systems are not unique to biomedical imagery biomedical imaging is becoming more important for both diagnostics and research some examples are high throughput and high fidelity quantification and sub cellular localization high content screening bioimage informatics morphometrics clinical image analysis and visualization determining the real time air flow patterns in breathing lungs of living animals quantifying occlusion size in real time imagery from the development of and recovery during arterial injury making behavioral observations from extended video recordings of laboratory animals infrared measurements for metabolic activity determination inferring clone overlaps in dna mapping the sulston score high throughput single cell data analysis computational techniques are used to analyse high throughput low measurement single cell data such as that obtained from flow cytometry these methods typically involve finding populations of cells that are relevant to particular disease state or experimental condition biodiversity informatics biodiversity informatics deals with the collection and analysis of biodiversity data such as taxonomic databases or microbiome data examples of such analyses include phylogenetics niche modelling species richness mapping or species identification tools databases databases are essential for bioinformatics research and applications there is huge number of available databases covering almost everything from dna and protein sequences molecular structures to phenotypes and biodiversity databases generally fall into one of three types some contain data resulting directly from empirical methods such as gene knockouts others consist of predicted data and most contain data from both sources there are meta databases that incorporate data compiled from multiple other databases some others are specialized such as those specific to an organism these databases vary in their format way of accession and whether they are public or not some of the most commonly used databases are listed below for more comprehensive list please check the link at the beginning of the subsection used in motif finding genomenet motif search used in gene ontology toppgene funcassociate enrichr gather used in gene finding hidden markov model used in finding protein structures family pfam used for next generation sequencing not database but data format fastq format used in gene expression analysis geo arrayexpress used in network analysis interaction analysis databases biogrid mint hprd curated human signaling network functional networks string kegg please keep in mind that this is quick sampling and generally most computation data is supported by wet lab data as well software and tools software tools for bioinformatics range from simple command line tools to more complex graphical programs and standalone web services available from various bioinformatics companies or public institutions open source bioinformatics software many free and open source software tools have existed and continued to grow since the the combination of continued need for new algorithms for the analysis of emerging types of biological readouts the potential for innovative in silico experiments and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open source software available regardless of their funding arrangements the open source tools often act as incubators of ideas or community supported plug ins in commercial applications they may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration the range of open source software packages includes titles such as bioconductor bioperl biopython biojava biojs bioruby bioclipse emboss net bio apache taverna and ugene to maintain this tradition and create further opportunities the non profit open bioinformatics foundation have supported the annual bioinformatics open source conference bosc since an alternative method to build public bioinformatics databases is to use the mediawiki engine with the wikiopener extension this system allows the database to be accessed and updated by all experts in the field web services in bioinformatics soap and rest based interfaces have been developed for wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms data and computing resources on servers in other parts of the world the main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads basic bioinformatics services are classified by the ebi into three categories sss sequence search services msa multiple sequence alignment and bsa biological sequence analysis the availability of these service oriented bioinformatics resources demonstrate the applicability of web based bioinformatics solutions and range from collection of standalone tools with common data format under single standalone or web based interface to integrative distributed and extensible bioinformatics workflow management systems bioinformatics workflow management systems bioinformatics workflow management system is specialized form of workflow management system designed specifically to compose and execute series of computational or data manipulation steps or workflow in bioinformatics application such systems are designed to provide an easy to use environment for individual application scientists themselves to create their own workflows provide interactive tools for the scientists enabling them to execute their workflows and view their results in real time simplify the process of sharing and reusing workflows between the scientists enable scientists to track the provenance of the workflow execution results and the workflow creation steps some of the platforms giving this service galaxy kepler taverna ugene anduril education platforms software platforms designed to teach bioinformatics concepts and methods include rosalind and online courses offered through the swiss institute of bioinformatics training portal the canadian bioinformatics workshops provides videos and slides from training workshops on their website under creative commons license conferences there are several large conferences that are concerned with bioinformatics some of the most notable examples are intelligent systems for molecular biology ismb european conference on computational biology eccb and research in computational molecular biology recomb see also references further reading raul isea the present day meaning of the word bioinformatics global journal of advanced research ilzins isea and hoebeke can bioinformatics be considered as an experimental biological science achuthsankar nair computational biology bioinformatics gentle overview communications of computer society of india january aluru srinivas ed handbook of computational molecular biology chapman hall crc isbn chapman hall crc computer and information science series baldi and brunak bioinformatics the machine learning approach nd edition mit press isbn barnes and gray eds bioinformatics for geneticists first edition wiley isbn baxevanis and ouellette eds bioinformatics practical guide to the analysis of genes and proteins third edition wiley isbn baxevanis petsko stein and stormo eds current protocols in bioinformatics wiley isbn cristianini and hahn introduction to computational genomics cambridge university press isbn isbn durbin eddy krogh and mitchison biological sequence analysis cambridge university press isbn keedwell intelligent bioinformatics the application of artificial intelligence techniques to bioinformatics problems wiley isbn kohane et al microarrays for an integrative genomics the mit press isbn lund et al immunological bioinformatics the mit press isbn pachter lior and sturmfels bernd algebraic statistics for computational biology cambridge university press isbn pevzner pavel computational molecular biology an algorithmic approach the mit press isbn soinov bioinformatics and pattern recognition come together journal of pattern recognition research jprr vol stevens hallam life out of sequence data driven history of bioinformatics chicago the university of chicago press isbn tisdall james beginning perl for bioinformatics reilly isbn dedicated issue of philosophical transactions on bioinformatics freely available catalyzing inquiry at the interface of computing and biology cstb report calculating the secrets of life contributions of the mathematical sciences and computing to molecular biology foundations of computational and systems biology mit course computational biology genomes networks evolution free mit course external links bioinformatics resource portal sib", "World Wide Web": "the world wide web www is an information space where documents and other web resources are identified by urls interlinked by hypertext links and can be accessed via the internet it has become known simply as the web the world wide web was central to the development of the information age and is the primary tool billions use to interact on the internet and it has changed people lives immeasurably the world wide web was invented by english scientist tim berners lee in he wrote the first web browser in while employed at cern in switzerland web pages are primarily text documents formatted and annotated with hypertext markup language html in addition to formatted text web pages may contain images video and software components that are rendered in the user web browser as coherent pages of multimedia content embedded hyperlinks permit users to navigate between web pages multiple web pages with common theme common domain name or both may be called website website content can largely be provided by the publisher or interactive where users contribute content or the content depends upon the user or their actions websites may be mostly informative primarily for entertainment or largely for commercial purposes history the next computer used by tim berners lee at cern berners lee vision of global hyperlinked information system became possibility by the second half of the by the global internet began to penetrate europe and the domain name system which the uniform resource locator is built upon came into being in the first direct ip connection between europe and north america was made and berners lee began to openly discuss the possibility of web like system at cern in march tim berners lee issued proposal to the management at cern for system called mesh that referenced enquire database and software project he had built in which used the term web and described more elaborate information management system based on links embedded in readable text imagine then the references in this document all being associated with the network address of the thing to which they referred so that while reading this document you could skip to them with click of the mouse such system he explained could be referred to using one of the existing meanings of the word hypertext term that he says was coined in the there is no reason the proposal continues why such hypertext links could not encompass multimedia documents including graphics speech and video so that berners lee goes on to propose the term hypermedia with help from his colleague and fellow hypertext enthusiast robert cailliau he published more formal proposal on november to build hypertext project called worldwideweb one word also as web of hypertext documents to be viewed by browsers using client server architecture at this point html and http had already been in development for about two months and the first web server was about month from completing its first successful test this proposal estimated that read only web would be developed within three months and that it would take six months to achieve the creation of new links and new material by readers so that authorship becomes universal as well as the automatic notification of reader when new material of interest to him her has become available while the read only goal was met accessible authorship of web content took longer to mature with the wiki concept webdav blogs web and rss atom the proposal was modeled after the sgml reader dynatext by electronic book technology spin off from the institute for research in information and scholarship at brown university the dynatext system licensed by cern was key player in the extension of sgml iso to hypermedia within hytime but it was considered too expensive and had an inappropriate licensing policy for use in the general high energy physics community namely fee for each document and each document alteration the cern data center in housing some www servers next computer was used by berners lee as the world first web server and also to write the first web browser worldwideweb in by christmas berners lee had built all the tools necessary for working web the first web browser which was web editor as well the first web server and the first web pages which described the project itself the first web page may be lost but paul jones of unc chapel hill in north carolina announced in may that berners lee gave him what he says is the oldest known web page during visit to unc jones stored it on magneto optical drive and on his next computer on august berners lee published short summary of the world wide web project on the newsgroup alt hypertext this date also marked the debut of the web as publicly available service on the internet although new users only accessed it after august for this reason this is considered the internaut day several newsmedia have reported that the first photo on the web was published by berners lee in an image of the cern house band les horribles cernettes taken by silvano de gennaro gennaro has disclaimed this story writing that media were totally distorting our words for the sake of cheap sensationalism the first server outside europe was installed at the stanford linear accelerator center slac in palo alto california to host the spires hep database accounts differ substantially as to the date of this event the world wide web consortium timeline says december whereas slac itself claims december as does document titled little history of the world wide web the underlying concept of hypertext originated in previous projects from the such as the hypertext editing system hes at brown university ted nelson project xanadu and douglas engelbart on line system nls both nelson and engelbart were in turn inspired by vannevar bush microfilm based memex which was described in the essay as we may think berners lee breakthrough was to marry hypertext to the internet in his book weaving the web he explains that he had repeatedly suggested that marriage between the two technologies was possible to members of both technical communities but when no one took up his invitation he finally assumed the project himself in the process he developed three essential technologies system of globally unique identifiers for resources on the web and elsewhere the universal document identifier udi later known as uniform resource locator url and uniform resource identifier uri the publishing language hypertext markup language html the hypertext transfer protocol http the world wide web had number of differences from other hypertext systems available at the time the web required only unidirectional links rather than bidirectional ones making it possible for someone to link to another resource without action by the owner of that resource it also significantly reduced the difficulty of implementing web servers and browsers in comparison to earlier systems but in turn presented the chronic problem of link rot unlike predecessors such as hypercard the world wide web was non proprietary making it possible to develop servers and clients independently and to add extensions without licensing restrictions on april cern announced that the world wide web would be free to anyone with no fees due coming two months after the announcement that the server implementation of the gopher protocol was no longer free to use this produced rapid shift away from gopher and towards the web an early popular web browser was violawww for unix and the windowing system robert cailliau jean fran\u00e7ois abramatic of ibm and tim berners lee at the th anniversary of the world wide web consortium scholars generally agree that turning point for the world wide web began with the introduction of the mosaic web browser in graphical browser developed by team at the national center for supercomputing applications at the university of illinois at urbana champaign ncsa uiuc led by marc andreessen funding for mosaic came from the high performance computing and communications initiative and the high performance computing and communication act of one of several computing developments initiated by senator al gore prior to the release of mosaic graphics were not commonly mixed with text in web pages and the web popularity was less than older protocols in use over the internet such as gopher and wide area information servers wais mosaic graphical user interface allowed the web to become by far the most popular internet protocol the world wide web consortium was founded by tim berners lee after he left the european organization for nuclear research cern in october it was founded at the massachusetts institute of technology laboratory for computer science mit lcs with support from the defense advanced research projects agency darpa which had pioneered the internet year later second site was founded at inria french national computer research lab with support from the european commission dg infso and in third continental site was created in japan at keio university by the end of the total number of websites was still relatively small but many notable websites were already active that foreshadowed or inspired today most popular services connected by the existing internet other websites were created around the world adding international standards for domain names and html since then berners lee has played an active role in guiding the development of web standards such as the markup languages to compose web pages in and has advocated his vision of semantic web the world wide web enabled the spread of information over the internet through an easy to use and flexible format it thus played an important role in popularizing use of the internet although the two terms are sometimes conflated in popular use world wide web is not synonymous with internet the web is an information space containing hyperlinked documents and other resources identified by their uris it is implemented as both client and server software using internet protocols such as tcp ip and http tim berners lee was knighted in by queen elizabeth ii for services to the global development of the internet function mosaic web browser helped to make the web much more usable the terms internet and world wide web are often used without much distinction however the two are not the same the internet is global system of interconnected computer networks in contrast the world wide web is one of the services transferred over these networks it is collection of text documents and other resources linked by hyperlinks and urls usually accessed by web browsers from web servers viewing web page on the world wide web normally begins either by typing the url of the page into web browser or by following hyperlink to that page or resource the web browser then initiates series of background communication messages to fetch and display the requested page in the using browser to view web pages and to move from one web page to another through hyperlinks came to be known as browsing web surfing after channel surfing or navigating the web early studies of this new behavior investigated user patterns in using web browsers one study for example found five user patterns exploratory surfing window surfing evolved surfing bounded navigation and targeted navigation the following example demonstrates the functioning of web browser when accessing page at the url the browser resolves the server name of the url www example org into an internet protocol address using the globally distributed domain name system dns this lookup returns an ip address such as the browser then requests the resource by sending an http request across the internet to the computer at that address it requests service from specific tcp port number that is well known for the http service so that the receiving host can distinguish an http request from other network protocols it may be servicing the http protocol normally uses port number the content of the http request can be as simple as two lines of text get home html http host www example org the computer receiving the http request delivers it to web server software listening for requests on port if the web server can fulfill the request it sends an http response back to the browser indicating success http ok content type text html charset utf followed by the content of the requested page hypertext markup language html for basic web page might look like this example org the world wide web the world wide web abbreviated as www and commonly known the web browser parses the html and interprets the markup for paragraph and such that surrounds the words to format the text on the screen many web pages use html to reference the urls of other resources such as images other embedded media scripts that affect page behavior and cascading style sheets that affect page layout the browser makes additional http requests to the web server for these other internet media types as it receives their content from the web server the browser progressively renders the page onto the screen as specified by its html and these additional resources linking most web pages contain hyperlinks to other related pages and perhaps to downloadable files source documents definitions and other web resources in the underlying html hyperlink looks like this example org homepage graphic representation of minute fraction of the www demonstrating hyperlinks such collection of useful related resources interconnected via hypertext links is dubbed web of information publication on the internet created what tim berners lee first called the worldwideweb in its original camelcase which was subsequently discarded in november the hyperlink structure of the www is described by the webgraph the nodes of the webgraph correspond to the web pages or urls the directed edges between them to the hyperlinks over time many web resources pointed to by hyperlinks disappear relocate or are replaced with different content this makes hyperlinks obsolete phenomenon referred to in some circles as link rot and the hyperlinks affected by it are often called dead links the ephemeral nature of the web has prompted many efforts to archive web sites the internet archive active since is the best known of such efforts dynamic updates of web pages javascript is scripting language that was initially developed in by brendan eich then of netscape for use within web pages the standardised version is ecmascript to make web pages more interactive some web applications also use javascript techniques such as ajax asynchronous javascript and xml client side script is delivered with the page that can make additional http requests to the server either in response to user actions such as mouse movements or clicks or based on elapsed time the server responses are used to modify the current page rather than creating new page with each response so the server needs only to provide limited incremental information multiple ajax requests can be handled at the same time and users can interact with the page while data is retrieved web pages may also regularly poll the server to check whether new information is available www prefix many hostnames used for the world wide web begin with www because of the long standing practice of naming internet hosts according to the services they provide the hostname of web server is often www in the same way that it may be ftp for an ftp server and news or nntp for usenet news server these host names appear as domain name system dns or subdomain names as in www example com the use of www is not required by any technical or policy standard and many web sites do not use it indeed the first ever web server was called nxoc cern ch according to paolo palazzi who worked at cern along with tim berners lee the popular use of www as subdomain was accidental the world wide web project page was intended to be published at www cern ch while info cern ch was intended to be the cern home page however the dns records were never switched and the practice of prepending www to an institution website domain name was subsequently copied many established websites still use the prefix or they employ other subdomain names such as www secure or en for special purposes many such web servers are set up so that both the main domain name example com and the www subdomain www example com refer to the same site others require one form or the other or they may map to different web sites the use of subdomain name is useful for load balancing incoming web traffic by creating cname record that points to cluster of web servers since currently only subdomain can be used in cname the same result cannot be achieved by using the bare domain root when user submits an incomplete domain name to web browser in its address bar input field some web browsers automatically try adding the prefix www to the beginning of it and possibly com org and net at the end depending on what might be missing for example entering may be transformed to and openoffice to this feature started appearing in early versions of mozilla firefox when it still had the working title firebird in early from an earlier practice in browsers such as lynx it is reported that microsoft was granted us patent for the same idea in but only for mobile devices in english www is usually read as double double double some users pronounce it dub dub dub particularly in new zealand stephen fry in his podgrammes series of podcasts pronounces it wuh wuh wuh the english writer douglas adams once quipped in the independent on sunday the world wide web is the only thing know of whose shortened form takes three times longer to say than what it short for in mandarin chinese world wide web is commonly translated via phono semantic matching to w\u00e0n w\u00e9i w\u01ceng which satisfies www and literally means myriad dimensional net translation that reflects the design concept and proliferation of the world wide web tim berners lee web space states that world wide web is officially spelled as three separate words each capitalised with no intervening hyphens use of the www prefix is declining as web web applications seek to brand their domain names and make them easily pronounceable as the mobile web grows in popularity services like gmail com myspace com facebook com and twitter com are most often mentioned without adding www or indeed com to the domain scheme specifiers the scheme specifiers and at the start of web uri refer to hypertext transfer protocol or http secure respectively they specify the communication protocol to use for the request and response the http protocol is fundamental to the operation of the world wide web and the added encryption layer in https is essential when browsers send or retrieve confidential data such as passwords or banking information web browsers usually automatically prepend to user entered uris if omitted web security for criminals the web has become the preferred way to spread malware cybercrime on the web can include identity theft fraud espionage and intelligence gathering web based vulnerabilities now outnumber traditional computer security concerns and as measured by google about one in ten web pages may contain malicious code most web based attacks take place on legitimate websites and most as measured by sophos are hosted in the united states china and russia the most common of all malware threats is sql injection attacks against websites through html and uris the web was vulnerable to attacks like cross site scripting xss that came with the introduction of javascript and were exacerbated to some degree by web and ajax web design that favors the use of scripts today by one estimate of all websites are open to xss attacks on their users phishing is another common threat to the web sa the security division of emc today announced the findings of its january fraud report estimating the global losses from phishing at billion in two of the well known phishing methods are covert redirect and open redirect proposed solutions vary to extremes large security vendors like mcafee already design governance and compliance suites to meet post regulations and some like finjan have recommended active real time inspection of code and all content regardless of its source some have argued that for enterprise to see security as business opportunity rather than cost center ubiquitous always on digital rights management enforced in the infrastructure by handful of organizations must replace the hundreds of companies that today secure data and networks jonathan zittrain has said users sharing responsibility for computing safety is far preferable to locking down the internet privacy every time client requests web page the server can identify the request ip address and usually logs it also unless set not to do so most web browsers record requested web pages in viewable history feature and usually cache much of the content locally unless the server browser communication uses https encryption web requests and responses travel in plain text across the internet and can be viewed recorded and cached by intermediate systems when web page asks for and the user supplies personally identifiable information such as their real name address mail address etc web based entities can associate current web traffic with that individual if the website uses http cookies username and password authentication or other tracking techniques it can relate other web visits before and after to the identifiable information provided in this way it is possible for web based organisation to develop and build profile of the individual people who use its site or sites it may be able to build record for an individual that includes information about their leisure activities their shopping interests their profession and other aspects of their demographic profile these profiles are obviously of potential interest to marketeers advertisers and others depending on the website terms and conditions and the local laws that apply information from these profiles may be sold shared or passed to other organisations without the user being informed for many ordinary people this means little more than some unexpected mails in their in box or some uncannily relevant advertising on future web page for others it can mean that time spent indulging an unusual interest can result in deluge of further targeted marketing that may be unwelcome law enforcement counter terrorism and espionage agencies can also identify target and track individuals based on their interests or proclivities on the web social networking sites try to get users to use their real names interests and locations they believe this makes the social networking experience more realistic and therefore more engaging for all their users on the other hand uploaded photographs or unguarded statements can be identified to an individual who may regret this exposure employers schools parents and other relatives may be influenced by aspects of social networking profiles that the posting individual did not intend for these audiences on line bullies may make use of personal information to harass or stalk users modern social networking websites allow fine grained control of the privacy settings for each individual posting but these can be complex and not easy to find or use especially for beginners photographs and videos posted onto websites have caused particular problems as they can add person face to an on line profile with modern and potential facial recognition technology it may then be possible to relate that face with other previously anonymous images events and scenarios that have been imaged elsewhere because of image caching mirroring and copying it is difficult to remove an image from the world wide web standards many formal standards and other technical specifications and software define the operation of different aspects of the world wide web the internet and computer information exchange many of the documents are the work of the world wide web consortium headed by berners lee but some are produced by the internet engineering task force ietf and other organizations usually when web standards are discussed the following publications are seen as foundational recommendations for markup languages especially html and xhtml from the these define the structure and interpretation of hypertext documents recommendations for stylesheets especially css from the standards for ecmascript usually in the form of javascript from ecma international recommendations for the document object model from additional publications provide definitions of other essential technologies for the world wide web including but not limited to the following uniform resource identifier uri which is universal system for referencing resources on the internet such as hypertext documents and images uris often called urls are defined by the ietf rfc std uniform resource identifier uri generic syntax as well as its predecessors and numerous uri scheme defining rfcs hypertext transfer protocol http especially as defined by rfc http and rfc http authentication which specify how the browser and server authenticate each other accessibility there are methods for accessing the web in alternative mediums and formats to facilitate use by individuals with disabilities these disabilities may be visual auditory physical speech related cognitive neurological or some combination accessibility features also help people with temporary disabilities like broken arm or aging users as their abilities change the web receives information as well as providing information and interacting with society the world wide web consortium claims it essential that the web be accessible so it can provide equal access and equal opportunity to people with disabilities tim berners lee once noted the power of the web is in its universality access by everyone regardless of disability is an essential aspect many countries regulate web accessibility as requirement for websites international cooperation in the web accessibility initiative led to simple guidelines that web content authors as well as software developers can use to make the web accessible to persons who may or may not be using assistive technology the activity assures that web technology works in all languages scripts and cultures beginning in or unicode gained ground and eventually in december surpassed both ascii and western european as the web most frequently used character encoding originally rfc allowed resources to be identified by uri in subset of us ascii rfc allows more characters any character in the universal character set and now resource can be identified by iri in any language statistics between and the number of web users doubled and was expected to surpass two billion in early studies in and estimating the size of the web using capture recapture methods showed that much of the web was not indexed by search engines and the web was much larger than expected according to study there was massive number over billion of documents on the web mostly in the invisible web or deep web survey of million web pages determined that by far the most web content was in the english language next were pages in german french and japanese more recent study which used web searches in different languages to sample the web determined that there were over billion web pages in the publicly indexable web as of the end of january the indexable web contains at least billion pages on july google software engineers jesse alpert and nissan hajaj announced that google search had discovered one trillion unique urls over million domains operated of these were commercial or other domains operating in the generic top level domain com statistics measuring website popularity are usually based either on the number of page views or on associated server hits file requests that it receives speed issues frustration over congestion issues in the internet infrastructure and the high latency that results in slow browsing has led to pejorative name for the world wide web the world wide wait speeding up the internet is an ongoing discussion over the use of peering and qos technologies other solutions to reduce the congestion can be found at guidelines for web response times are second one tenth of second ideal response time the user does not sense any interruption second highest acceptable response time download times above second interrupt the user experience seconds unacceptable response time the user experience is interrupted and the user is likely to leave the site or system web caching web cache is server computer located either on the public internet or within an enterprise that stores recently accessed web pages to improve response time for users when the same content is requested within certain time after the original request most web browsers also implement browser cache for recently obtained data usually on the local disk drive http requests by browser may ask only for data that has changed since the last access web pages and resources may contain expiration information to control caching to secure sensitive data such as in online banking or to facilitate frequently updated sites such as news media even sites with highly dynamic content may permit basic resources to be refreshed only occasionally web site designers find it worthwhile to collate resources such as css data and javascript into few site wide files so that they can be cached efficiently enterprise firewalls often cache web resources requested by one user for the benefit of many some search engines store cached content of frequently accessed websites see also electronic publishing internet metaphors internet security lists of websites prestel streaming media web web development tools webgraph web literacy further reading niels br\u00fcgger ed web history pages historical perspective on the world wide web including issues of culture content and preservation external links the first website early archive of the first web site internet statistics growth and usage of the web and the internet living internet comprehensive history of the internet including the world wide web world wide web consortium recommendations reduce world wide wait world wide web size daily estimated size of the world wide web antonio casilli some elements for sociology of online interactions the erd\u0151s webgraph server offers weekly updated graph representation of constantly increasing fraction of the www the th anniversary of the world wide web is an animated video produced by usaid and techchange which explores the role of the www in addressing extreme poverty references", "Internet privacy": "internet privacy involves the right or mandate of personal privacy concerning the storing repurposing provision to third parties and displaying of information pertaining to oneself via the internet internet privacy is subset of data privacy privacy concerns have been articulated from the beginnings of large scale computer sharing privacy can entail either personally identifying information pii or non pii information such as site visitor behavior on website pii refers to any information that can be used to identify an individual for example age and physical address alone could identify who an individual is without explicitly disclosing their name as these two factors are unique enough to typically identify specific person some experts such as steve rambam private investigator specializing in internet privacy cases believe that privacy no longer exists saying privacy is dead get over it in fact it has been suggested that the appeal of online services is to broadcast personal information on purpose on the other hand in his essay the value of privacy security expert bruce schneier says privacy protects us from abuses by those in power even if we re doing nothing wrong at the time of surveillance visualization of internet trackers levels of privacy people with only casual concern for internet privacy need not achieve total anonymity internet users may protect their privacy through controlled disclosure of personal information the revelation of ip addresses non personally identifiable profiling and similar information might become acceptable trade offs for the convenience that users could otherwise lose using the workarounds needed to suppress such details rigorously on the other hand some people desire much stronger privacy in that case they may try to achieve internet anonymity to ensure privacy use of the internet without giving any third parties the ability to link the internet activities to personally identifiable information of the internet user in order to keep their information private people need to be careful with what they submit to and look at online when filling out forms and buying merchandise that becomes tracked and because the information was not private companies are now sending internet users spam and advertising on similar products there are also several governmental organizations that protect individual privacy and anonymity on the internet to point in an article presented by the ftc in october number of pointers were brought to attention that helps an individual internet user avoid possible identity theft and other cyber attacks preventing or limiting the usage of social security numbers online being wary and respectful of emails including spam messages being mindful of personal financial details creating and managing strong passwords and intelligent web browsing behaviours are recommended among others posting things on the internet can be harmful or in danger of malicious attack some information posted on the internet is permanent depending on the terms of service and privacy policies of particular services offered online this can include comments written on blogs pictures and internet sites such as facebook and twitter it is absorbed into cyberspace and once it is posted anyone can potentially find it and access it some employers may research potential employee by searching online for the details of their online behaviours possibly affecting the outcome of the success of the candidate risks to internet privacy companies are hired to watch what internet sites people visit and then use the information for instance by sending advertising based on one browsing history there are many ways in which people can divulge their personal information for instance by use of social media and by sending bank and credit card information to various websites moreover directly observed behaviour such as browsing logs search queries or contents of the facebook profile can be automatically processed to infer potentially more intrusive details about an individual such as sexual orientation political and religious views race substance use intelligence and personality those concerned about internet privacy often cite number of privacy risks events that can compromise privacy which may be encountered through internet use these range from the gathering of statistics on users to more malicious acts such as the spreading of spyware and the exploitation of various forms of bugs software faults several social networking sites try to protect the personal information of their subscribers on facebook for example privacy settings are available to all registered users they can block certain individuals from seeing their profile they can choose their friends and they can limit who has access to one pictures and videos privacy settings are also available on other social networking sites such as google plus and twitter the user can apply such settings when providing personal information on the internet in late facebook launched the beacon program where user rental records were released on the public for friends to see many people were enraged by this breach in privacy and the lane facebook inc case ensued children and adolescents often use the internet including social media in ways which risk their privacy cause for growing concern among parents young people also may not realise that all their information and browsing can and may be tracked while visiting particular site and that it is up to them to protect their own privacy they must be informed about all these risks for example on twitter threats include shortened links that lead one to potentially harmful places in their mail inbox threats include email scams and attachments that get them to install malware and disclose personal information on torrent sites threats include malware hiding in video music and software downloads even when using smartphone threats include geolocation meaning that one phone can detect where they are and post it online for all to see users can protect themselves by updating virus protection using security settings downloading patches installing firewall screening mail shutting down spyware controlling cookies using encryption fending off browser hijackers and blocking pop ups however most people have little idea how to go about doing many of these things how can the average user with no training be expected to know how to run their own network security especially as things are getting more complicated all the time many businesses hire professionals to take care of these issues but most individuals can only do their best to learn about all this in the federal trade commission in the usa considered the lack of privacy for children on the internet and created the children online privacy protection act coppa coppa limits the options which gather information from children and created warning labels if potential harmful information or content was presented in children internet protection act cipa was developed to implement safe internet policies such as rules and filter software these laws awareness campaigns parental and adult supervision strategies and internet filters can all help to make the internet safer for children around the world http cookies an http cookie is data stored on user computer that assists in automated access to websites or web features or other state information required in complex web sites it may also be used for user tracking by storing special usage history data in cookie and such cookies for example those used by google analytics are called tracking cookies cookies are common concern in the field of internet privacy although website developers most commonly use cookies for legitimate technical purposes cases of abuse occur in two researchers noted that social networking profiles could be connected to cookies allowing the social networking profile to be connected to browsing habits in the past web sites have not generally made the user explicitly aware of the storing of cookies however tracking cookies and especially third party tracking cookies are commonly used as ways to compile long term records of individuals browsing histories privacy concern that prompted european and us law makers to take action in cookies can also have implications for computer forensics in past years most computer users were not completely aware of cookies but recently users have become conscious of possible detrimental effects of internet cookies recent study done has shown that of users have at least once deleted cookies from their computer and that of users delete cookies from their computer every month since cookies are advertisers main way of targeting potential customers and some customers are deleting cookies some advertisers started to use persistent flash cookies and zombie cookies but modern browsers and anti malware software can now block or detect and remove such cookies the original developers of cookies intended that only the website that originally distributed cookies to users could retrieve them therefore returning only data already possessed by the website however in practice programmers can circumvent this restriction possible consequences include the placing of personally identifiable tag in browser to facilitate web profiling see below or use of cross site scripting or other techniques to steal information from user cookies cookies do have benefits that many people may not know one benefit is that for websites that one frequently visits that requires password cookies make it so they do not have to sign in every time cookie can also track one preferences to show them websites that might interest them cookies make more websites free to use without any type of payment some of these benefits are also seen as negative for example one of the most common ways of theft is hackers taking one user name and password that cookie saves while lot of sites are free they have to make profit some how so they sell their space to advertisers these ads which are personalized to one likes can often freeze one computer or cause annoyance cookies are mostly harmless except for third party cookies these cookies are not made by the website itself but by web banner advertising companies these third party cookies are so dangerous because they take the same information that regular cookies do such as browsing habits and frequently visited websites but then they give out this information to other companies cookies are often associated with pop up windows because these windows are often but not always tailored to person preferences these windows are an irritation because they are often hard to close out of because the close button is strategically hidden in an unlikely part of the screen in the worst cases these pop up ads can take over the screen and while trying to exit out of it can take one to another unwanted website cookies are seen so negatively because they are not understood and go unnoticed while someone is simply surfing the internet the idea that every move one makes while on the internet is being watched would frighten most users some users choose to disable cookies in their web browsers such an action can reduce some privacy risks but may severely limit or prevent the functionality of many websites all significant web browsers have this disabling ability built in with no external program required as an alternative users may frequently delete any stored cookies some browsers such as mozilla firefox and opera offer the option to clear cookies automatically whenever the user closes the browser third option involves allowing cookies in general but preventing their abuse there are also host of wrapper applications that will redirect cookies and cache data to some other location concerns exist that the privacy benefits of deleting cookies have been over stated the process of profiling also known as tracking assembles and analyzes several events each attributable to single originating entity in order to gain information especially patterns of activity relating to the originating entity some organizations engage in the profiling of people web browsing collecting the urls of sites visited the resulting profiles can potentially link with information that personally identifies the individual who did the browsing some web oriented marketing research organizations may use this practice legitimately for example in order to construct profiles of typical internet users such profiles which describe average trends of large groups of internet users rather than of actual individuals can then prove useful for market analysis although the aggregate data does not constitute privacy violation some people believe that the initial profiling does profiling becomes more contentious privacy issue when data matching associates the profile of an individual with personally identifiable information of the individual governments and organizations may set up honeypot websites featuring controversial topics with the purpose of attracting and tracking unwary people this constitutes potential danger for individuals flash cookies when some users choose to disable http cookies to reduce privacy risks as noted new types of cookies were invented since cookies are advertisers main way of targeting potential customers and some customers were deleting cookies some advertisers started to use persistent flash cookies and zombie cookies in study flash cookies were found to be popular mechanism for storing data on the top most visited sites another study of social media found that of the top web sites had at least one overlap between http and flash cookies however modern browsers and anti malware software can now block or detect and remove such cookies flash cookies also known as local shared objects work the same ways as normal cookies and are used by the adobe flash player to store information at the user computer they exhibit similar privacy risk as normal cookies but are not as easily blocked meaning that the option in most browsers to not accept cookies does not affect flash cookies one way to view and control them is with browser extensions or add ons flash cookies are unlike http cookies in sense that they are not transferred from the client back to the server web browsers read and write these cookies and can track any data by web usage although browsers such as internet explorer and firefox have added privacy browsing setting they still allow flash cookies to track the user and operate fully however the flash player browser plugin can be disabled or uninstalled and flash cookies can be disabled on per site or global basis adobe flash and pdf reader are not the only browser plugins whose past security defects have allowed spyware or malware to be installed there have also been problems with oracle java evercookies evercookies created by samy kamkar are javascript based applications which produce cookies in web browser that actively resist deletion by redundantly copying themselves in different forms on the user machine flash local shared objects various html storage mechanisms window name caching etc and resurrecting copies that are missing or expired evercookie accomplishes this by storing the cookie data in several types of storage mechanisms that are available on the local browser it has the ability to store cookies in over ten types of storage mechanisms so that once they are on one computer they will never be gone additionally if evercookie has found the user has removed any of the types of cookies in question it recreates them using each mechanism available evercookies are one type of zombie cookie however modern browsers and anti malware software can now block or detect and remove such cookies anti fraud uses some anti fraud companies have realized the potential of evercookies to protect against and catch cyber criminals these companies already hide small files in several places on the perpetrator computer but hackers can usually easily get rid of these the advantage to evercookies is that they resist deletion and can rebuild themselves advertising uses there is controversy over where the line should be drawn on the use of this technology cookies store unique identifiers on person computer that are used to predict what one wants many advertisement companies want to use this technology to track what their customers are looking at online evercookies enable advertisers to continue to track customer regardless of if one deletes their cookies or not some companies are already using this technology but the ethics are still being widely debated criticism anonymizer nevercookies are part of free firefox plugin that protects against evercookies this plugin extends firefox private browsing mode so that users will be completely protected from evercookies nevercookies eliminate the entire manual deletion process while keeping the cookies users want like browsing history and saved account information device fingerprinting device fingerprinting is fairly new technology that is useful in fraud prevention and safeguarding any information from one computer device fingerprinting uses data from the device and browser sessions to determine the risk of conducting business with the person using the device this technology allows companies to better assess the risks when business is conducted through sites that include commerce sites social networking and online dating sites and banks and other financial institutions threatmetrix is one of the leading vendors of device fingerprinting this company employs number of techniques to prevent fraud for example threatmetrix will pierce the proxy to determine the true location of device due to the growing number of hackers and fraudsters using botnets of millions of computers that are being unknowingly controlled this technology will help not only the companies at risk but the people who are unaware their computers are being used it is difficult to surf the web without being tracked by device fingerprinting today however for people who do not want device fingerprinting there are ways to attempt to block fingerprinting the only ways to stop device fingerprinting cause web browsing to be very slow and websites to display information incorrectly there are not convenient options for privacy when it comes to device fingerprinting said peter eckersely staff scientist at the electronic frontier foundation and privacy advocacy group trying to avoid device fingerprinting is mostly just impractical and inconvenient fingerprints are tough to avoid because they are taken from data that are routinely passed from computers to websites automatically even if someone changes something slightly the fingerprinters can still recognize the machine there is one way to figure out that device is being fingerprinted the software javascript can be used to collect fingerprinting data if it asks browser for specific information that could be clue that fingerprinter is working companies that are most known for conducting fingerprinting are advertisers sentinel advanced detection analysis and predator tracking sentinel advanced detection analysis and predator tracking is device fingerprinting software technology that identifies the device computer tablet smartphone being used to access website this information in turn can be used to help differentiate legitimate users from those using false identities or those attempting to work anonymously uses only http and javascript to identify device and identifies devices without requesting any personal information entered directly by the user it makes an accurate fingerprint of the device by using many different pieces of information including operating system browser and pc characteristics is concealed in that the user of the device has no idea that the device is being fingerprinted and there is no actual tagging of the device canvas fingerprinting canvas fingerprinting is one of number of browser fingerprinting techniques of tracking online users that allow websites to uniquely identify and track visitors using html canvas element instead of browser cookies or other similar means photographs on the internet no photos tag at wikimania today many people have digital cameras and post their photographs online the people depicted in these photos might not want to have them appear on the internet some organizations attempt to respond to this privacy related concern for example the wikimania conference required that photographers have the prior permission of the people in their pictures some people wore no photos tag to indicate they would prefer not to have their photo taken the harvard law review published short piece called in the face of danger facial recognition and privacy law much of it explaining how privacy law in its current form is of no help to those unwillingly tagged any individual can be unwillingly tagged in photo and displayed in manner that might violate them personally in some way and by the time facebook gets to taking down the photo many people will have already had the chance to view share or distribute it furthermore traditional tort law does not protect people who are captured by photograph in public because this is not counted as an invasion of privacy the extensive facebook privacy policy covers these concerns and much more for example the policy states that they reserve the right to disclose member information or share photos with companies lawyers courts government entities etc if they feel it absolutely necessary the policy also informs users that profile pictures are mainly to help friends connect to each other however these as well as other pictures can allow other people to invade person privacy by finding out information that can be used to track and locate certain individual in an article featured in abc news it was stated that two teams of scientists found out that hollywood stars could be giving up information about their private whereabouts very easily through pictures uploaded to the internet moreover it was found that pictures taken by some phones and tablets including iphones automatically attach the latitude and longitude of the picture taken through metadata unless this function is manually disabled face recognition technology can be used to gain access to person private data according to new study researchers at carnegie mellon university combined image scanning cloud computing and public profiles from social network sites to identify individuals in the offline world data captured even included user social security number experts have warned of the privacy risks faced by the increased merging of our online and offline identities the researchers have also developed an augmented reality mobile app that can display personal data over person image captured on smartphone screen since these technologies are widely available our future identities may become exposed to anyone with smartphone and an internet connection researchers believe this could force us to reconsider our future attitudes to privacy google street view google street view released in the in is currently the subject of an ongoing debate about possible infringement on individual privacy in an article entitled privacy reconsidered new representations data practices and the geoweb sarah elwood and agnieszka leszczynski argue that google street view facilitates identification and disclosure with more immediacy and less abstraction the medium through which street view disseminates information the photograph is very immediate in the sense that it can potentially provide direct information and evidence about person whereabouts activities and private property moreover the technology disclosure of information about person is less abstract in the sense that if photographed person is represented on street view in virtual replication of his or her own real life appearance in other words the technology removes abstractions of person appearance or that of his or her personal belongings there is an immediate disclosure of the person and object as they visually exist in real life although street view began to blur license plates and people faces in the technology is faulty and does not entirely ensure against accidental disclosure of identity and private property elwood and leszczynski note that many of the concerns leveled at street view stem from situations where its photograph like images were treated as definitive evidence of an individual involvement in particular activities in one instance ruedi noser swiss politician barely avoided public scandal when he was photographed in on google street view walking with woman who was not his wife the woman was actually his secretary similar situations necessarily arise from the fact that street view provides high resolution photographs and photographs hypothetically offer compelling objective evidence but as the case of the swiss politician illustrates even supposedly compelling photographic evidence is sometimes subject to gross this example further suggests that google street view may provide opportunities for privacy infringement and harassment through public dissemination of the photographs google street view does however blur or remove photographs of individuals and private property from image frames if the individuals request further blurring and or removal of the images this request can be submitted for review through the report problem button that is located on the bottom left hand side of every image window on google street view however google has made attempts to report problem difficult by disabling the why are you reporting the street view icon search engines search engines have the ability to track user searches personal information can be revealed through searches by the user computer account or ip address being linked to the search terms used search engines have claimed necessity to retain such information in order to provide better services protect against security pressure and protect against fraud search engine takes all of its users and assigns each one specific id number those in control of the database often keep records of where on the internet each member has traveled to aol system is one example aol has database million members deep each with their own specific id number the way that aolsearch is set up however allows for aol to keep records of all the websites visited by any given member even though the true identity of the user isn known full profile of member can be made just by using the information stored by aolsearch by keeping records of what people query through aolsearch the company is able to learn great deal about them without knowing their names search engines also are able to retain user information such as location and time spent using the search engine for up to ninety days most search engine operators use the data to get sense of which needs must be met in certain areas of their field people working in the legal field are also allowed to use information collected from these search engine websites the google search engine is given as an example of search engine that retains the information entered for period of three fourths of year before it becomes obsolete for public usage yahoo follows in the footsteps of google in the sense that it also deletes user information after period of ninety days other search engines such as ask search engine has promoted tool of askeraser which essentially takes away personal information when requested some changes made to internet search engines included that of google search engine beginning in google began to run new system where the google search became personalized the item that is searched and the results that are shown remembers previous information that pertains to the individual google search engine not only seeks what is searched but also strives to allow the user to feel like the search engine recognizes their interests this is achieved by using online advertising system that google uses to filter advertisements and search results that might interest the user is by having ranking system that tests relevancy that include observation of the behavior users exude while searching on google another function of search engines is the predictability of location search engines are able to predict where one location is currently by locating ip addresses and geographical locations google had publicly stated on january that its privacy policy will once again be altered this new policy will change the following for its users the privacy policy will become shorter and easier to comprehend and the information that users provide will be used in more ways than it is presently being used the goal of google is to make users experiences better than they currently are this new privacy policy is planned to come into effect on march peter fleischer the global privacy counselor for google has explained that if person is logged into his her google account and only if he she is logged in information will be gathered from multiple google services in which he she has used in order to be more accommodating google new privacy policy will combine all data used on google search engines youtube and gmail in order to work along the lines of person interests person in effect will be able to find what he she wants at more efficient rate because all searched information during times of login will help to narrow down new search results google privacy policy explains information they collect and why they collect it how they use the information and how to access and update information google will collect information to better service its users such as their language which ads they find useful or people that are important to them online google announces they will use this information to provide maintain protect google and its users the information google uses will give users more relevant search results and advertisements the new privacy policy explains that google can use shared information on one service in other google services from people who have google account and are logged in google will treat user as single user across all of their products google claims the new privacy policy will benefit its users by being simpler google will for example be able to correct the spelling of user friend name in google search or notify user they are late based on their calendar and current location even though google is updating their privacy policy its core privacy guidelines will not change for example google does not sell personal information or share it externally users and public officials have raised many concerns regarding google new privacy policy the main concern issue involves the sharing of data from multiple sources because this policy gathers all information and data searched from multiple engines when logged into google and uses it to help assist users privacy becomes an important element public officials and google account users are worried about online safety because of all this information being gathered from multiple sources some users do not like the overlapping privacy policy wishing to keep the service of google separate the update to google privacy policy has alarmed both public and private sectors the european union has asked google to delay the onset of the new privacy policy in order to ensure that it does not violate law this move is in accordance with objections to decreasing online privacy raised in other foreign nations where surveillance is more heavily scrutinized canada and germany have both held investigations into the legality of both facebook against respective privacy acts in the new privacy policy only heightens unresolved concerns regarding user privacy an additional feature of concern to the new google privacy policy is the nature of the policy one must accept all features or delete existing google accounts the update will affect the google social network therefore making google settings uncustomizable unlike other customizable social networking sites customizing the privacy settings of social network is key tactic that many feel is necessary for social networking sites this update in the system has some google users wary of continuing service additionally some fear the sharing of data amongst google services could lead to revelations of identities many using pseudonyms are concerned about this possibility and defend the role of pseudonyms in literature and history some solutions to being able to protect user privacy on the internet can include programs such as rapleaf which is website that has search engine that allows users to make all of one search information and personal information private other websites that also give this option to their users are facebook and amazon other search engines such as duckduckgo don store personal information scroogle anonymized google searches from still operating alternative to scroogle is the dutch search engine startpage com which also anonymizes google searches safe search engines metager metager is search engine and in germany by far the most popular safe search engine all servers are stationed in germany plus considering that the german legislation tends to respect privacy rights better than many other european countries ixquick ixquick is dutch based search engine it commits also to the protection of the privacy of his users ixquick uses similar safety features as metager yacy yacy is developed on the basis of community project which started in the search engine follows slightly different approach to the two previous ones using peer to peer principle that does not require any stationary and centralized servers this has its disadvantages but also the simple advantage of greater privacy when surfing due to basically no possibility of hacking privacy issues of social networking sites the advent of the web has caused social profiling and is growing concern for internet privacy web is the system that facilitates participatory information sharing and collaboration on the internet in social networking media websites like facebook instagram twitter and myspace these social networking sites have seen boom in their popularity starting from the late through these websites many people are giving their personal information out on the internet it has been topic of discussion of who is held accountable for the collection and distribution of personal information some will say that it is the fault of the social networks because they are the ones who are storing the vast amounts of information and data but others claim that it is the users who are responsible for the issue because it is the users themselves that provide the information in the first place this relates to the ever present issue of how society regards social media sites there is growing number of people that are discovering the risks of putting their personal information online and trusting website to keep it private once information is online it is no longer completely private it is an increasing risk because younger people are having easier internet access than ever before therefore they put themselves in position where it is all too easy for them to upload information but they may not have the caution to consider how difficult it can be to take that information down once it is out in the open this is becoming bigger issue now that so much of society interacts online which was not the case fifteen years ago in addition because of the quickly evolving digital media arena peoples interpretation of privacy is evolving as well and it is important to consider that when interacting online new forms of social networking and digital media such as instagram and snapchat may call for new guidelines regarding privacy what makes this difficult is the wide range of opinions surrounding the topic so it is left mainly up to our judgement to respect other people online privacy in some circumstances sometimes it may be necessary to take extra precautions in situations where somebody else may have tighter view on privacy ethics no matter the situation it is beneficial to know about the potential consequences and issues that can come from careless activity on social networks internet service providers internet users obtain internet access through an internet service provider isp all data transmitted to and from users must pass through the isp thus an isp has the potential to observe users activities on the internet however isps are usually prohibited from participating in such activities due to legal ethical business or technical reasons normally isps do collect at least some information about the consumers using their services from privacy standpoint isps would ideally collect only as much information as they require in order to provide internet connectivity ip address billing information if applicable etc which information an isp collects what it does with that information and whether it informs its consumers pose significant privacy issues beyond the usage of collected information typical of third parties isps sometimes state that they will make their information available to government authorities upon request in the us and other countries such request does not necessarily require warrant an isp cannot know the contents of properly encrypted data passing between its consumers and the internet for encrypting web traffic https has become the most popular and best supported standard even if users encrypt the data the isp still knows the ip addresses of the sender and of the recipient however see the ip addresses section for workarounds an anonymizer such as the anonymous network or tor can be used for accessing web services without them knowing one ip address and without one isp knowing what the services are that one accesses additional software has been developed that may provide more secure and anonymous alternatives to other applications for example bitmessage can be used as an alternative for email and cryptocat as an alternative for online chat on the other hand in addition to end to end encryption software there are web services such as qlink which provide privacy through novel security protocol which does not require installing any software while signing up for internet services each computer contains unique ip internet protocol address this particular address will not give away private or personal information however weak link could potentially reveal information from one isp general concerns regarding internet user privacy have become enough of concern for un agency to issue report on the dangers of identity fraud in the council of europe held its first annual data protection day on january which has since evolved into the annual data privacy day mobile usa doesn store any information on web browsing verizon wireless keeps record of the websites subscriber visits for up to year virgin mobile keeps text messages for three months verizon keeps text messages for three to five days none of the other carriers keep texts of messages at all but they keep record of who texted who for over year at mobility keeps for five to seven years record of who text messages who and the date and time but not the content of the messages virgin mobile keeps that data for two to three months html html is the latest version of hypertext markup language specification html defines how user agents such as web browsers are to present websites based upon their underlying code this new web standard changes the way that users are affected by the internet and their privacy on the internet html expands the number of methods given to website to store information locally on client as well as the amount of data that can be stored as such privacy risks are increased for instance merely erasing cookies may not be enough to remove potential tracking methods since data could be mirrored in web storage another means of keeping information in user web browser there are so many sources of data storage that it is challenging for web browsers to present sensible privacy settings as the power of web standards increases so do potential misuses html also expands access to user media potentially granting access to computer microphone or webcam capability previously only possible through the use of plug ins like flash it is also possible to find user geographical location using the geolocation api with this expanded access comes increased potential for abuse as well as more vectors for attackers if malicious site was able to gain access to user media it could potentially use recordings to uncover sensitive information thought to be unexposed however the world wide web consortium responsible for many web standards feels that the increased capabilities of the web platform outweigh potential privacy concerns they state that by documenting new capabilities in an open standardization process rather than through closed source plug ins made by companies it is easier to spot flaws in specifications and cultivate expert advice besides elevating privacy concerns html also adds few tools to enhance user privacy mechanism is defined whereby user agents can share blacklists of domains that should not be allowed to access web storage content security policy is proposed standard whereby sites may assign privileges to different domains enforcing harsh limitations on javascript use to mitigate cross site scripting attacks html also adds html templating and standard html parser which replaces the various parsers of web browser vendors these new features formalize previously inconsistent implementations reducing the number of vulnerabilities though not eliminating them entirely big data big data is generally defined as the rapid accumulation and compiling of massive amounts of information that is being exchanged over digital communication systems the data is large often exceeding exabytes and cannot be handled by conventional computer processors and are instead stored on large server system databases this information is assessed by analytic scientists using software programs which paraphrase this information into multi layered user trends and demographics this information is collected from all around the internet such as by popular services like facebook google apple spotify or gps systems big data provides companies with the ability to infer detailed psycho demographic profiles of internet users even if they were not directly expressed or indicated by users inspect product availability and optimize prices for maximum profit while clearing inventory swiftly reconfigure risk portfolios in minutes and understand future opportunities to mitigate risk mine customer data for insight and create advertising strategies for customer acquisition and retention identify customers who matter the most create retail coupons based on proportional scale to how much the customer has spent to ensure higher redemption rate send tailored recommendations to mobile devices at just the right time while customers are in the right location to take advantage of offers analyze data from social media to detect new market trends and changes in demand use clickstream analysis and data mining to detect fraudulent behavior determine root causes of failures issues and defects by investigating user sessions network logs and machine sensors other potential internet privacy risks malware is term short for malicious software and is used to describe software to cause damage to single computer server or computer network whether that is through the use of virus trojan horse spyware etc spyware is piece of software that obtains information from user computer without that user consent web bug is an object embedded into web page or email and is usually invisible to the user of the website or reader of the email it allows checking to see if person has looked at particular website or read specific email message phishing is criminally fraudulent process of trying to obtain sensitive information such as user names passwords credit card or bank information phishing is an internet crime in which someone masquerades as trustworthy entity in some form of electronic communication pharming is hacker attempt to redirect traffic from legitimate website to completely different internet address pharming can be conducted by changing the hosts file on victim computer or by exploiting vulnerability on the dns server social engineering where people are manipulated or tricked into performing actions or divulging confidential information malicious proxy server or other anonymity services use of weak passwords that are short consist of all numbers all lowercase or all uppercase letters or that can be easily guessed such as single words common phrases person name pet name the name of place an address phone number social security number or birth date using the same login name and or password for multiple accounts where one compromised account leads to other accounts being compromised allowing unused or little used accounts where unauthorized use is likely to go unnoticed to remain active using out of date software that may contain vulnerabilities that have been fixed in newer more up to date versions webrtc is protocol which suffers from serious security flaw that compromises the privacy of vpn tunnels by allowing the true ip address of the user to be read it is enabled by default in major browsers such as firefox and google chrome reduction of risks to internet privacy inc magazine reports that the internet biggest corporations have hoarded internet users personal data and sold it for large financial profits the magazine reports on band of startup companies that are demanding privacy and aiming to overhaul the social media business such as wickr mobile messaging app described as using peer to peer encryption and giving the user the capacity to control what information is retained on the other end ansa an ephemeral chat application also described as employing peer to peer encryption and omlet an open mobile social network described as giving the user control over their data so that if user does not want their data saved they are able to delete it from the data repository noise society protection through information overflow according to nicklas lundblad another perspective on privacy protection is the assumption that the quickly growing amount of information produced will be beneficial the reasons for this are that the costs for the surveillance will raise and that there is more noise noise being understood as anything that interferes the process of receiver trying to extract private data from sender in this noise society the collective expectation of privacy will increase but the individual expectation of privacy will decrease in other words not everyone can be analyzed in detail but one individual can be also in order to stay unobserved it can hence be better to blend in with the others than trying to use for example encryption technologies and similar methods technologies for this can be called jante technologies after the law of jante which states that you are nobody special this view offers new challenges and perspectives for the privacy discussion public views while internet privacy is widely acknowledged as the top consideration in any online interaction as evinced by the public outcry over sopa cispa public understanding of online privacy policies is actually being negatively affected by the current trends regarding online privacy statements users have tendency to skim internet privacy policies for information regarding the distribution of personal information only and the more legalistic the policies appear the less likely users are to even read the information coupling this with the increasingly exhaustive license agreements companies require consumers to agree to before using their product consumers are reading less about their rights furthermore if the user has already done business with company or is previously familiar with product they have tendency to not read the privacy policies that the company has posted as internet companies become more established their policies may change but their clients will be less likely to inform themselves of the change this tendency is interesting because as consumers become more acquainted with the internet they are also more likely to be interested in online privacy finally consumers have been found to avoid reading the privacy policies if the policies are not in simple format and even perceive these policies to be irrelevant the less readily available terms and conditions are the less likely the public is to inform themselves of their rights regarding the service they are using concerns of internet privacy and real life implications while dealing with the issue of internet privacy one must first be concerned with not only the technological implications such as damaged property corrupted files and the like but also with the potential for implications on their real lives one such implication which is rather commonly viewed as being one of the most daunting fears risks of the internet is the potential for identity theft although it is typical belief that larger companies and enterprises are the usual focus of identity thefts rather than individuals recent reports seem to show trend opposing this belief specifically it was found in internet security threat report that roughly ninety three percent of gateway attacks were targeted at unprepared home users it should be noted that the term gateway attack was used to refer to attack which aimed not at stealing data immediately but rather at gaining access for future attacks but how one might ask is this still thriving given the increasing emphasis on internet security the simple but unfortunate solution according to symantec internet security threat report is that of the expanding underground economy with more than fifty percent of the supporting servers located in the united states this underground economy has become haven for internet thieves who use the system in order to sell stolen information these pieces of information can range from generic things such as user account or email to something as personal as bank account number and pin personal identification numbers while the processes these internet thieves use are abundant and unique one popular trap unsuspecting people fall into is that of online purchasing this is not to allude to the idea that every purchase one makes online will leave them susceptible to identity theft but rather that it increases the chances in fact in article titled consumer watch the popular online site pc world went as far as calling secure shopping myth though unlike the gateway attacks mentioned above these incidents of information being stolen through online purchases generally are more prevalent in medium to large sized commerce sites rather than smaller individualized sites this is assumed to be result of the larger consumer population and purchases which allow for more potential leeway with information ultimately however the potential for violation of one privacy is typically out of their hands after purchasing from an online tailer or store one of the most common forms in which hackers receive private information from online tailers actually comes from an attack placed upon the site servers responsible for maintaining information about previous transactions for as experts explain these tailers are not doing nearly enough to maintain or improve their security measures even those sites that clearly present privacy or security policy can be subject to hackers havoc as most policies only rely upon encryption technology which only apply to the actual transfer of customer data however with this being said most tailers have been making improvements going as far as covering some of the credit card fees if the information abuse can be tracked back to the site servers as one of the largest growing concerns american adults have of current internet privacy policies identity and credit theft remain constant figure in the debate surrounding privacy online study by the boston consulting group showed that participants of the study were most concerned about their privacy on the internet compared to any other media however it is important to recall that these issues are not the only prevalent concerns our society has though some may call it modern day version of mccarthyism another prevalent issue also remains members of our own society sending disconcerting emails to one another it is for this reason in that for one of the first times ever the public demonstrated an approval of government intervention in their private lives with the overall public anxiety regarding the constantly expanding trend of online crimes in roughly fifty four percent of americans polled showed general approval for the fbi monitoring those emails deemed suspicious thus it was born the idea for the fbi program carnivore which was going to be used as searching method allowing the fbi to hopefully home in on potential criminals unlike the overall approval of the fbi intervention carnivore was not met with as much of majority approval rather the public seemed to be divided with forty five percent siding in its favor forty five percent opposed to the idea for its ability to potentially interfere with ordinary citizen messages and ten percent claiming indifference while this may seem slightly tangent to the topic of internet privacy it is important to consider that at the time of this poll the general population approval on government actions was declining reaching thirty one percent versus the forty one percent it held decade prior this figure in collaboration with the majority approval of fbi intervention demonstrates an emerging emphasis on the issue of internet privacy in society and more importantly the potential implications it may hold on citizens lives laws and regulations global privacy policies google has long been attacked for their lack of privacy in the as well as abroad in however the tables began to turn peter fleischer google representative addressed the in france regarding privacy issues and expressed that the current international privacy policies were not adequately protecting consumers instead of continuing to enforce broken and ineffective internet privacy laws the google representative proposed that the united nations establish global privacy policy that would efficiently protect consumers privacy while causing the least possible amount of negative impact on web browsers such as google at that time google was under investigation by the european union for violating the global privacy policies that were already in place the greatest issue related to internet privacy internationally is that of data collection at this point in time the and the european union had separate sets of privacy policies making it increasingly difficult for companies such as google to exist globally without violating such policies google is just one example of large company whose primary goal is to make money by serving their product web browsing to consumers consumers however are concerned with the quality of that product and their privacy online data collection by search engines allows internet businesses to track consumer online roadmap everything from the sites they visit to the purchases they make this poses problems globally to those who are web users around the world especially in world where there is no overarching privacy policy the general consensus of this issue regarding international privacy violations at the time of fleischer address is that since the internet is global the privacy policies should also be global and unified data protection regulation currently as of march the need for set of unified privacy policies has been met by the european union with proposed legislation the data protection regulation is proposed set of consistent regulations across the european union that will protect internet users from clandestine tracking and unauthorized personal data usage this regulation will further protect users privacy rights in two key ways clearly defining the term personal data and increasing punishments for those who violate users online privacy in article of the proposed legislation the definition of personal data is expanded significantly to include any information online that could be traced to an individual in articles and of the proposed legislation appropriate punishments are outlined for many possible violations of users privacy rights by controllers and effective enforcement of data protection is guaranteed the data protection regulation will also hold companies accountable for violations of the regulation by implementing unified legislation outlining specific repercussions for various types of violations based on severity the cdt the center for democracy technology has carefully evaluated this proposed legislation in detail and officially issued an analysis on march the center for democracy technology is nonprofit organization that advocates for internet freedom and online privacy through government public policy analyses such as this interpret the governmental propositions for internet users and promote democracy by allowing all the opportunity to agree or disagree with the proposition prior to its ruling this analysis is posted publicly on the internet in compliance with the mission of cdt and addresses each section of the data protection regulation and the potential pitfalls of each article the two major issues the cdt addresses in this analysis of the data protection regulation are the inflexible rules against profiling users based on their internet usage and the parental consent policy in regards to controlling the online information of children the european union seems to be following the lead of the obama administration recently implemented privacy bill and global internet privacy policies are on the horizon internet privacy in china one of the most popular topics of discussion in regards to internet privacy is china the main concern with privacy of internet users in china is the lack thereof china has well known policy of censorship when it comes to the spread of information through public media channels censorship has been prominent in mainland china since the communist party gained power in china over years ago with the development of the internet however privacy became more of problem for the government the chinese government has been accused of actively limiting and editing the information that flows into the country via various media the internet poses particular set of issues for this type of censorship especially when search engines are involved yahoo for example encountered problem after entering china in the mid chinese journalist who was also yahoo user sent private emails using the yahoo server regarding the chinese government the chinese staff of yahoo intercepted these emails and sent the journalist reportedly bad impression of the country to the chinese government which in turn sentenced the journalist to ten years in prison these types of occurrences have been reported numerous times and have been criticised by foreign entities such as the creators of the tor anonymity network which was designed to circumvent network surveillance in multiple countries user privacy in china is not as cut and dry as it is in other parts of the world china reportedly has much more invasive policy when internet activity involves the chinese government for this reason search engines are under constant pressure to conform to chinese rules and regulations on censorship while still attempting to keep their integrity therefore most search engines operate differently in china than in the other countries such as the us or britain if they operate in china at all there are two types of intrusions that occur in china regarding the internet the alleged intrusion of the company providing users with internet service and the alleged intrusion of the chinese government the intrusion allegations made against companies providing users with internet service are based upon reports that companies such as yahoo in the previous example are using their access to the internet users private information to track and monitor users internet activity the claims made against the chinese government lies in the fact that the government is forcing internet based companies to track users private online data without the user knowing that they are being monitored both alleged intrusions are relatively harsh and possibly force foreign internet service providers to decide if they value the chinese market over internet privacy internet privacy in sweden sweden is often considered to be at the forefront of internet use and internet regulations they are constantly innovating the way that the internet is used and how it impacts their people in sweden received web index score of score that measures how the internet significantly influences political social and economic impact placing them first among other nations sweden received this score while in the process of exceeding new mandatory implementations from the european union sweden placed more restrictive guidelines on the directive on intellectual property rights enforcement ipred and passed the fra law in that allowed for the legal sanctioning of surveillance of internet traffic by state authorities the fra has history of intercepting radio signals and has stood as the main intelligence agency in sweden since sweden is an interesting topic when discussing laws and regulations of internet privacy because of mixture of their government strong push towards implementing policy and their citizens continued perception of free and neutral internet both of the previously mentioned additions created controversy by critics but they did not change the public perception even though the new fra law was brought in front of the european court of human rights for human rights violations the law was established by the national defense radio establishment forsvarets radio anstalt fra to eliminate outside threats however the law also allowed for authorities to monitor all cross border communication without warrant sweden recent emergence into internet dominance may be explained by their recent climb in users only of all swedes were connected to the internet in but at last count in had broadband access this was due in large part once again to the active swedish government introducing regulatory provisions to promote competition among internet service providers these regulations helped grow web infrastructure and forced prices below the european average to add to the intrigue around sweden laws and regulations one must also mention how copyright laws evolved in sweden sweden was the birthplace of the pirate bay an infamous file sharing web site file sharing has been illegal in sweden since it was developed however there was never any real fear of being persecuted for the crime until when swedish parliament was the first in the european union to pass the intellectual property rights directive this directive persuaded internet service providers to announce the identity of suspected violators final piece of legislation worth mentioning when discussing sweden regulations is the infamous centralized block list the list is generated by authorities and was originally crafted to eliminate sites hosting child pornography however there is no legal way to appeal site that ends up on the list and as result many non child pornography sites have been black listed it is important to consider that sweden government enjoys high level of trust from their citizens without this trust many of these regulations would not be possible and thus many of these regulations may only be feasible in the swedish context legal threats used by government agencies are array of technologies designed to track and gather internet users information are the topic of much debate between privacy advocates civil liberties advocate and those who believe such measures are necessary for law enforcement to keep pace with rapidly changing communications technology specific examples following decision by the european union council of ministers in brussels in january the uk home office adopted plan to allow police to access the contents of individuals computers without warrant the process called remote searching allows one party at remote location to examine another hard drive and internet traffic including email browsing history and websites visited police across the eu are now permitted to request that the british police conduct remote search on their behalf the search can be granted and the material gleaned turned over and used as evidence on the basis of senior officer believing it necessary to prevent serious crime opposition mps and civil liberties advocates are concerned about this move toward widening surveillance and its possible impact on personal privacy says shami chakrabarti director of the human rights group liberty the public will want this to be controlled by new legislation and judicial authorisation without those safeguards it devastating blow to any notion of personal privacy the fbi magic lantern software program was the topic of much debate when it was publicized in november magic lantern is trojan horse program that logs users keystrokes rendering encryption useless to those infected see also anonymity anonymous blogging anonymous anonymous post anonymous remailer anonymous web browsing internet censorship internet censorship circumvention internet vigilantism privacy enhancing technologies freenet friend to friend gnunet metadata removal tool privacy software prism privacy law right to be forgotten privacy in australian law australian privacy act canadian privacy law canadian privacy act personal information protection and electronic documents act european union data protection directive privacy in english law uk data protection act privacy laws in russia privacy laws of the united states bank secrecy communications assistance for law enforcement act calea privacy act of surveillance computer and network surveillance mass surveillance mass surveillance disclosures mass surveillance in the united kingdom mass surveillance in the united states unauthorized access in online social networks references further reading lohr steve how privacy can vanish online bit at time the new york times wednesday march gazaleh mark online trust and perceived utility for consumers of web privacy statements overview wbs june federal trade commission protecting consumer privacy in an era of rapid change proposed framework for businesses and policymakers december topolsky february tempted by cool apps users should see apple privacy issues as wake up call washington post prism proof security considerations internet draft phillip hallam baker internet engineering task force ietf september external links electronic frontier foundation an organization devoted to privacy and intellectual freedom advocacy expectation of privacy for company email not deemed objectively reasonable bourke nissan internet privacy the views of the ftc the fcc and ntia joint hearing before the subcommittee on commerce manufacturing and trade and the subcommittee on communications and technology of the committee on energy and commerce house of representatives one hundred twelfth congress first session july", "Pattern recognition": "pattern recognition is branch of machine learning that focuses on the recognition of patterns and regularities in data although it is in some cases considered to be nearly synonymous with machine learning pattern recognition systems are in many cases trained from labeled training data supervised learning but when no labeled data are available other algorithms can be used to discover previously unknown patterns unsupervised learning the terms pattern recognition machine learning data mining and knowledge discovery in databases kdd are hard to separate as they largely overlap in their scope machine learning is the common term for supervised learning methods and originates from artificial intelligence whereas kdd and data mining have larger focus on unsupervised methods and stronger connection to business use pattern recognition has its origins in engineering and the term is popular in the context of computer vision leading computer vision conference is named conference on computer vision and pattern recognition in pattern recognition there may be higher interest to formalize explain and visualize the pattern while machine learning traditionally focuses on maximizing the recognition rates yet all of these domains have evolved substantially from their roots in artificial intelligence engineering and statistics and they ve become increasingly similar by integrating developments and ideas from each other in machine learning pattern recognition is the assignment of label to given input value in statistics discriminant analysis was introduced for this same purpose in an example of pattern recognition is classification which attempts to assign each input value to one of given set of classes for example determine whether given email is spam or non spam however pattern recognition is more general problem that encompasses other types of output as well other examples are regression which assigns real valued output to each input sequence labeling which assigns class to each member of sequence of values for example part of speech tagging which assigns part of speech to each word in an input sentence and parsing which assigns parse tree to an input sentence describing the syntactic structure of the sentence pattern recognition algorithms generally aim to provide reasonable answer for all possible inputs and to perform most likely matching of the inputs taking into account their statistical variation this is opposed to pattern matching algorithms which look for exact matches in the input with pre existing patterns common example of pattern matching algorithm is regular expression matching which looks for patterns of given sort in textual data and is included in the search capabilities of many text editors and word processors in contrast to pattern recognition pattern matching is generally not considered type of machine learning although pattern matching algorithms especially with fairly general carefully tailored patterns can sometimes succeed in providing similar quality output of the sort provided by pattern recognition algorithms overview pattern recognition is generally categorized according to the type of learning procedure used to generate the output value supervised learning assumes that set of training data the training set has been provided consisting of set of instances that have been properly labeled by hand with the correct output learning procedure then generates model that attempts to meet two sometimes conflicting objectives perform as well as possible on the training data and generalize as well as possible to new data usually this means being as simple as possible for some technical definition of simple in accordance with occam razor discussed below unsupervised learning on the other hand assumes training data that has not been hand labeled and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances combination of the two that has recently been explored is semi supervised learning which uses combination of labeled and unlabeled data typically small set of labeled data combined with large amount of unlabeled data note that in cases of unsupervised learning there may be no training data at all to speak of in other words the data to be labeled is the training data note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output for example the unsupervised equivalent of classification is normally known as clustering based on the common perception of the task as involving no training data to speak of and of grouping the input data into clusters based on some inherent similarity measure the distance between instances considered as vectors in multi dimensional vector space rather than assigning each input instance into one of set of pre defined classes note also that in some fields the terminology is different for example in community ecology the term classification is used to refer to what is commonly known as clustering the piece of input data for which an output value is generated is formally termed an instance the instance is formally described by vector of features which together constitute description of all known characteristics of the instance these feature vectors can be seen as defining points in an appropriate space and methods for manipulating vectors in vector spaces can be correspondingly applied to them such as computing the dot product or the angle between two vectors typically features are either categorical also known as nominal consisting of one of set of unordered items such as gender of male or female or blood type of ab or ordinal consisting of one of set of ordered items large medium or small integer valued count of the number of occurrences of particular word in an email or real valued measurement of blood pressure often categorical and ordinal data are grouped together likewise for integer valued and real valued data furthermore many algorithms work only in terms of categorical data and require that real valued or integer valued data be discretized into groups less than between and or greater than probabilistic classifiers many common pattern recognition algorithms are probabilistic in nature in that they use statistical inference to find the best label for given instance unlike other algorithms which simply output best label often probabilistic algorithms also output probability of the instance being described by the given label in addition many probabilistic algorithms output list of the best labels with associated probabilities for some value of instead of simply single best label when the number of possible labels is fairly small in the case of classification may be set so that the probability of all possible labels is output probabilistic algorithms have many advantages over non probabilistic algorithms they output confidence value associated with their choice note that some other algorithms may also output confidence values but in general only for probabilistic algorithms is this value mathematically grounded in probability theory non probabilistic confidence values can in general not be given any specific meaning and only used to compare against other confidence values output by the same algorithm correspondingly they can abstain when the confidence of choosing any particular output is too low because of the probabilities output probabilistic pattern recognition algorithms can be more effectively incorporated into larger machine learning tasks in way that partially or completely avoids the problem of error propagation how many feature variables are important feature selection algorithms attempt to directly prune out redundant or irrelevant features general introduction to feature selection which summarizes approaches and challenges has been given the complexity of feature selection is because of its non monotonous character an optimization problem where given total of features the powerset consisting of all subsets of features need to be explored the branch and bound algorithm does reduce this complexity but is intractable for medium to large values of the number of available features for large scale comparison of feature selection algorithms see techniques to transform the raw feature vectors feature extraction are sometimes used prior to application of the pattern matching algorithm for example feature extraction algorithms attempt to reduce large dimensionality feature vector into smaller dimensionality vector that is easier to work with and encodes less redundancy using mathematical techniques such as principal components analysis pca the distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of different sort than the original features and may not easily be interpretable while the features left after feature selection are simply subset of the original features problem statement supervised version formally the problem of supervised pattern recognition can be stated as follows given an unknown function the ground truth that maps input instances to output labels along with training data assumed to represent accurate examples of the mapping produce function that approximates as closely as possible the correct mapping for example if the problem is filtering spam then is some representation of an email and is either spam or non spam in order for this to be well defined problem approximates as closely as possible needs to be defined rigorously in decision theory this is defined by specifying loss function that assigns specific value to loss resulting from producing an incorrect label the goal then is to minimize the expected loss with the expectation taken over the probability distribution of in practice neither the distribution of nor the ground truth function are known exactly but can be computed only empirically by collecting large number of samples of and hand labeling them using the correct value of time consuming process which is typically the limiting factor in the amount of data of this sort that can be collected the particular loss function depends on the type of label being predicted for example in the case of classification the simple zero one loss function is often sufficient this corresponds simply to assigning loss of to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data counting up the fraction of instances that the learned function labels wrongly which is equivalent to maximizing the number of correctly classified instances the goal of the learning procedure is then to minimize the error rate maximize the correctness on typical test set for probabilistic pattern recognizer the problem is instead to estimate the probability of each possible output label given particular input instance to estimate function of the form where the feature vector input is and the function is typically parameterized by some parameters in discriminative approach to the problem is estimated directly in generative approach however the inverse probability is instead estimated and combined with the prior probability using bayes rule as follows when the labels are continuously distributed in regression analysis the denominator involves integration rather than summation the value of is typically learned using maximum posteriori map estimation this finds the best value that simultaneously meets two conflicting objects to perform as well as possible on the training data smallest error rate and to find the simplest possible model essentially this combines maximum likelihood estimation with regularization procedure that favors simpler models over more complex models in bayesian context the regularization procedure can be viewed as placing prior probability on different values of mathematically where is the value used for in the subsequent evaluation procedure and the posterior probability of is given by in the bayesian approach to this problem instead of choosing single parameter vector the probability of given label for new instance is computed by integrating over all possible values of weighted according to the posterior probability frequentist or bayesian approach to pattern recognition the first pattern classifier the linear discriminant presented by fisher was developed in the frequentist tradition the frequentist approach entails that the model parameters are considered unknown but objective the parameters are then computed estimated from the collected data for the linear discriminant these parameters are precisely the mean vectors and the covariance matrix also the probability of each class is estimated from the collected dataset note that the usage of bayes rule in pattern classifier does not make the classification approach bayesian bayesian statistics has its origin in greek philosophy where distinction was already made between the priori and the posteriori knowledge later kant defined his distinction between what is priori known before observation and the empirical knowledge gained from observations in bayesian pattern classifier the class probabilities can be chosen by the user which are then priori moreover experience quantified as priori parameter values can be weighted with empirical observations using the beta conjugate prior and dirichlet distributions the bayesian approach facilitates seamless intermixing between expert knowledge in the form of subjective probabilities and objective observations probabilistic pattern classifiers can be used according to frequentist or bayesian approach uses the face was automatically detected by special software within medical science pattern recognition is the basis for computer aided diagnosis cad systems cad describes procedure that supports the doctor interpretations and findings pattern shape recognition technology srt in people counter system other typical applications of pattern recognition techniques are automatic speech recognition classification of text into several categories spam non spam email messages the automatic recognition of handwritten postal codes on postal envelopes automatic recognition of images of human faces or handwriting image extraction from medical forms the last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems optical character recognition is classic example of the application of pattern classifier see ocr example the method of signing one name was captured with stylus and overlay starting in the strokes speed relative min relative max acceleration and pressure is used to uniquely identify and confirm identity banks were first offered this technology but were content to collect from the fdic for any bank fraud and did not want to inconvenience customers artificial neural networks neural net classifiers and deep learning have many real world applications in image processing few examples identification and authentication license plate recognition fingerprint analysis and face detection verification medical diagnosis screening for cervical cancer papnet or breast tumors defence various navigation and guidance systems target recognition systems shape recognition technology etc for discussion of the aforementioned applications of neural networks in image processing see in psychology pattern recognition making sense of and identifying the objects we see is closely related to perception which explains how the sensory inputs we receive are made meaningful pattern recognition can be thought of in two different ways the first being template matching and the second being feature detection template is pattern used to produce items of the same proportions the template matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory if there is match the stimulus is identified feature detection models such as the pandemonium system for classifying letters selfridge suggest that the stimuli are broken down into their component parts for identification for example capital has three horizontal lines and one vertical line algorithms algorithms for pattern recognition depend on the type of label output on whether learning is supervised or unsupervised and on whether the algorithm is statistical or non statistical in nature statistical algorithms can further be categorized as generative or discriminative classification algorithms supervised algorithms predicting categorical labels parametric linear discriminant analysis quadratic discriminant analysis maximum entropy classifier aka logistic regression multinomial logistic regression note that logistic regression is an algorithm for classification despite its name the name comes from the fact that logistic regression uses an extension of linear regression model to model the probability of an input being in particular class nonparametric decision trees decision lists kernel estimation and nearest neighbor algorithms naive bayes classifier neural networks multi layer perceptrons perceptrons support vector machines gene expression programming clustering algorithms unsupervised algorithms predicting categorical labels categorical mixture models deep learning methods hierarchical clustering agglomerative or divisive means clustering correlation clustering kernel principal component analysis kernel pca ensemble learning algorithms supervised meta algorithms for combining multiple learning algorithms together boosting meta algorithm bootstrap aggregating bagging ensemble averaging mixture of experts hierarchical mixture of experts general algorithms for predicting arbitrarily structured sets of labels bayesian networks markov random fields multilinear subspace learning algorithms predicting labels of data using tensor representations unsupervised multilinear principal component analysis mpca real valued sequence labeling algorithms predicting sequences of real valued labels supervised kalman filters particle filters regression algorithms predicting real valued labels supervised gaussian process regression kriging linear regression and extensions neural networks and deep learning methods unsupervised independent component analysis ica principal components analysis pca sequence labeling algorithms predicting sequences of categorical labels supervised conditional random fields crfs hidden markov models hmms maximum entropy markov models memms recurrent neural networks unsupervised hidden markov models hmms see also adaptive resonance theory cache language model compound term processing computer aided diagnosis data mining deep learning list of numerical analysis software list of numerical libraries machine learning multilinear subspace learning neocognitron perception perceptual learning predictive analytics prior knowledge for pattern recognition sequence mining template matching contextual image classification references further reading an introductory tutorial to classifiers introducing the basic terms with numeric example external links the international association for pattern recognition list of pattern recognition web sites journal of pattern recognition research pattern recognition info pattern recognition journal of the pattern recognition society international journal of pattern recognition and artificial intelligence international journal of applied pattern recognition open pattern recognition project intended to be an open source platform for sharing algorithms of pattern recognition", "Software engineering": "software engineer programming for the wikimedia foundation software engineering is the study and an application of engineering to the design development and maintenance of software typical formal definitions of software engineering are research design develop and test operating systems level software compilers and network distribution software for medical industrial military communications aerospace business scientific and general computing applications the systematic application of scientific and technological knowledge methods and experience to the design implementation testing and documentation of software the application of systematic disciplined quantifiable approach to the development operation and maintenance of software an engineering discipline that is concerned with all aspects of software production and the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines history margaret hamilton who adopted oettinger term software engineering standing with the data results from the simulation of the code her team designed for apollo when the first digital computers appeared in the early the instructions to make them operate were wired into the machine practitioners quickly realized that this design was not flexible and came up with the stored program architecture or von neumann architecture thus the division between hardware and software began with abstraction being used to deal with the complexity of computing programming languages started to appear in the and this was also another major step in abstraction major languages such as fortran algol and cobol were released in the late to deal with scientific algorithmic and business problems respectively dijkstra wrote his seminal paper go to statement considered harmful in and david parnas introduced the key concept of modularity and information hiding in to help programmers deal with the ever increasing complexity of software systems the term software engineering coined first by anthony oettinger and then used by margaret hamilton was used in as title for the world first conference on software engineering sponsored and facilitated by nato the conference was attended by international experts on software who agreed on defining best practices for software grounded in the application of engineering the result of the conference is report that defines how software should be developed software engineering foundations the original report is publicly available the discipline of software engineering was created to address poor quality of software get projects exceeding time and budget under control and ensure that software is built systematically rigorously measurably on time on budget and within specification engineering already addresses all these issues hence the same principles used in engineering can be applied to software the widespread lack of best practices for software at the time was perceived as software crisis barry boehm documented several key advances to the field in his book software engineering economics these include his constructive cost model cocomo which relates software development effort for program in man years to source lines of code sloc the book analyzes sixty three software projects and concludes the cost of fixing errors escalates as the project moves toward field use the book also asserts that the key driver of software cost is the capability of the software development team in the software engineering institute sei was established as federally funded research and development center headquartered on the campus of carnegie mellon university in pittsburgh pennsylvania united states watts humphrey founded the sei software process program aimed at understanding and managing the software engineering process his book managing the software process asserts that the software development process can and should be controlled measured and improved the process maturity levels introduced would become the capability maturity model integration for development cmmi dev which has defined how the us government evaluates the abilities of software development team modern generally accepted best practices for software engineering have been collected by the iso iec jtc sc subcommittee and published as the software engineering body of knowledge swebok subdisciplines software engineering can be divided into ten subdisciplines they are requirements engineering the elicitation analysis specification and validation of requirements for software software design the process of defining the architecture components interfaces and other characteristics of system or component it is also defined as the result of that process software construction the detailed creation of working meaningful software through combination of coding verification unit testing integration testing and debugging software testing an empirical technical investigation conducted to provide stakeholders with information about the quality of the product or service under test software maintenance the totality of activities required to provide cost effective support to software software configuration management the identification of the configuration of system at distinct points in time for the purpose of systematically controlling changes to the configuration and maintaining the integrity and traceability of the configuration throughout the system life cycle software engineering management the application of management activities planning coordinating measuring monitoring controlling and reporting to ensure that the development and maintenance of software is systematic disciplined and quantified software engineering process the definition implementation assessment measurement management change and improvement of the software life cycle process itself software engineering tools and methods the computer based tools that are intended to assist the software life cycle processes see computer aided software engineering and the methods which impose structure on the software engineering activity with the goal of making the activity systematic and ultimately more likely to be successful software quality management the degree to which set of inherent characteristics fulfills requirements software evolution the process of developing software initially then repeatedly updating it for various reasons education knowledge of computer programming is prerequisite to becoming software engineer in the ieee computer society produced the swebok which has been published as iso iec technical report describing the body of knowledge that they recommend to be mastered by graduate software engineer with four years of experience many software engineers enter the profession by obtaining university degree or training at vocational school one standard international curriculum for undergraduate software engineering degrees was defined by the ccse and updated in number of universities have software engineering degree programs there were campus programs online programs masters level programs doctorate level programs and certificate level programs in the united states for practitioners who wish to become proficient and recognized as professional software engineers the ieee offers two certifications that extend knowledge above level achieved by an academic degree certified software development associate and certified software development professional in addition to university education many companies sponsor internships for students wishing to pursue careers in information technology these internships can introduce the student to interesting real world tasks that typical software engineers encounter every day similar experience can be gained through military service in software engineering profession legal requirements for the licensing or certification of professional software engineers vary around the world in the uk the british computer society licenses software engineers and members of the society can also become chartered engineers ceng while in some areas of canada such as alberta british columbia ontario and quebec software engineers can hold the professional engineer eng designation and or the information systems professional designation in canada there is legal requirement to have eng when one wants to use the title engineer or practice software engineering the united states starting from offers an ncees professional engineer exam for software engineering thereby allowing software engineers to be licensed and recognized mandatory licensing is currently still largely debated and perceived as controversial in some parts of the us such as texas the use of the term engineer is regulated by law and reserved only for use by individuals who have professional engineer license the ieee informs the professional engineer license is not required unless the individual would work for public where health of others could be at risk if the engineer was not fully qualified to required standards by the particular state professional engineer licenses are specific to the state which has awarded them and have to be regularly retaken the ieee computer society and the acm the two main us based professional organizations of software engineering publish guides to the profession of software engineering the ieee guide to the software engineering body of knowledge version or swebok defines the field and describes the knowledge the ieee expects practicing software engineer to have the most current swebok is an updated version and was released in the ieee also promulgates software engineering code of ethics employment in the bureau of labor statistics counted software engineers holding jobs in the in the same time period there were some million practitioners employed in the in all other engineering disciplines combined due to its relative newness as field of study formal education in software engineering is often taught as part of computer science curriculum and many software engineers hold computer science degrees and have no engineering background whatsoever many software engineers work as employees or contractors software engineers work with businesses government agencies civilian or military and non profit organizations some software engineers work for themselves as freelancers some organizations have specialists to perform each of the tasks in the software development process other organizations require software engineers to do many or all of them in large projects people may specialize in only one role in small projects people may fill several or all roles at the same time specializations include in industry analysts architects developers testers technical support middleware analysts managers and in academia educators researchers most software engineers and programmers work hours week but about percent of software engineers and percent of programmers worked more than hours week in injuries in these occupations are rare however like other workers who spend long periods in front of computer terminal typing at keyboard engineers and programmers are susceptible to eyestrain back discomfort and hand and wrist problems such as carpal tunnel syndrome the field future looks bright according to money magazine and salary com which rated software engineer as the best job in the united states in in software engineering was again ranked as the best job in the united states this time by careercast com certification the software engineering institute offers certifications on specific topics like security process improvement and software architecture apple ibm microsoft and other companies also sponsor their own certification examinations many it certification programs are oriented toward specific technologies and managed by the vendors of these technologies these certification programs are tailored to the institutions that would employ people who use these technologies broader certification of general software engineering skills is available through various professional societies the ieee had certified over software professionals as certified software development professional csdp in they added an entry level certification known as the certified software development associate csda the acm had professional certification program in the early which was discontinued due to lack of interest the acm examined the possibility of professional certification of software engineers in the late but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering in the the british computer society has developed legally recognized professional certification called chartered it professional citp available to fully qualified members mbcs software engineers may be eligible for membership of the institution of engineering and technology and so qualify for chartered engineer status in canada the canadian information processing society has developed legally recognized professional certification called information systems professional isp in ontario canada software engineers who graduate from canadian engineering accreditation board ceab accredited program successfully complete peo professional engineers ontario professional practice examination ppe and have at least months of acceptable engineering experience are eligible to be licensed through the professional engineers ontario and can become professional engineers eng the peo does not recognize any online or distance education however and does not consider computer science programs to be equivalent to software engineering programs despite the tremendous overlap between the two this has sparked controversy and certification war it has also held the number of eng holders for the profession exceptionally low the vast majority of working professionals in the field hold degree in cs not se given the difficult certification path for holders of non se degrees most never bother to pursue the license impact of globalization the initial impact of outsourcing and the relatively lower cost of international human resources in developing third world countries led to massive migration of software development activities from corporations in north america and europe to india and later china russia and other developing countries this approach had some flaws mainly the distance timezone difference that prevented human interaction between clients and developers and the massive job transfer this had negative impact on many aspects of the software engineering profession for example some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing importing software products or services from other countries and of being displaced by foreign visa workers although statistics do not currently show threat to software engineering itself related career computer programming does appear to have been affected nevertheless the ability to smartly leverage offshore and near shore resources via the follow the sun workflow has improved the overall operational capability of many organizations when north americans are leaving work asians are just arriving to work when asians are leaving work europeans are arriving to work this provides continuous ability to have human oversight on business critical processes hours per day without paying overtime compensation or disrupting key human resource sleep patterns while global outsourcing has several advantages global and generally distributed development can run into serious difficulties resulting from the distance between developers this is due to the key elements of this type of distance which have been identified as geographical temporal cultural and communication which includes the use of different languages and dialects of english in different locations research has been carried out in the area of global software development over the last years and an extensive body of relevant work published which highlights the benefits and problems associated with the complex activity as with other aspects of software engineering research is ongoing in this and related areas related fields software engineering is direct sub field of engineering and has an overlap with computer science and management science it is also considered part of overall systems engineering controversy over definition typical formal definitions of software engineering are the application of systematic disciplined quantifiable approach to the development operation and maintenance of software an engineering discipline that is concerned with all aspects of software production the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines the term has been used less formally as the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis as the broad term for all aspects of the practice of computer programming as opposed to the theory of computer programming which is called computer science as the term embodying the advocacy of specific approach to computer programming one that urges that it be treated as an engineering discipline rather than an art or craft and advocates the codification of recommended practices criticism software engineering sees its practitioners as individuals who follow well defined engineering approaches to problem solving these approaches are specified in various software engineering books and research papers always with the connotations of predictability precision mitigated risk and professionalism this perspective has led to calls for licensing certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field software craftsmanship has been proposed by body of software developers as an alternative that emphasizes the coding skills and accountability of the software developers themselves without professionalism or any prescribed curriculum leading to ad hoc problem solving craftmanship without engineering lack of predictability precision missing risk mitigation methods are informal and poorly defined the software craftsmanship manifesto extends the agile software manifesto and draws metaphor between modern software development and the apprenticeship model of medieval europe software engineering extends engineering and draws on the engineering model engineering process engineering project management engineering requirements engineering design engineering construction and engineering validation the concept is so new that it is rarely understood and it is widely misinterpreted including in software engineering textbooks papers and among the communities of programmers and crafters one of the core issues in software engineering is that its approaches are not empirical enough because real world validation of approaches is usually absent or very limited and hence software engineering is often misinterpreted as feasible only in theoretical environment dijkstra who developed computer languages in the last century refuted the concepts of software engineering which was prevalent thirty years ago in the arguing that those terms were poor analogies for what he called the radical novelty of computer science see also bachelor of science in information technology bachelor of software engineering list of software engineering conferences list of software engineering publications software craftsmanship software engineering institute notes references further reading external links guide to the software engineering body of knowledge the open systems engineering and software development life cycle framework opensdlc org the integrated creative commons sdlc software engineering institute carnegie mellon learn software engineering software engineering society", "Database": "database is an organized collection of data it is the collection of schemes tables queries reports views and other objects the data is typically organized to model aspects of reality in way that supports processes requiring information such as modelling the availability of rooms in hotels in way that supports finding hotel with vacancies database management system dbms is computer software application that interacts with the user other applications and the database itself to capture and analyze data general purpose dbms is designed to allow the definition creation querying update and administration of databases well known dbmss include mysql postgresql microsoft sql server oracle sybase and ibm db database is not generally portable across different dbmss but different dbms can interoperate by using standards such as sql and odbc or jdbc to allow single application to work with more than one dbms database management systems are often classified according to the database model that they support the most popular database systems since the have all supported the relational model as represented by the sql language sometimes dbms is loosely referred to as database terminology and overview formally database refers to set of related data and the way it is organized access to this data is usually provided by database management system dbms consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database although restrictions may exist that limit access to particular data the dbms provides various functions that allow entry storage and retrieval of large quantities of information and provides ways to manage how that information is organized because of the close relationship between them the term database is often used casually to refer to both database and the dbms used to manipulate it outside the world of professional information technology the term database is often used to refer to any collection of related data such as spreadsheet or card index this article is concerned only with databases where the size and usage requirements necessitate use of database management system existing dbmss provide various functions that allow management of database and its data which can be classified into four main functional groups data definition creation modification and removal of definitions that define the organization of the data update insertion modification and deletion of the actual data retrieval providing information in form directly usable or for further processing by other applications the retrieved data may be made available in form basically the same as it is stored in the database or in new form obtained by altering or combining existing data from the database administration registering and monitoring users enforcing data security monitoring performance maintaining data integrity dealing with concurrency control and recovering information that has been corrupted by some event such as an unexpected system failure both database and its dbms conform to the principles of particular database model database system refers collectively to the database model database management system and database physically database servers are dedicated computers that hold the actual databases and run only the dbms and related software database servers are usually multiprocessor computers with generous memory and raid disk arrays used for stable storage raid is used for recovery of data if any of the disks fail hardware database accelerators connected to one or more servers via high speed channel are also used in large volume transaction processing environments dbmss are found at the heart of most database applications dbmss may be built around custom multitasking kernel with built in networking support but modern dbmss typically rely on standard operating system to provide these functions since dbmss comprise significant economical market computer and storage vendors often take into account dbms requirements in their own development plans databases and dbmss can be categorized according to the database model that they support such as relational or xml the type of computer they run on from server cluster to mobile phone the query language used to access the database such as sql or xquery and their internal engineering which affects performance scalability resilience and security applications databases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers see enterprise software databases are used to hold administrative information and more specialized data such as engineering data or economic models examples of database applications include computerized library systems flight reservation systems and computerized parts inventory systems some application areas of dbms banking for customer information accounts and loans and banking transactions airlines for reservations and schedule information airlines were among the first to use databases in geographically distributed manner terminals situated around the world accessed the central database system through phone lines and other data networks universities for student information course registrations and grades credit card transactions for purchases on credit cards and generation of monthly statements for keeping records of calls made generating monthly bills maintaining balances on prepaid calling cards and storing information about the communication networks finance for storing information about holdings sales and purchases of financial instruments such as stocks and bonds sales for customer product and purchase information manufacturing for management of supply chain and for tracking production of items in factories inventories of items in warehouses stores and orders for items human resources for information about employees salaries payroll taxes and benefits and for generation of paychecks general purpose and special purpose dbmss dbms has evolved into complex software system and its development typically requires thousands of person years of development effort some general purpose dbmss such as adabas oracle and db have been undergoing upgrades since the general purpose dbmss aim to meet the needs of as many applications as possible which adds to the complexity however the fact that their development cost can be spread over large number of users means that they are often the most cost effective approach however general purpose dbms is not always the optimal solution in some cases general purpose dbms may introduce unnecessary overhead therefore there are many examples of systems that use special purpose databases common example is an email system that performs many of the functions of general purpose dbms such as the insertion and deletion of messages composed of various items of data or associating messages with particular email address but these functions are limited to what is required to handle email and don provide the user with the all of the functionality that would be available using general purpose dbms many other databases have application software that accesses the database on behalf of end users without exposing the dbms interface directly application programmers may use wire protocol directly or more likely through an application programming interface database designers and database administrators interact with the dbms through dedicated interfaces to build and maintain the applications databases and thus need some more knowledge and understanding about how dbmss operate and the dbmss external interfaces and tuning parameters history following the technology progress in the areas of processors computer memory computer storage and computer networks the sizes capabilities and performance of databases and their respective dbmss have grown in orders of magnitude the development of database technology can be divided into three eras based on data model or structure navigational sql relational and post relational the two main early navigational data models were the hierarchical model epitomized by ibm ims system and the codasyl model network model implemented in number of products such as idms the relational model first proposed in by edgar codd departed from this tradition by insisting that applications should search for data by content rather than by following links the relational model employs sets of ledger style tables each used for different type of entity only in the mid did computing hardware become powerful enough to allow the wide deployment of relational systems dbmss plus applications by the early however relational systems dominated in all large scale data processing applications and they remain dominant ibm db oracle mysql and microsoft sql server are the top dbms the dominant database language standardised sql for the relational model has influenced database languages for other data models object databases were developed in the to overcome the inconvenience of object relational impedance mismatch which led to the coining of the term post relational and also the development of hybrid object relational databases the next generation of post relational databases in the late became known as nosql databases introducing fast key value stores and document oriented databases competing next generation known as newsql databases attempted new implementations that retained the relational sql model while aiming to match the high performance of nosql compared to commercially available relational dbmss navigational dbms basic structure of navigational codasyl database model the introduction of the term database coincided with the availability of direct access storage disks and drums from the mid onwards the term represented contrast with the tape based systems of the past allowing shared interactive use rather than daily batch processing the oxford english dictionary cites report by the system development corporation of california as the first to use the term data base in specific technical sense as computers grew in speed and capability number of general purpose database systems emerged by the mid number of such systems had come into commercial use interest in standard began to grow and charles bachman author of one such product the integrated data store ids founded the database task group within codasyl the group responsible for the creation and standardization of cobol in the database task group delivered their standard which generally became known as the codasyl approach and soon number of commercial products based on this approach entered the market the codasyl approach relied on the manual navigation of linked data set which was formed into large network applications could find records by one of three methods use of primary key known as calc key typically implemented by hashing navigating relationships called sets from one record to another scanning all the records in sequential order later systems added trees to provide alternate access paths many codasyl databases also added very straightforward query language however in the final tally codasyl was very complex and required significant training and effort to produce useful applications ibm also had their own dbms in known as information management system ims ims was development of software written for the apollo program on the system ims was generally similar in concept to codasyl but used strict hierarchy for its model of data navigation instead of codasyl network model both concepts later became known as navigational databases due to the way data was accessed and bachman turing award presentation was the programmer as navigator ims is classified as hierarchical database idms and cincom systems total database are classified as network databases ims remains in use relational dbms edgar codd worked at ibm in san jose california in one of their offshoot offices that was primarily involved in the development of hard disk systems he was unhappy with the navigational model of the codasyl approach notably the lack of search facility in he wrote number of papers that outlined new approach to database construction that eventually culminated in the groundbreaking relational model of data for large shared data banks in this paper he described new system for storing and working with large databases instead of records being stored in some sort of linked list of free form records as in codasyl codd idea was to use table of fixed length records with each table used for different type of entity linked list system would be very inefficient when storing sparse databases where some of the data for any one record could be left empty the relational model solved this by splitting the data into series of normalized tables or relations with optional elements being moved out of the main table to where they would take up room only if needed data may be freely inserted deleted and edited in these tables with the dbms doing whatever maintenance needed to present table view to the application user in the relational model records are linked using virtual keys not stored in the database but defined as needed between the data contained in the records the relational model also allowed the content of the database to evolve without constant rewriting of links and pointers the relational part comes from entities referencing other entities in what is known as one to many relationship like traditional hierarchical model and many to many relationship like navigational network model thus relational model can express both hierarchical and navigational models as well as its native tabular model allowing for pure or combined modeling in terms of these three models as the application requires for instance common use of database system is to track information about users their name login information various addresses and phone numbers in the navigational approach all of these data would be placed in single record and unused items would simply not be placed in the database in the relational approach the data would be normalized into user table an address table and phone number table for instance records would be created in these optional tables only if the address or phone numbers were actually provided linking the information back together is the key to this system in the relational model some bit of information was used as key uniquely defining particular record when information was being collected about user information stored in the optional tables would be found by searching for this key for instance if the login name of user is unique addresses and phone numbers for that user would be recorded with the login name as its key this simple re linking of related data back into single collection is something that traditional computer languages are not designed for just as the navigational approach would require programs to loop in order to collect records the relational approach would require loops to collect information about any one record codd solution to the necessary looping was set oriented language suggestion that would later spawn the ubiquitous sql using branch of mathematics known as tuple calculus he demonstrated that such system could support all the operations of normal databases inserting updating etc as well as providing simple system for finding and returning sets of data in single operation codd paper was picked up by two people at berkeley eugene wong and michael stonebraker they started project known as ingres using funding that had already been allocated for geographical database project and student programmers to produce code beginning in ingres delivered its first test products which were generally ready for widespread use in ingres was similar to system in number of ways including the use of language for data access known as quel over time ingres moved to the emerging sql standard ibm itself did one test implementation of the relational model prtv and production one business system both now discontinued honeywell wrote mrds for multics and now there are two new implementations alphora dataphor and rel most other dbms implementations usually called relational are actually sql dbmss in the university of michigan began development of the micro information management system based on childs set theoretic data model micro was used to manage very large data sets by the us department of labor the environmental protection agency and researchers from the university of alberta the university of michigan and wayne state university it ran on ibm mainframe computers using the michigan terminal system the system remained in production until integrated approach in the and attempts were made to build database systems with integrated hardware and software the underlying philosophy was that such integration would provide higher performance at lower cost examples were ibm system the early offering of teradata and the britton lee inc database machine another approach to hardware support for database management was icl cafs accelerator hardware disk controller with programmable search capabilities in the long term these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general purpose computers thus most database systems nowadays are software systems running on general purpose hardware using general purpose computer data storage however this idea is still pursued for certain applications by some companies like netezza and oracle exadata late sql dbms ibm started working on prototype system loosely based on codd concepts as system in the early the first version was ready in and work then started on multi table systems in which the data could be split so that all of the data for record some of which is optional did not have to be stored in single large chunk subsequent multi user versions were tested by customers in and by which time standardized query language sql had been added codd ideas were establishing themselves as both workable and superior to codasyl pushing ibm to develop true production version of system known as sql ds and later database db larry ellison oracle started from different chain based on ibm papers on system and beat ibm to market when the first version was released in stonebraker went on to apply the lessons from ingres to develop new database postgres which is now known as postgresql postgresql is often used for global mission critical applications the org and info domain name registries use it as their primary data store as do many large companies and financial institutions in sweden codd paper was also read and mimer sql was developed from the mid at uppsala university in this project was consolidated into an independent enterprise in the early mimer introduced transaction handling for high robustness in applications an idea that was subsequently implemented on most other dbmss another data model the entity relationship model emerged in and gained popularity for database design as it emphasized more familiar description than the earlier relational model later on entity relationship constructs were retrofitted as data modeling construct for the relational model and the difference between the two have become irrelevant on the desktop the ushered in the age of desktop computing the new computers empowered their users with spreadsheets like lotus and database software like dbase the dbase product was lightweight and easy for any computer user to understand out of the box wayne ratliff the creator of dbase stated dbase was different from programs like basic fortran and cobol in that lot of the dirty work had already been done the data manipulation is done by dbase instead of by the user so the user can concentrate on what he is doing rather than having to mess with the dirty details of opening reading and closing files and managing space allocation dbase was one of the top selling software titles in the and early object oriented the along with rise in object oriented programming saw growth in how data in various databases were handled programmers and designers began to treat the data in their databases as objects that is to say that if person data were in database that person attributes such as their address phone number and age were now considered to belong to that person instead of being extraneous data this allows for relations between data to be relations to objects and their attributes and not to individual fields the term object relational impedance mismatch described the inconvenience of translating between programmed objects and database tables object databases and object relational databases attempt to solve this problem by providing an object oriented language sometimes as extensions to sql that programmers can use as alternative to purely relational sql on the programming side libraries known as object relational mappings orms attempt to solve the same problem nosql and newsql the next generation of post relational databases in the became known as nosql databases including fast key value stores and document oriented databases xml databases are type of structured document oriented database that allows querying based on xml document attributes xml databases are mostly used in enterprise database management where xml is being used as the machine to machine data standard xml database management systems include commercial software marklogic and oracle berkeley db xml and free use software clusterpoint distributed xml json database all are enterprise software database platforms and support industry standard acid compliant transaction processing with strong database consistency characteristics and high level of database security nosql databases are often very fast do not require fixed table schemas avoid join operations by storing denormalized data and are designed to scale horizontally the most popular nosql systems include mongodb couchbase riak memcached redis couchdb hazelcast apache cassandra and hbase which are all open source software products in recent years there was high demand for massively distributed databases with high partition tolerance but according to the cap theorem it is impossible for distributed system to simultaneously provide consistency availability and partition tolerance guarantees distributed system can satisfy any two of these guarantees at the same time but not all three for that reason many nosql databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with reduced level of data consistency newsql is class of modern relational databases that aims to provide the same scalable performance of nosql systems for online transaction processing read write workloads while still using sql and maintaining the acid guarantees of traditional database system such databases include scalebase clustrix enterprisedb memsql nuodb and voltdb research database technology has been an active research topic since the both in academia and in the research and development groups of companies for example ibm research research activity includes theory and development of prototypes notable research topics have included models the atomic transaction concept and related concurrency control techniques query languages and query optimization methods raid and more the database research area has several dedicated academic journals for example acm transactions on database systems tods data and knowledge engineering dke and annual conferences acm sigmod acm pods vldb ieee icde examples one way to classify databases involves the type of their contents for example bibliographic document text statistical or multimedia objects another way is by their application area for example accounting music compositions movies banking manufacturing or insurance third way is by some technical aspect such as the database structure or interface type this section lists few of the adjectives used to characterize different kinds of databases an in memory database is database that primarily resides in main memory but is typically backed up by non volatile computer data storage main memory databases are faster than disk databases and so are often used where response time is critical such as in network equipment sap hana platform is very hot topic for in memory database by may hana was able to run on servers with tb main memory powered by ibm the co founder of the company claimed that the system was big enough to run the largest sap customers an active database includes an event driven architecture which can respond to conditions both inside and outside the database possible uses include security monitoring alerting statistics gathering and authorization many databases provide active database features in the form of database triggers cloud database relies on cloud technology both the database and most of its dbms reside remotely in the cloud while its applications are both developed by programmers and later maintained and utilized by application end users through web browser and open apis data warehouses archive data from operational databases and often from external sources such as market research firms the warehouse becomes the central source of data for use by managers and other end users who may not have access to operational data for example sales data might be aggregated to weekly totals and converted from internal product codes to use upcs so that they can be compared with acnielsen data some basic and essential components of data warehousing include extracting analyzing and mining data transforming loading and managing data so as to make them available for further use deductive database combines logic programming with relational database for example by using the datalog language distributed database is one in which both the data and the dbms span multiple computers document oriented database is designed for storing retrieving and managing document oriented or semi structured data information document oriented databases are one of the main categories of nosql databases an embedded database system is dbms which is tightly integrated with an application software that requires access to stored data in such way that the dbms is hidden from the application end users and requires little or no ongoing maintenance end user databases consist of data developed by individual end users examples of these are collections of documents spreadsheets presentations multimedia and other files several products exist to support such databases some of them are much simpler than full fledged dbmss with more elementary dbms functionality federated database system comprises several distinct databases each with its own dbms it is handled as single database by federated database management system fdbms which transparently integrates multiple autonomous dbmss possibly of different types in which case it would also be heterogeneous database system and provides them with an integrated conceptual view sometimes the term multi database is used as synonym to federated database though it may refer to less integrated without an fdbms and managed integrated schema group of databases that cooperate in single application in this case typically middleware is used for distribution which typically includes an atomic commit protocol acp the two phase commit protocol to allow distributed global transactions across the participating databases graph database is kind of nosql database that uses graph structures with nodes edges and properties to represent and store information general graph databases that can store any graph are distinct from specialized graph databases such as triplestores and network databases an array dbms is kind of nosql dbms that allows to model store and retrieve usually large multi dimensional arrays such as satellite images and climate simulation output in hypertext or hypermedia database any word or piece of text representing an object another piece of text an article picture or film can be hyperlinked to that object hypertext databases are particularly useful for organizing large amounts of disparate information for example they are useful for organizing online encyclopedias where users can conveniently jump around the text the world wide web is thus large distributed hypertext database knowledge base abbreviated kb kb or is special kind of database for knowledge management providing the means for the computerized collection organization and retrieval of knowledge also collection of data representing problems with their solutions and related experiences mobile database can be carried on or synchronized from mobile computing device operational databases store detailed data about the operations of an organization they typically process relatively high volumes of updates using transactions examples include customer databases that record contact credit and demographic information about business customers personnel databases that hold information such as salary benefits skills data about employees enterprise resource planning systems that record details about product components parts inventory and financial databases that keep track of the organization money accounting and financial dealings parallel database seeks to improve performance through parallelization for tasks such as loading data building indexes and evaluating queries the major parallel dbms architectures which are induced by the underlying hardware architecture are shared memory architecture where multiple processors share the main memory space as well as other data storage shared disk architecture where each processing unit typically consisting of multiple processors has its own main memory but all units share the other storage shared nothing architecture where each processing unit has its own main memory and other storage probabilistic databases employ fuzzy logic to draw inferences from imprecise data real time databases process transactions fast enough for the result to come back and be acted on right away spatial database can store the data with features the queries on such data include location based queries like where is the closest hotel in my area temporal database has built in time aspects for example temporal data model and temporal version of sql more specifically the temporal aspects usually include valid time and transaction time terminology oriented database builds upon an object oriented database often customized for specific field an unstructured data database is intended to store in manageable and protected way diverse objects that do not fit naturally and conveniently in common databases it may include email messages documents journals multimedia objects etc the name may be misleading since some objects can be highly structured however the entire possible object collection does not fit into predefined structured framework most established dbmss now support unstructured data in various ways and new dedicated dbmss are emerging design and modeling the first task of database designer is to produce conceptual data model that reflects the structure of the information to be held in the database common approach to this is to develop an entity relationship model often with the aid of drawing tools another popular approach is the unified modeling language successful data model will accurately reflect the possible state of the external world being modeled for example if people can have more than one phone number it will allow this information to be captured designing good conceptual data model requires good understanding of the application domain it typically involves asking deep questions about the things of interest to an organisation like can customer also be supplier or if product is sold with two different forms of packaging are those the same product or different products or if plane flies from new york to dubai via frankfurt is that one flight or two or maybe even three the answers to these questions establish definitions of the terminology used for entities customers products flights flight segments and their relationships and attributes producing the conceptual data model sometimes involves input from business processes or the analysis of workflow in the organization this can help to establish what information is needed in the database and what can be left out for example it can help when deciding whether the database needs to hold historic data as well as current data having produced conceptual data model that users are happy with the next stage is to translate this into schema that implements the relevant data structures within the database this process is often called logical database design and the output is logical data model expressed in the form of schema whereas the conceptual data model is in theory at least independent of the choice of database technology the logical data model will be expressed in terms of particular database model supported by the chosen dbms the terms data model and database model are often used interchangeably but in this article we use data model for the design of specific database and database model for the modelling notation used to express that design the most popular database model for general purpose databases is the relational model or more precisely the relational model as represented by the sql language the process of creating logical database design using this model uses methodical approach known as normalization the goal of normalization is to ensure that each elementary fact is only recorded in one place so that insertions updates and deletions automatically maintain consistency the final stage of database design is to make the decisions that affect performance scalability recovery security and the like this is often called physical database design key goal during this stage is data independence meaning that the decisions made for performance optimization purposes should be invisible to end users and applications physical design is driven mainly by performance requirements and requires good knowledge of the expected workload and access patterns and deep understanding of the features offered by the chosen dbms another aspect of physical database design is security it involves both defining access control to database objects as well as defining security levels and methods for the data itself models collage of five types of database models database model is type of data model that determines the logical structure of database and fundamentally determines in which manner data can be stored organized and manipulated the most popular example of database model is the relational model or the sql approximation of relational which uses table based format common logical data models for databases include navigational databases hierarchical database model network model graph database relational model entity relationship model enhanced entity relationship model object model document model entity attribute value model star schema an object relational database combines the two related structures physical data models include inverted index flat file other models include associative model model array model multivalue model specialized models are optimized for particular types of data xml database semantic model content store event store time series model external conceptual and internal views traditional view of data database management system provides three views of the database data the external level defines how each group of end users sees the organization of data in the database single database can have any number of views at the external level the conceptual level unifies the various external views into compatible global view it provides the synthesis of all the external views it is out of the scope of the various database end users and is rather of interest to database application developers and database administrators the internal level or physical level is the internal organization of data inside dbms see implementation section below it is concerned with cost performance scalability and other operational matters it deals with storage layout of the data using storage structures such as indexes to enhance performance occasionally it stores data of individual views materialized views computed from generic data if performance justification exists for such redundancy it balances all the external views performance requirements possibly conflicting in an attempt to optimize overall performance across all activities while there is typically only one conceptual or logical and physical or internal view of the data there can be any number of different external views this allows users to see database information in more business related way rather than from technical processing viewpoint for example financial department of company needs the payment details of all employees as part of the company expenses but does not need details about employees that are the interest of the human resources department thus different departments need different views of the company database the three level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model the idea is that changes made at certain level do not affect the view at higher level for example changes in the internal level do not affect application programs written using conceptual level interfaces which reduces the impact of making physical changes to improve performance the conceptual view provides level of indirection between internal and external on one hand it provides common view of the database independent of different external view structures and on the other hand it abstracts away details of how the data is stored or managed internal level in principle every level and even every external view can be presented by different data model in practice usually given dbms uses the same data model for both the external and the conceptual levels relational model the internal level which is hidden inside the dbms and depends on its implementation see implementation section below requires different level of detail and uses its own types of data structure types separating the external conceptual and internal levels was major feature of the relational database model implementations that dominate st century databases languages database languages are special purpose languages which do one or more of the following data definition language defines data types and the relationships among them data manipulation language performs tasks such as inserting updating or deleting data occurrences query language allows searching for information and computing derived information database languages are specific to particular data model notable examples include sql combines the roles of data definition data manipulation and query in single language it was one of the first commercial languages for the relational model although it departs in some respects from the relational model as described by codd for example the rows and columns of table can be ordered sql became standard of the american national standards institute ansi in and of the international organization for standardization iso in the standards have been regularly enhanced since and is supported with varying degrees of conformance by all mainstream commercial relational dbmss oql is an object model language standard from the object data management group it has influenced the design of some of the newer query languages like jdoql and ejb ql xquery is standard xml query language implemented by xml database systems such as marklogic and exist by relational databases with xml capability such as oracle and db and also by in memory xml processors such as saxon sql xml combines xquery with sql database language may also incorporate features like dbms specific configuration and storage engine management computations to modify query results like counting summing averaging sorting grouping and cross referencing constraint enforcement in an automotive database only allowing one engine type per car application programming interface version of the query language for programmer convenience performance security and availability because of the critical importance of database technology to the smooth running of an enterprise database systems include complex mechanisms to deliver the required performance security and availability and allow database administrators to control the use of these features storage database storage is the container of the physical materialization of database it comprises the internal physical level in the database architecture it also contains all the information needed metadata data about the data and internal data structures to reconstruct the conceptual level and external level from the internal level when needed putting data into permanent storage is generally the responsibility of the database engine storage engine though typically accessed by dbms through the underlying operating system and often utilizing the operating systems file systems as intermediates for storage layout storage properties and configuration setting are extremely important for the efficient operation of the dbms and thus are closely maintained by database administrators dbms while in operation always has its database residing in several types of storage memory and external storage the database data and the additional needed information possibly in very large amounts are coded into bits data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels but in ways that attempt to optimize the best possible these levels reconstruction when needed by users and programs as well as for computing additional types of needed information from the data when querying the database some dbmss support specifying which character encoding was used to store data so multiple encodings can be used in the same database various low level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice techniques such as indexing may be used to improve performance conventional storage is row oriented but there are also column oriented and correlation databases materialized views often storage redundancy is employed to increase performance common example is storing materialized views which consist of frequently needed external views or query results storing such views saves the expensive computing of them each time they are needed the downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data and the cost of storage redundancy replication occasionally database employs storage redundancy by database objects replication with one or more copies to increase data availability both to improve performance of simultaneous multiple end user accesses to same database object and to provide resiliency in case of partial failure of distributed database updates of replicated object need to be synchronized across the object copies in many cases the entire database is replicated security database security deals with all various aspects of protecting the database content its owners and its users it ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities person or computer program database access control deals with controlling who person or certain computer program is allowed to access what information in the database the information may comprise specific database objects record types specific records data structures certain computations over certain objects query types or specific queries or utilizing specific access paths to the former using specific indexes or other data structures to access information database access controls are set by special authorized by the database owner personnel that uses dedicated protected security dbms interfaces this may be managed directly on an individual basis or by the assignment of individuals and privileges to groups or in the most elaborate models through the assignment of individuals and groups to roles which are then granted entitlements data security prevents unauthorized users from viewing or updating the database using passwords users are allowed access to the entire database or subsets of it called subschemas for example an employee database can contain all the data about an individual employee but one group of users may be authorized to view only payroll data while others are allowed access to only work history and medical data if the dbms provides way to interactively enter and update the database as well as interrogate it this capability allows for managing personal databases data security in general deals with protecting specific chunks of data both physically from corruption or destruction or removal see physical security or the interpretation of them or parts of them to meaningful information by looking at the strings of bits that they comprise concluding specific valid credit card numbers see data encryption change and access logging records who accessed which attributes what was changed and when it was changed logging services allow for forensic database audit later by keeping record of access occurrences and changes sometimes application level code is used to record changes rather than leaving this to the database monitoring can be set up to attempt to detect security breaches transactions and concurrency database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from crash database transaction is unit of work typically encapsulating number of operations over database reading database object writing acquiring lock etc an abstraction supported in database and also other systems each transaction has well defined boundaries in terms of which program code executions are included in that transaction determined by the transaction programmer via special transaction commands the acronym acid describes some ideal properties of database transaction atomicity consistency isolation and durability migration see also section database migration in article data migration database built with one dbms is not portable to another dbms the other dbms cannot run it however in some situations it is desirable to move migrate database from one dbms to another the reasons are primarily economical different dbmss may have different total costs of ownership or tcos functional and operational different dbmss may have different capabilities the migration involves the database transformation from one dbms type to another the transformation should maintain if possible the database related application all related application programs intact thus the database conceptual and external architectural levels should be maintained in the transformation it may be desired that also some aspects of the architecture internal level are maintained complex or large database migration may be complicated and costly one time project by itself which should be factored into the decision to migrate this in spite of the fact that tools may exist to help migration between specific dbmss typically dbms vendor provides tools to help importing databases from other popular dbmss building maintaining and tuning after designing database for an application the next stage is building the database typically an appropriate general purpose dbms can be selected to be utilized for this purpose dbms provides the needed user interfaces to be utilized by database administrators to define the needed application data structures within the dbms respective data model other user interfaces are used to select needed dbms parameters like security related storage allocation parameters etc when the database is ready all its data structures and other needed components are defined it is typically populated with initial application data database initialization which is typically distinct project in many cases using specialized dbms interfaces that support bulk insertion before making it operational in some cases the database becomes operational while empty of application data and data is accumulated during its operation after the database is created initialised and populated it needs to be maintained various database parameters may need changing and the database may need to be tuned tuning for better performance application data structures may be changed or added new related application programs may be written to add to the application functionality etc backup and restore sometimes it is desired to bring database back to previous state for many reasons cases when the database is found corrupted due to software error or if it has been updated with erroneous data to achieve this backup operation is done occasionally or continuously where each desired database state the values of its data and their embedding in database data structures is kept within dedicated backup files many techniques exist to do this effectively when this state is needed when it is decided by database administrator to bring the database back to this state by specifying this state by desired point in time when the database was in this state these files are utilized to restore that state static analysis static analysis techniques for software verification can be applied also in the scenario of query languages in particular the abstract interpretation framework has been extended to the field of query languages for relational databases as way to support sound approximation techniques the semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data the abstraction of relational database system has many interesting applications in particular for security purposes such as fine grained access control watermarking etc other other dbms features might include database logs graphics component for producing graphs and charts especially in data warehouse system query optimizer performs query optimization on every query to choose for it the most efficient query plan partial order tree of operations to be executed to compute the query result may be specific to particular storage engine tools or hooks for database design application programming application program maintenance database performance analysis and monitoring database configuration monitoring dbms hardware configuration dbms and related database may span computers networks and storage units and related database mapping especially for distributed dbms storage allocation and database layout monitoring storage migration etc see also comparison of database tools comparison of object database management systems comparison of object relational database management systems comparison of relational database management systems data hierarchy data bank data store database theory database testing database centric architecture question focused dataset references further reading ling liu and tamer \u00f6zsu eds encyclopedia of database systems illus isbn beynon davies database systems rd edition palgrave houndmills basingstoke connolly thomas and carolyn begg database systems new york harlow gray and reuter transaction processing concepts and techniques st edition morgan kaufmann publishers kroenke david and david auer database concepts rd ed new york prentice raghu ramakrishnan and johannes gehrke database management systems abraham silberschatz henry korth sudarshan database system concepts discussion on database systems teorey lightstone and nadeau database modeling design logical design th edition morgan kaufmann press isbn external links db file extension informations about files with db extension", "Graphics": "graphics from greek graphikos something written autograph are visual images or designs on some surface such as wall canvas screen paper or stone to inform illustrate or entertain in contemporary usage it includes neeke pictorial representation of data as in computer aided design and manufacture in typesetting and the graphic arts and in educational and neeke recreational software images that are generated by computer are called computer graphics examples are photographs drawings line art graphs diagrams typography numbers symbols geometric designs maps engineering drawings or other images graphics often combine text illustration and color graphic design may consist of the deliberate selection creation or arrangement of typography alone as in brochure flyer poster web site or book without any other element clarity or effective communication may be the objective association with other cultural elements may be sought or merely the creation of distinctive style graphics can be functional or artistic the latter can be recorded version such as photograph or an interpretation by scientist to highlight essential features or an artist in which case the distinction with imaginary graphics may become blurred history the earliest graphics known to anthropologists studying prehistoric periods are cave paintings and markings on boulders bone ivory and antlers which were created during the upper palaeolithic period from or earlier many of these were found to record astronomical seasonal and chronological details some of the earliest graphics and drawings known to the modern world from almost years ago are that of engraved stone tablets and ceramic cylinder seals marking the beginning of the historic periods and the keeping of records for accounting and inventory purposes records from egypt predate these and papyrus was used by the egyptians as material on which to plan the building of pyramids they also used slabs of limestone and wood from bc the greeks played major role in geometry they used graphics to represent their mathematical theories such as the circle theorem and the pythagorean theorem in art graphics is often used to distinguish work in monotone and made up of lines as opposed to painting drawing drawing generally involves making marks on surface by applying pressure from tool or moving tool across surface in which tool is always used as if there were no tools it would be art graphical drawing is an instrumental guided drawing printmaking woodblock printing including images is first seen in china after paper was invented about in the west the main techniques have been woodcut engraving and etching but there are many others etching etching is an intaglio method of printmaking in which the image is incised into the surface of metal plate using an acid the acid eats the metal leaving behind roughened areas or if the surface exposed to the acid is very thin burning line into the plate the use of the process in printmaking is believed to have been invented by daniel hopfer of augsburg germany who decorated armour in this way etching is also used in the manufacturing of printed circuit boards and semiconductor devices line art line art is rather non specific term sometimes used for any image that consists of distinct straight and curved lines placed against usually plain background without gradations in shade darkness or hue color to represent two dimensional or three dimensional objects line art is usually monochromatic although lines may be of different colors illustration an illustration of character from story also an illustration of illustrations an illustration is visual representation such as drawing painting photograph or other work of art that stresses subject more than form the aim of an illustration is to elucidate or decorate story poem or piece of textual information such as newspaper article traditionally by providing visual representation of something described in the text the editorial cartoon also known as political cartoon is an illustration containing political or social message illustrations can be used to display wide range of subject matter and serve variety of functions such as giving faces to characters in story displaying number of examples of an item described in an academic textbook typology visualising step wise sets of instructions in technical manual communicating subtle thematic tone in narrative linking brands to the ideas of human expression individuality and creativity making reader laugh or smile for fun to make laugh funny graphs graph or chart is information graphic that represents tabular numeric data charts are often used to make it easier to understand large quantities of data and the relationships between different parts of the data diagrams diagram is simplified and structured visual representation of concepts ideas constructions relations statistical data etc used to visualize and clarify the topic symbols symbol in its basic sense is representation of concept or quantity an idea object concept quality etc in more psychological and philosophical terms all concepts are symbolic in nature and representations for these concepts are simply token artifacts that are allegorical to but do not directly codify symbolic meaning or symbolism maps map is simplified depiction of space navigational aid which highlights relations between objects within that space usually map is two dimensional geometrically accurate representation of three dimensional space one of the first modern maps was made by waldseem\u00fcller photography one difference between photography and other forms of graphics is that photographer in principle just records single moment in reality with seemingly no interpretation however photographer can choose the field of view and angle and may also use other techniques such as various lenses to distort the view or filters to change the colors in recent times digital photography has opened the way to an infinite number of fast but strong manipulations even in the early days of photography there was controversy over photographs of enacted scenes that were presented as real life especially in war photography where it can be very difficult to record the original events shifting the viewer eyes ever so slightly with simple pinpricks in the negative could have dramatic effect the choice of the field of view can have strong effect effectively censoring out other parts of the scene accomplished by cropping them out or simply not including them in the photograph this even touches on the philosophical question of what reality is the human brain processes information based on previous experience making us see what we want to see or what we were taught to see photography does the same although the photographer interprets the scene for their viewer engineering drawings image of part represented in first angle projection an engineering drawing is type of drawing and is technical in nature used to fully and clearly define requirements for engineered items it is usually created in accordance with standardized conventions for layout nomenclature interpretation appearance such as typefaces and line styles size etc computer graphics there are two types of computer graphics raster graphics where each pixel is separately defined as in digital photograph and vector graphics where mathematical formulas are used to draw lines and shapes which are then interpreted at the viewer end to produce the graphic using vectors results in infinitely sharp graphics and often smaller files but when complex like vectors take time to render and may have larger file sizes than raster equivalent in the first computer driven display was attached to mit whirlwind computer to generate simple pictures this was followed by mit tx and tx interactive computing which increased interest in computer graphics during the late in ivan sutherland invented sketchpad an innovative program that influenced alternative forms of interaction with computers in the mid large computer graphics research projects were begun at mit general motors bell labs and lockheed corporation douglas ross of mit developed an advanced compiler language for graphics programming coons also at mit and ferguson at boeing began work in sculptured surfaces gm developed their dac system and other companies such as douglas lockheed and mcdonnell also made significant developments in ray tracing was first described by arthur appel of the ibm research center yorktown heights during the late personal computers became more powerful capable of drawing both basic and complex shapes and designs in the artists and graphic designers began to see the personal computer particularly the commodore amiga and macintosh as serious design tool one that could save time and draw more accurately than other methods computer graphics became possible in the late with the powerful sgi computers which were later used to create some of the first fully computer generated short films at pixar the macintosh remains one of the most popular tools for computer graphics in graphic design studios and businesses modern computer systems dating from the and onwards often use graphical user interface gui to present data and information with symbols icons and pictures rather than text graphics are one of the five key elements of multimedia technology graphics became more popular in the in gaming multimedia and animation in quake one of the first fully games was released in toy story the first full length computer generated animation film was released in cinemas since then computer graphics have become more accurate and detailed due to more advanced computers and better modeling software applications such as maya studio max and cinema another use of computer graphics is screensavers originally intended to preventing the layout of much used guis from burning into the computer screen they have since evolved into true pieces of art their practical purpose obsolete modern screens are not susceptible to such burn in artifacts web graphics signature art used on web forums in the internet speeds increased and internet browsers capable of viewing images were released the first being mosaic websites began to use the gif format to display small graphics such as banners advertisements and navigation buttons on web pages modern web browsers can now display jpeg png and increasingly svg images in addition to gifs on web pages svg and to some extent vml support in some modern web browsers have made it possible to display vector graphics that are clear at any size plugins expand the web browser functions to display animated interactive and graphics contained within file formats such as swf and modern web graphics can be made with software such as adobe photoshop the gimp or corel paint shop pro users of microsoft windows have ms paint which many find to be lacking in features this is because ms paint is drawing package and not graphics package numerous platforms and websites have been created to cater to web graphics artists and to host their communities growing number of people use create internet forum signatures generally appearing after user post and other digital artwork such as photo manipulations and large graphics with computer games developers creating their own communities around their products many more websites are being developed to offer graphics for the fans and to enable them to show their appreciation of such games in their own gaming profiles uses graphics are visual elements often used to point readers and viewers to particular information they are also used to supplement text in an effort to aid readers in their understanding of particular concept or make the concept more clear or interesting popular magazines such as time wired and newsweek usually contain graphic material in abundance to attract readers unlike the majority of scholarly journals in computing they are used to create graphical interface for the user and graphics are one of the five key elements of multimedia technology graphics are among the primary ways of advertising the sale of goods or services business graphics are commonly used in business and economics to create financial charts and tables the term business graphics came into use in the late when personal computers became capable of drawing graphs and charts instead of using tabular format business graphics can be used to highlight changes over period of time advertising advertising is one of the most profitable uses of graphics artists often do advertising work or take advertising potential into account when creating art to increase the chances of selling the artwork most importantly graphics gives good look to artwork whenever it is applied graphics contribute to the general outlook of designed artwork this in turn lure interested members of the public to look at the work of art or purchasing it any graphical work especially advertisement or any work of art that is poorly design will not persuade the audience therefore for an advertisement to persuade and convince readers or viewers it must be well designed with needed graphical tools so as to bring profit to the designer or advertiser political the use of graphics for overtly political purposes cartoons graffiti poster art flag design etc is centuries old practice which thrives today in every part of the world the northern irish murals are one such example more recent example is shepard fairey presidential election barack obama hope poster it was first published on the web but soon found its way onto streets throughout the united states education graphics are heavily used in textbooks especially those concerning subjects such as geography science and mathematics in order to illustrate theories and concepts such as the human anatomy diagrams are also used to label photographs and pictures educational animation is an important emerging field of graphics animated graphics have obvious advantages over static graphics when explaining subject matter that changes over time the oxford illustrated dictionary uses graphics and technical illustrations to make reading material more interesting and easier to understand in an encyclopedia graphics are used to illustrate concepts and show examples of the particular topic being discussed in order for graphic to function effectively as an educational aid the learner must be able to interpret it successfully this interpretative capacity is one aspect of graphicacy film and animation computer graphics are often used in the majority of new feature films especially those with large budget films that heavily use computer graphics include the lord of the rings film trilogy the harry potter films spider man and war of the worlds graphics education the majority of schools colleges and universities around the world educate students on the subject of graphics and art the subject is taught in broad variety of ways each course teaching its own distinctive balance of craft skills and intellectual response to the client needs some graphics courses prioritize traditional craft skills drawing printmaking and typography over modern craft skills other courses may place an emphasis on teaching digital craft skills still other courses may downplay the crafts entirely concentrating on training students to generate novel intellectual responses that engage with the brief despite these apparent differences in training and curriculum the staff and students on any of these courses will generally consider themselves to be graphic designers the typical pedagogy of graphic design or graphic communication visual communication graphic arts or any number of synonymous course titles will be broadly based on the teaching models developed in the bauhaus school in germany or vkhutemas in russia the teaching model will tend to expose students to variety of craft skills currently everything from drawing to motion capture combined with an effort to engage the student with the world of visual culture famous graphic designers aldus manutius designed the first italic type style which is often used in desktop publishing and graphic design april greiman is known for her influential poster design paul rand is well known as design pioneer for designing many popular corporate logos including the logo for ibm next and ups william caslon during the mid th century designed many typefaces including itc founder caslon itc founder caslon ornaments caslon graphique itc caslon no caslon old face and big caslon examples of graphics image tulip jpg photograph image portrait_ jpg drawing see also semiotics editorial cartoon references \u0987\u09a8 external links free graphics download historical timeline of computer graphics and animation", "Machine learning": "machine learning is subfield of computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence machine learning explores the study and construction of algorithms that can learn from and make predictions on data such algorithms operate by building model from example inputs in order to make data driven predictions or decisions rather than following strictly static program instructions machine learning is closely related to and often overlaps with computational statistics discipline that also specializes in prediction making it has strong ties to mathematical optimization which delivers methods theory and application domains to the field machine learning is employed in range of computing tasks where designing and programming explicit algorithms is infeasible example applications include spam filtering optical character recognition ocr search engines and computer vision machine learning is sometimes conflated with data mining although that focuses more on exploratory data analysis machine learning and pattern recognition can be viewed as two facets of the same field when employed in industrial contexts machine learning methods may be referred to as predictive analytics or predictive modelling overview in arthur samuel defined machine learning as field of study that gives computers the ability to learn without being explicitly programmed tom mitchell provided widely quoted more formal definition computer program is said to learn from experience with respect to some class of tasks and performance measure if its performance at tasks in as measured by improves with experience this definition is notable for its defining machine learning in fundamentally operational rather than cognitive terms thus following alan turing proposal in his paper computing machinery and intelligence that the question can machines think be replaced with the question can machines do what we as thinking entities can do types of problems and tasks machine learning tasks are typically classified into three broad categories depending on the nature of the learning signal or feedback available to learning system these are supervised learning the computer is presented with example inputs and their desired outputs given by teacher and the goal is to learn general rule that maps inputs to outputs unsupervised learning no labels are given to the learning algorithm leaving it on its own to find structure in its input unsupervised learning can be goal in itself discovering hidden patterns in data or means towards an end reinforcement learning computer program interacts with dynamic environment in which it must perform certain goal such as driving vehicle without teacher explicitly telling it whether it has come close to its goal or not another example is learning to play game by playing against an opponent between supervised and unsupervised learning is semi supervised learning where the teacher gives an incomplete training signal training set with some often many of the target outputs missing transduction is special case of this principle where the entire set of problem instances is known at learning time except that part of the targets are missing support vector machine is classifier that divides its input space into two regions separated by linear boundary here it has learned to distinguish black and white circles among other categories of machine learning problems learning to learn learns its own inductive bias based on previous experience developmental learning elaborated for robot learning generates its own sequences also called curriculum of learning situations to cumulatively acquire repertoires of novel skills through autonomous self exploration and social interaction with human teachers and using guidance mechanisms such as active learning maturation motor synergies and imitation another categorization of machine learning tasks arises when one considers the desired output of machine learned system in classification inputs are divided into two or more classes and the learner must produce model that assigns unseen inputs to one or multi label classification or more of these classes this is typically tackled in supervised way spam filtering is an example of classification where the inputs are email or other messages and the classes are spam and not spam in regression also supervised problem the outputs are continuous rather than discrete in clustering set of inputs is to be divided into groups unlike in classification the groups are not known beforehand making this typically an unsupervised task density estimation finds the distribution of inputs in some space dimensionality reduction simplifies inputs by mapping them into lower dimensional space topic modeling is related problem where program is given list of human language documents and is tasked to find out which documents cover similar topics history and relationships to other fields as scientific endeavour machine learning grew out of the quest for artificial intelligence already in the early days of ai as an academic discipline some researchers were interested in having machines learn from data they attempted to approach the problem with various symbolic methods as well as what were then termed neural networks these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics probabilistic reasoning was also employed especially in automated medical diagnosis however an increasing emphasis on the logical knowledge based approach caused rift between ai and machine learning probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation by expert systems had come to dominate ai and statistics was out of favor work on symbolic knowledge based learning did continue within ai leading to inductive logic programming but the more statistical line of research was now outside the field of ai proper in pattern recognition and information retrieval neural networks research had been abandoned by ai and computer science around the same time this line too was continued outside the ai cs field as connectionism by researchers from other disciplines including hopfield rumelhart and hinton their main success came in the mid with the reinvention of backpropagation machine learning reorganized as separate field started to flourish in the the field changed its goal from achieving artificial intelligence to tackling solvable problems of practical nature it shifted focus away from the symbolic approaches it had inherited from ai and toward methods and models borrowed from statistics and probability theory it also benefited from the increasing availability of digitized information and the possibility to distribute that via the internet machine learning and data mining often employ the same methods and overlap significantly they can be roughly distinguished as follows machine learning focuses on prediction based on known properties learned from the training data data mining focuses on the discovery of previously unknown properties in the data this is the analysis step of knowledge discovery in databases the two areas overlap in many ways data mining uses many machine learning methods but often with slightly different goal in mind on the other hand machine learning also employs data mining methods as unsupervised learning or as preprocessing step to improve learner accuracy much of the confusion between these two research communities which do often have separate conferences and separate journals ecml pkdd being major exception comes from the basic assumptions they work with in machine learning performance is usually evaluated with respect to the ability to reproduce known knowledge while in knowledge discovery and data mining kdd the key task is the discovery of previously unknown knowledge evaluated with respect to known knowledge an uninformed unsupervised method will easily be outperformed by supervised methods while in typical kdd task supervised methods cannot be used due to the unavailability of training data machine learning also has intimate ties to optimization many learning problems are formulated as minimization of some loss function on training set of examples loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances for example in classification one wants to assign label to instances and models are trained to correctly predict the pre assigned labels of set examples the difference between the two fields arises from the goal of generalization while optimization algorithms can minimize the loss on training set machine learning is concerned with minimizing the loss on unseen samples relation to statistics machine learning and statistics are closely related fields according to michael jordan the ideas of machine learning from methodological principles to theoretical tools have had long pre history in statistics he also suggested the term data science as placeholder to call the overall field leo breiman distinguished two statistical modelling paradigms data model and algorithmic model wherein algorithmic model means more or less the machine learning algorithms like random forest some statisticians have adopted methods from machine learning leading to combined field that they call statistical learning theory core objective of learner is to generalize from its experience generalization in this context is the ability of learning machine to perform accurately on new unseen examples tasks after having experienced learning data set the training examples come from some generally unknown probability distribution considered representative of the space of occurrences and the learner has to build general model about this space that enables it to produce sufficiently accurate predictions in new cases the computational analysis of machine learning algorithms and their performance is branch of theoretical computer science known as computational learning theory because training sets are finite and the future is uncertain learning theory usually does not yield guarantees of the performance of algorithms instead probabilistic bounds on the performance are quite common the bias variance decomposition is one way to quantify generalization error in addition to performance bounds computational learning theorists study the time complexity and feasibility of learning in computational learning theory computation is considered feasible if it can be done in polynomial time there are two kinds of time complexity results positive results show that certain class of functions can be learned in polynomial time negative results show that certain classes cannot be learned in polynomial time there are many similarities between machine learning theory and statistical inference although they use different terms approaches decision tree learning decision tree learning uses decision tree as predictive model which maps observations about an item to conclusions about the item target value association rule learning association rule learning is method for discovering interesting relations between variables in large databases artificial neural networks an artificial neural network ann learning algorithm usually called neural network nn is learning algorithm that is inspired by the structure and functional aspects of biological neural networks computations are structured in terms of an interconnected group of artificial neurons processing information using connectionist approach to computation modern neural networks are non linear statistical data modeling tools they are usually used to model complex relationships between inputs and outputs to find patterns in data or to capture the statistical structure in an unknown joint probability distribution between observed variables inductive logic programming inductive logic programming ilp is an approach to rule learning using logic programming as uniform representation for input examples background knowledge and hypotheses given an encoding of the known background knowledge and set of examples represented as logical database of facts an ilp system will derive hypothesized logic program that entails all positive and no negative examples inductive programming is related field that considers any kind of programming languages for representing hypotheses and not only logic programming such as functional programs support vector machines support vector machines svms are set of related supervised learning methods used for classification and regression given set of training examples each marked as belonging to one of two categories an svm training algorithm builds model that predicts whether new example falls into one category or the other clustering cluster analysis is the assignment of set of observations into subsets called clusters so that observations within the same cluster are similar according to some predesignated criterion or criteria while observations drawn from different clusters are dissimilar different clustering techniques make different assumptions on the structure of the data often defined by some similarity metric and evaluated for example by internal compactness similarity between members of the same cluster and separation between different clusters other methods are based on estimated density and graph connectivity clustering is method of unsupervised learning and common technique for statistical data analysis bayesian networks bayesian network belief network or directed acyclic graphical model is probabilistic graphical model that represents set of random variables and their conditional independencies via directed acyclic graph dag for example bayesian network could represent the probabilistic relationships between diseases and symptoms given symptoms the network can be used to compute the probabilities of the presence of various diseases efficient algorithms exist that perform inference and learning reinforcement learning reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long term reward reinforcement learning algorithms attempt to find policy that maps states of the world to the actions the agent ought to take in those states reinforcement learning differs from the supervised learning problem in that correct input output pairs are never presented nor sub optimal actions explicitly corrected representation learning several learning algorithms mostly unsupervised learning algorithms aim at discovering better representations of the inputs provided during training classical examples include principal components analysis and cluster analysis representation learning algorithms often attempt to preserve the information in their input but transform it in way that makes it useful often as pre processing step before performing classification or predictions allowing to reconstruct the inputs coming from the unknown data generating distribution while not being necessarily faithful for configurations that are implausible under that distribution manifold learning algorithms attempt to do so under the constraint that the learned representation is low dimensional sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse has many zeros multilinear subspace learning algorithms aim to learn low dimensional representations directly from tensor representations for data without reshaping them into high dimensional vectors deep learning algorithms discover multiple levels of representation or hierarchy of features with higher level more abstract features defined in terms of or generating lower level features it has been argued that an intelligent machine is one that learns representation that disentangles the underlying factors of variation that explain the observed data similarity and metric learning in this problem the learning machine is given pairs of examples that are considered similar and pairs of less similar objects it then needs to learn similarity function or distance metric function that can predict if new objects are similar it is sometimes used in recommendation systems sparse dictionary learning in this method datum is represented as linear combination of basis functions and the coefficients are assumed to be sparse let be dimensional datum be by matrix where each column of represents basis function is the coefficient to represent using mathematically sparse dictionary learning means the following where is sparse generally speaking is assumed to be larger than to allow the freedom for sparse representation learning dictionary along with sparse representations is strongly np hard and also difficult to solve approximately popular heuristic method for sparse dictionary learning is svd sparse dictionary learning has been applied in several contexts in classification the problem is to determine which classes previously unseen datum belongs to suppose dictionary for each class has already been built then new datum is associated with the class such that it best sparsely represented by the corresponding dictionary sparse dictionary learning has also been applied in image de noising the key idea is that clean image patch can be sparsely represented by an image dictionary but the noise cannot genetic algorithms genetic algorithm ga is search heuristic that mimics the process of natural selection and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to given problem in machine learning genetic algorithms found some uses in the and vice versa machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms applications applications for machine learning include adaptive websites affective computing bioinformatics brain machine interfaces cheminformatics classifying dna sequences computational finance computer vision including object recognition detecting credit card fraud game playing information retrieval internet fraud detection machine perception medical diagnosis natural language processing optimization and metaheuristic online advertising recommender systems robot locomotion search engines sentiment analysis or opinion mining sequence mining software engineering speech and handwriting recognition stock market analysis structural health monitoring syntactic pattern recognition in the online movie company netflix held the first netflix prize competition to find program to better predict user preferences and improve the accuracy on its existing cinematch movie recommendation algorithm by at least joint team made up of researchers from at labs research in collaboration with the teams big chaos and pragmatic theory built an ensemble model to win the grand prize in for million shortly after the prize was awarded netflix realized that viewers ratings were not the best indicators of their viewing patterns everything is recommendation and they changed their recommendation engine accordingly in the wall street journal wrote about money management firm rebellion research use of machine learning to predict economic movements the article describes rebellion research prediction of the financial crisis and economic recovery in it has been reported that machine learning algorithm has been applied in art history to study fine art paintings and that it may have revealed previously unrecognized influences between artists software software suites containing variety of machine learning algorithms include the following open source software dlib elki encog mahout mlpy mlpack moa massive online analysis nd with deeplearning nupic opencv opennn orange pymc scikit learn scikit image shogun torch machine learning spark yooreeka weka commercial software with open source editions knime rapidminer commercial software amazon machine learning angoss knowledgestudio databricks ibm spss modeler kxen modeler lionsolver mathematica matlab microsoft azure machine learning neural designer neurosolutions oracle data mining rcase sas enterprise miner statistica data miner journals journal of machine learning research machine learning neural computation conferences conference on neural information processing systems international conference on machine learning international conference on learning representations see also references further reading mehryar mohri afshin rostamizadeh ameet talwalkar foundations of machine learning the mit press isbn ian witten and eibe frank data mining practical machine learning tools and techniques morgan kaufmann pp isbn sergios theodoridis konstantinos koutroumbas pattern recognition th edition academic press isbn mierswa ingo and wurst michael and klinkenberg ralf and scholz martin and euler timm yale rapid prototyping for complex data mining tasks in proceedings of the th acm sigkdd international conference on knowledge discovery and data mining kdd bing liu web data mining exploring hyperlinks contents and usage data springer isbn toby segaran programming collective intelligence reilly isbn huang kecman kopriva kernel based algorithms for mining huge data sets supervised semi supervised and unsupervised learning springer verlag berlin heidelberg pp illus hardcover isbn ethem alpayd\u0131n introduction to machine learning adaptive computation and machine learning mit press isbn mackay information theory inference and learning algorithms cambridge university press isbn kecman vojislav learning and soft computing support vector machines neural networks and fuzzy logic models the mit press cambridge ma pp illus isbn trevor hastie robert tibshirani and jerome friedman the elements of statistical learning springer isbn richard duda peter hart david stork pattern classification nd edition wiley new york isbn bishop neural networks for pattern recognition oxford university press isbn ryszard michalski george tecuci machine learning multistrategy approach volume iv morgan kaufmann isbn sholom weiss and casimir kulikowski computer systems that learn morgan kaufmann isbn yves kodratoff ryszard michalski machine learning an artificial intelligence approach volume iii morgan kaufmann isbn ryszard michalski jaime carbonell tom mitchell machine learning an artificial intelligence approach volume ii morgan kaufmann isbn ryszard michalski jaime carbonell tom mitchell machine learning an artificial intelligence approach tioga publishing company isbn vladimir vapnik statistical learning theory wiley interscience isbn ray solomonoff an inductive inference machine ire convention record section on information theory part pp ray solomonoff an inductive inference machine privately circulated report from the dartmouth summer research conference on ai external links international machine learning society popular online course by andrew ng at coursera it uses gnu octave the course is free version of stanford university actual course taught by ng whose lectures are also available for free machine learning video lectures mloss is an academic database of open source machine learning software", "Simulation": "wooden mechanical horse simulator during world war simulation is the imitation of the operation of real world process or system over time the act of simulating something first requires that model be developed this model represents the key characteristics or behaviors functions of the selected physical or abstract system or process the model represents the system itself whereas the simulation represents the operation of the system over time simulation is used in many contexts such as simulation of technology for performance optimization safety engineering testing training education and video games often computer experiments are used to study simulation models simulation is also used with scientific modelling of natural systems or human systems to gain insight into their functioning simulation can be used to show the eventual real effects of alternative conditions and courses of action simulation is also used when the real system cannot be engaged because it may not be accessible or it may be dangerous or unacceptable to engage or it is being designed but not yet built or it may simply not exist key issues in simulation include acquisition of valid source information about the relevant selection of key characteristics and behaviours the use of simplifying approximations and assumptions within the simulation and fidelity and validity of the simulation outcomes classification and terminology human in the loop simulation of outer space visualization of direct numerical simulation model historically simulations used in different fields developed largely independently but th century studies of systems theory and cybernetics combined with spreading use of computers across all those fields have led to some unification and more systematic view of the concept physical simulation refers to simulation in which physical objects are substituted for the real thing some circles use the term for computer simulations modelling selected laws of physics but this article doesn these physical objects are often chosen because they are smaller or cheaper than the actual object or system interactive simulation is special kind of physical simulation often referred to as human in the loop simulation in which physical simulations include human operators such as in flight simulator or driving simulator human in the loop simulations can include computer simulation as so called synthetic environment simulation in failure analysis refers to simulation in which we create environment conditions to identify the cause of equipment failure this was the best and fastest method to identify the failure cause computer simulation computer simulation or sim is an attempt to model real life or hypothetical situation on computer so that it can be studied to see how the system works by changing variables in the simulation predictions may be made about the behaviour of the system it is tool to virtually investigate the behaviour of the system under study computer simulation has become useful part of modeling many natural systems in physics chemistry and biology and human systems in economics and social science computational sociology as well as in engineering to gain insight into the operation of those systems good example of the usefulness of using computers to simulate can be found in the field of network traffic simulation in such simulations the model behaviour will change each simulation according to the set of initial parameters assumed for the environment traditionally the formal modeling of systems has been via mathematical model which attempts to find analytical solutions enabling the prediction of the behaviour of the system from set of parameters and initial conditions computer simulation is often used as an adjunct to or substitution for modeling systems for which simple closed form analytic solutions are not possible there are many different types of computer simulation the common feature they all share is the attempt to generate sample of representative scenarios for model in which complete enumeration of all possible states would be prohibitive or impossible several software packages exist for running computer based simulation modeling monte carlo simulation stochastic modeling multimethod modeling that makes all the modeling almost effortless modern usage of the term computer simulation may encompass virtually any computer based representation computer science in computer science simulation has some specialized meanings alan turing used the term simulation to refer to what happens when universal machine executes state transition table in modern terminology computer runs program that describes the state transitions inputs and outputs of subject discrete state machine the computer simulates the subject machine accordingly in theoretical computer science the term simulation is relation between state transition systems useful in the study of operational semantics less theoretically an interesting application of computer simulation is to simulate computers using computers in computer architecture type of simulator typically called an emulator is often used to execute program that has to run on some inconvenient type of computer for example newly designed computer that has not yet been built or an obsolete computer that is no longer available or in tightly controlled testing environment see computer architecture simulator and platform virtualization for example simulators have been used to debug microprogram or sometimes commercial application programs before the program is downloaded to the target machine since the operation of the computer is simulated all of the information about the computer operation is directly available to the programmer and the speed and execution of the simulation can be varied at will simulators may also be used to interpret fault trees or test vlsi logic designs before they are constructed symbolic simulation uses variables to stand for unknown values in the field of optimization simulations of physical processes are often used in conjunction with evolutionary computation to optimize control strategies simulation in education and training simulation is extensively used for educational purposes it is frequently used by way of adaptive hypermedia simulation is often used in the training of civilian and military personnel this usually occurs when it is prohibitively expensive or simply too dangerous to allow trainees to use the real equipment in the real world in such situations they will spend time learning valuable lessons in safe virtual environment yet living lifelike experience or at least it is the goal often the convenience is to permit mistakes during training for safety critical system there is distinction though between simulations used for training and instructional simulation training simulations typically come in one of three categories live simulation where actual players use genuine systems in real environment virtual simulation where actual players use simulated systems in synthetic environment or constructive simulation where simulated players use simulated systems in synthetic environment constructive simulation is often referred to as wargaming since it bears some resemblance to table top war games in which players command armies of soldiers and equipment that move around board in standardized tests live simulations are sometimes called high fidelity producing samples of likely performance as opposed to low fidelity pencil and paper simulations producing only signs of possible performance but the distinction between high moderate and low fidelity remains relative depending on the context of particular comparison simulations in education are somewhat like training simulations they focus on specific tasks the term microworld is used to refer to educational simulations which model some abstract concept rather than simulating realistic object or environment or in some cases model real world environment in simplistic way so as to help learner develop an understanding of the key concepts normally user can create some sort of construction within the microworld that will behave in way consistent with the concepts being modeled seymour papert was one of the first to advocate the value of microworlds and the logo programming environment developed by papert is one of the most famous microworlds as another example the global challenge award online stem learning web site uses microworld simulations to teach science concepts related to global warming and the future of energy other projects for simulations in educations are open source physics netsim etc project management simulation is increasingly used to train students and professionals in the art and science of project management using simulation for project management training improves learning retention and enhances the learning process social simulations may be used in social science classrooms to illustrate social and political processes in anthropology economics history political science or sociology courses typically at the high school or university level these may for example take the form of civics simulations in which participants assume roles in simulated society or international relations simulations in which participants engage in negotiations alliance formation trade diplomacy and the use of force such simulations might be based on fictitious political systems or be based on current or historical events an example of the latter would be barnard college reacting to the past series of historical educational games the national science foundation has also supported the creation of reacting games that address science and math education in recent years there has been increasing use of social simulations for staff training in aid and development agencies the carana simulation for example was first developed by the united nations development programme and is now used in very revised form by the world bank for training staff to deal with fragile and conflict affected countries common user interaction systems for virtual simulations virtual simulations represent specific category of simulation that utilizes simulation equipment to create simulated world for the user virtual simulations allow users to interact with virtual world virtual worlds operate on platforms of integrated software and hardware components in this manner the system can accept input from the user body tracking voice sound recognition physical controllers and produce output to the user visual display aural display haptic display virtual simulations use the aforementioned modes of interaction to produce sense of immersion for the user virtual simulation input hardware motorcycle simulator of bienal do autom\u00f3vel exhibition in belo horizonte brazil there is wide variety of input hardware available to accept user input for virtual simulations the following list briefly describes several of them body tracking the motion capture method is often used to record the user movements and translate the captured data into inputs for the virtual simulation for example if user physically turns their head the motion would be captured by the simulation hardware in some way and translated to corresponding shift in view within the simulation capture suits and or gloves may be used to capture movements of users body parts the systems may have sensors incorporated inside them to sense movements of different body parts fingers alternatively these systems may have exterior tracking devices or marks that can be detected by external ultrasound optical receivers or electromagnetic sensors internal inertial sensors are also available on some systems the units may transmit data either wirelessly or through cables eye trackers can also be used to detect eye movements so that the system can determine precisely where user is looking at any given instant physical controllers physical controllers provide input to the simulation only through direct manipulation by the user in virtual simulations tactile feedback from physical controllers is highly desirable in number of simulation environments omni directional treadmills can be used to capture the users locomotion as they walk or run high fidelity instrumentation such as instrument panels in virtual aircraft cockpits provides users with actual controls to raise the level of immersion for example pilots can use the actual global positioning system controls from the real device in simulated cockpit to help them practice procedures with the actual device in the context of the integrated cockpit system voice sound recognition this form of interaction may be used either to interact with agents within the simulation virtual people or to manipulate objects in the simulation information voice interaction presumably increases the level of immersion for the user users may use headsets with boom microphones lapel microphones or the room may be equipped with strategically located microphones current research into user input systems research in future input systems hold great deal of promise for virtual simulations systems such as brain computer interfaces bcis brain computer interface offer the ability to further increase the level of immersion for virtual simulation users lee keinrath scherer bischof pfurtscheller proved that na\u00efve subjects could be trained to use bci to navigate virtual apartment with relative ease using the bci the authors found that subjects were able to freely navigate the virtual environment with relatively minimal effort it is possible that these types of systems will become standard input modalities in future virtual simulation systems virtual simulation output hardware there is wide variety of output hardware available to deliver stimulus to users in virtual simulations the following list briefly describes several of them visual display visual displays provide the visual stimulus to the user stationary displays can vary from conventional desktop display to degree wrap around screens to stereo three dimensional screens conventional desktop displays can vary in size from to inches wrap around screens are typically utilized in what is known as cave automatic virtual environment cave cave automatic virtual environment stereo three dimensional screens produce three dimensional images either with or without special glasses depending on the design head mounted displays hmds have small displays that are mounted on headgear worn by the user these systems are connected directly into the virtual simulation to provide the user with more immersive experience weight update rates and field of view are some of the key variables that differentiate hmds naturally heavier hmds are undesirable as they cause fatigue over time if the update rate is too slow the system is unable to update the displays fast enough to correspond with quick head turn by the user slower update rates tend to cause simulation sickness and disrupt the sense of immersion field of view or the angular extent of the world that is seen at given moment field of view can vary from system to system and has been found to affect the users sense of immersion aural display several different types of audio systems exist to help the user hear and localize sounds spatially special software can be used to produce audio effects audio to create the illusion that sound sources are placed within defined three dimensional space around the user stationary conventional speaker systems may be used provide dual or multi channel surround sound however external speakers are not as effective as headphones in producing audio effects conventional headphones offer portable alternative to stationary speakers they also have the added advantages of masking real world noise and facilitate more effective audio sound effects haptic display these displays provide sense of touch to the user haptic technology this type of output is sometimes referred to as force feedback tactile tile displays use different types of actuators such as inflatable bladders vibrators low frequency sub woofers pin actuators and or thermo actuators to produce sensations for the user end effector displays can respond to users inputs with resistance and force these systems are often used in medical applications for remote surgeries that employ robotic instruments vestibular display these displays provide sense of motion to the user motion simulator they often manifest as motion bases for virtual vehicle simulation such as driving simulators or flight simulators motion bases are fixed in place but use actuators to move the simulator in ways that can produce the sensations pitching yawing or rolling the simulators can also move in such way as to produce sense of acceleration on all axes the motion base can produce the sensation of falling clinical healthcare simulators medical simulators are increasingly being developed and deployed to teach therapeutic and diagnostic procedures as well as medical concepts and decision making to personnel in the health professions simulators have been developed for training procedures ranging from the basics such as blood draw to laparoscopic surgery and trauma care they are also important to help on prototyping new devices for biomedical engineering problems currently simulators are applied to research and develop tools for new therapies treatments and early diagnosis in medicine many medical simulators involve computer connected to plastic simulation of the relevant anatomy sophisticated simulators of this type employ life size mannequin that responds to injected drugs and can be programmed to create simulations of life threatening emergencies in other simulations visual components of the procedure are reproduced by computer graphics techniques while touch based components are reproduced by haptic feedback devices combined with physical simulation routines computed in response to the user actions medical simulations of this sort will often use ct or mri scans of patient data to enhance realism some medical simulations are developed to be widely distributed such as web enabled simulations and procedural simulations that can be viewed via standard web browsers and can be interacted with using standard computer interfaces such as the keyboard and mouse another important medical application of simulator although perhaps denoting slightly different meaning of simulator is the use of placebo drug formulation that simulates the active drug in trials of drug efficacy see placebo origins of technical term improving patient safety patient safety is concern in the medical industry patients have been known to suffer injuries and even death due to management error and lack of using best standards of care and training according to building national agenda for simulation based medical education eder van hook jackie health care provider ability to react prudently in an unexpected situation is one of the most critical factors in creating positive outcome in medical emergency regardless of whether it occurs on the battlefield freeway or hospital emergency room simulation eder van hook also noted that medical errors kill up to with an estimated cost between and million and to billion for preventable adverse events dollars per year deaths due to preventable adverse events exceed deaths attributable to motor vehicle accidents breast cancer or aids eder van hook with these types of statistics it is no wonder that improving patient safety is prevalent concern in the industry innovative simulation training solutions are now being used to train medical professionals in an attempt to reduce the number of safety concerns that have adverse effects on the patients however according to the article does simulation improve patient safety self efficacy competence operational performance and patient safety nishisaki keren and nadkarni the jury is still out nishisaki states that there is good evidence that simulation training improves provider and team self efficacy and competence on manikins there is also good evidence that procedural simulation improves actual operational performance in clinical settings however no evidence yet shows that crew resource management training through simulation despite its promise improves team operational performance at the bedside although evidence that simulation based training actually improves patient outcome has been slow to accrue today the ability of simulation to provide hands on experience that translates to the operating room is no longer in doubt one such attempt to improve patient safety through the use of simulations training is pediatric care to deliver just in time service or and just in place this training consists of minutes of simulated training just before workers report to shift it is hoped that the recentness of the training will increase the positive and reduce the negative results that have generally been associated with the procedure the purpose of this study is to determine if just in time training improves patient safety and operational performance of orotracheal intubation and decrease occurrences of undesired associated events and to test the hypothesis that high fidelity simulation may enhance the training efficacy and patient safety in simulation settings the conclusion as reported in abstract just in time simulation training improves icu physician trainee airway resuscitation participation without compromising procedural success or safety nishisaki were that simulation training improved resident participation in real cases but did not sacrifice the quality of service it could be therefore hypothesized that by increasing the number of highly trained residents through the use of simulation training that the simulation training does in fact increase patient safety this hypothesis would have to be researched for validation and the results may or may not generalize to other situations history of simulation in healthcare the first medical simulators were simple models of human patients since antiquity these representations in clay and stone were used to demonstrate clinical features of disease states and their effects on humans models have been found from many cultures and continents these models have been used in some cultures chinese culture as diagnostic instrument allowing women to consult male physicians while maintaining social laws of modesty models are used today to help students learn the anatomy of the musculoskeletal system and organ systems type of models active models active models that attempt to reproduce living anatomy or physiology are recent developments the famous harvey mannequin was developed at the university of miami and is able to recreate many of the physical findings of the cardiology examination including palpation auscultation and interactive models more recently interactive models have been developed that respond to actions taken by student or physician until recently these simulations were two dimensional computer programs that acted more like textbook than patient computer simulations have the advantage of allowing student to make judgments and also to make errors the process of iterative learning through assessment evaluation decision making and error correction creates much stronger learning environment than passive instruction computer simulators diteams learner is percussing the patient chest in virtual field hospital simulators have been proposed as an ideal tool for assessment of students for clinical skills for patients cybertherapy can be used for sessions simulating traumatic experiences from fear of heights to social anxiety programmed patients and simulated clinical situations including mock disaster drills have been used extensively for education and evaluation these lifelike simulations are expensive and lack reproducibility fully functional di simulator would be the most specific tool available for teaching and measurement of clinical skills gaming platforms have been applied to create these virtual medical environments to create an interactive method for learning and application of information in clinical context immersive disease state simulations allow doctor or hcp to experience what disease actually feels like using sensors and transducers symptomatic effects can be delivered to participant allowing them to experience the patients disease state such simulator meets the goals of an objective and standardized examination for clinical competence this system is superior to examinations that use standard patients because it permits the quantitative measurement of competence as well as reproducing the same objective findings simulation in entertainment simulation in entertainment encompasses many large and popular industries such as film television video games including serious games and rides in theme parks although modern simulation is thought to have its roots in training and the military in the th century it also became conduit for enterprises which were more hedonistic in nature advances in technology in the and caused simulation to become more widely used and it began to appear in movies such as jurassic park and in computer based games such as atari battlezone history early history and the first simulation game may have been created as early as by thomas goldsmith jr and estle ray mann this was straightforward game that simulated missile being fired at target the curve of the missile and its speed could be adjusted using several knobs in computer game called tennis for two was created by willy higginbotham which simulated tennis game between two players who could both play at the same time using hand controls and was displayed on an oscilloscope this was one of the first electronic video games to use graphical display modern simulation present advances in technology in the made the computer more affordable and more capable than they were in previous decades which facilitated the rise of computer such as the xbox gaming the first video game consoles released in the and early fell prey to the industry crash in but in nintendo released the nintendo entertainment system nes which became one of the best selling consoles in video game history in the computer games became widely popular with the release of such game as the sims and command conquer and the still increasing power of desktop computers today computer simulation games such as world of warcraft are played by millions of people around the world computer generated imagery was used in film to simulate objects as early as though in the film tron was the first film to use computer generated imagery for more than couple of minutes however the commercial failure of the movie may have caused the industry to step away from the technology in the film jurassic park became the first popular film to use computer generated graphics extensively integrating the simulated dinosaurs almost seamlessly into live action scenes this event transformed the film industry in the film toy story was the first film to use only computer generated images and by the new millennium computer generated graphics were the leading choice for special effects in films simulators have been used for entertainment since the link trainer in the the first modern simulator ride to open at theme park was disney star tours in soon followed by universal the funtastic world of hanna barbera in which was the first ride to be done entirely with computer graphics examples of entertainment simulation computer and video games simulation games as opposed to other genres of video and computer games represent or simulate an environment accurately moreover they represent the interactions between the playable characters and the environment realistically these kinds of games are usually more complex in terms of game play simulation games have become incredibly popular among people of all ages popular simulation games include simcity tiger woods pga tour and virtonomics there are also flight simulation and driving simulation games film computer generated imagery is the application of the field of computer graphics to special effects this technology is used for visual effects because they are high in quality controllable and can create effects that would not be feasible using any other technology either because of cost resources or safety computer generated graphics can be seen in many live action movies today especially those of the action genre further computer generated imagery has almost completely supplanted hand drawn animation in children movies which are increasingly computer generated only examples of movies that use computer generated imagery include finding nemo and iron man theme park rides simulator rides are the progeny of military training simulators and commercial simulators but they are different in fundamental way while military training simulators react realistically to the input of the trainee in real time ride simulators only feel like they move realistically and move according to prerecorded motion scripts one of the first simulator rides star tours which cost millon used hydraulic motion based cabin the movement was programmed by joystick today simulator rides such as the amazing adventures of spider man include elements to increase the amount of immersion experienced by the riders such as imagery physical effects spraying water or producing scents and movement through an environment examples of simulation rides include mission space and the simpsons ride there are many simulation rides at themeparks like disney universal etc examples are flint stones earth quake time machine king kong simulation and manufacturing manufacturing represents one of the most important applications of simulation this technique represents valuable tool used by engineers when evaluating the effect of capital investment in equipments and physical facilities like factory plants warehouses and distribution centers simulation can be used to predict the performance of an existing or planned system and to compare alternative solutions for particular design problem another important goal of manufacturing simulations is to quantify system performance common measures of system performance include the following throughput under average and peak loads system cycle time how long it take to produce one part utilization of resource labor and machines bottlenecks and choke points queuing at work locations queuing and delays caused by material handling devices and systems wip storages needs staffing requirements effectiveness of scheduling systems effectiveness of control systems more examples of simulation automobiles car racing simulator soldier tests out heavy wheeled vehicle driver simulator an automobile simulator provides an opportunity to reproduce the characteristics of real vehicles in virtual environment it replicates the external factors and conditions with which vehicle interacts enabling driver to feel as if they are sitting in the cab of their own vehicle scenarios and events are replicated with sufficient reality to ensure that drivers become fully immersed in the experience rather than simply viewing it as an educational experience the simulator provides constructive experience for the novice driver and enables more complex exercises to be undertaken by the more mature driver for novice drivers truck simulators provide an opportunity to begin their career by applying best practice for mature drivers simulation provides the ability to enhance good driving or to detect poor practice and to suggest the necessary steps for remedial action for companies it provides an opportunity to educate staff in the driving skills that achieve reduced maintenance costs improved productivity and most importantly to ensure the safety of their actions in all possible situations biomechanics an open source simulation platform for creating dynamic mechanical models built from combinations of rigid and deformable bodies joints constraints and various force actuators it is specialized for creating biomechanical models of human anatomical structures with the intention to study their function and eventually assist in the design and planning of medical treatment biomechanics simulator is used to analyze walking dynamics study sports performance simulate surgical procedures analyze joint loads design medical devices and animate human and animal movement neuromechanical simulator that combines biomechanical and biologically realistic neural network simulation it allows the user to test hypotheses on the neural basis of behavior in physically accurate virtual environment city and urban city simulator can be city building game but can also be tool used by urban planners to understand how cities are likely to evolve in response to various policy decisions anylogic is an example of modern large scale urban simulators designed for use by urban planners city simulators are generally agent based simulations with explicit representations for land use and transportation urbansim and leam are examples of large scale urban simulation models that are used by metropolitan planning agencies and military bases for land use and transportation planning classroom of the future the classroom of the future will probably contain several kinds of simulators in addition to textual and visual learning tools this will allow students to enter the clinical years better prepared and with higher skill level the advanced student or postgraduate will have more concise and comprehensive method of retraining or of incorporating new clinical procedures into their skill set and regulatory bodies and medical institutions will find it easier to assess the proficiency and competency of individuals the classroom of the future will also form the basis of clinical skills unit for continuing education of medical personnel and in the same way that the use of periodic flight training assists airline pilots this technology will assist practitioners throughout their career the simulator will be more than living textbook it will become an integral part of the practice of medicine the simulator environment will also provide standard platform for curriculum development in institutions of medical education communication satellites modern satellite communications systems satcom are often large and complex with many interacting parts and elements in addition the need for broadband connectivity on moving vehicle has increased dramatically in the past few years for both commercial and military applications to accurately predict and deliver high quality of service satcom system designers have to factor in terrain as well as atmospheric and meteorological conditions in their planning to deal with such complexity system designers and operators increasingly turn towards computer models of their systems to simulate real world operational conditions and gain insights into usability and requirements prior to final product sign off modeling improves the understanding of the system by enabling the satcom system designer or planner to simulate real world performance by injecting the models with multiple hypothetical atmospheric and environmental conditions simulation is often used in the training of civilian and military personnel this usually occurs when it is prohibitively expensive or simply too dangerous to allow trainees to use the real equipment in the real world in such situations they will spend time learning valuable lessons in safe virtual environment yet living lifelike experience or at least it is the goal often the convenience is to permit mistakes during training for safety critical system digital lifecycle simulation of airflow over an engine simulation solutions are being increasingly integrated with cax cad cam cae solutions and processes the use of simulation throughout the product lifecycle especially at the earlier concept and design stages has the potential of providing substantial benefits these benefits range from direct cost issues such as reduced prototyping and shorter time to market to better performing products and higher margins however for some companies simulation has not provided the expected benefits the research firm aberdeen group has found that nearly all best in class manufacturers use simulation early in the design process as compared to or laggards who do not the successful use of simulation early in the lifecycle has been largely driven by increased integration of simulation tools with the entire cad cam and plm solution set simulation solutions can now function across the extended enterprise in multi cad environment and include solutions for managing simulation data and processes and ensuring that simulation results are made part of the product lifecycle history the ability to use simulation across the entire lifecycle has been enhanced through improved user interfaces such as tailorable user interfaces and wizards which allow all appropriate plm participants to take part in the simulation process disaster preparedness simulation training has become method for preparing people for disasters simulations can replicate emergency situations and track how learners respond thanks to lifelike experience disaster preparedness simulations can involve training on how to handle terrorism attacks natural disasters pandemic outbreaks or other life threatening emergencies one organization that has used simulation training for disaster preparedness is cade center for advancement of distance education cade has used video game to prepare emergency workers for multiple types of attacks as reported by news medical net the video game is the first in series of simulations to address bioterrorism pandemic flu smallpox and other disasters that emergency personnel must prepare for developed by team from the university of illinois at chicago uic the game allows learners to practice their emergency skills in safe controlled environment the emergency simulation program esp at the british columbia institute of technology bcit vancouver british columbia canada is another example of an organization that uses simulation to train for emergency situations esp uses simulation to train on the following situations forest fire fighting oil or chemical spill response earthquake response law enforcement municipal fire fighting hazardous material handling military training and response to terrorist attack one feature of the simulation system is the implementation of dynamic run time clock which allows simulations to run simulated time frame speeding up or slowing down time as desired additionally the system allows session recordings picture icon based navigation file storage of individual simulations multimedia components and launch external applications at the university of qu\u00e9bec in chicoutimi research team at the outdoor research and expertise laboratory laboratoire expertise et de recherche en plein air lerpa specializes in using wilderness backcountry accident simulations to verify emergency response coordination instructionally the benefits of emergency training through simulations are that learner performance can be tracked through the system this allows the developer to make adjustments as necessary or alert the educator on topics that may require additional attention other advantages are that the learner can be guided or trained on how to respond appropriately before continuing to the next emergency segment this is an aspect that may not be available in the live environment some emergency training simulators also allows for immediate feedback while other simulations may provide summary and instruct the learner to engage in the learning topic again in live emergency situation emergency responders do not have time to waste simulation training in this environment provides an opportunity for learners to gather as much information as they can and practice their knowledge in safe environment they can make mistakes without risk of endangering lives and be given the opportunity to correct their errors to prepare for the real life emergency economics in economics and especially macroeconomics the effects of proposed policy actions such as fiscal policy changes or monetary policy changes are simulated to judge their desirability mathematical model of the economy having been fitted to historical economic data is used as proxy for the actual economy proposed values of government spending taxation open market operations etc are used as inputs to the simulation of the model and various variables of interest such as the inflation rate the unemployment rate the balance of trade deficit the government budget deficit etc are the outputs of the simulation the simulated values of these variables of interest are compared for different proposed policy inputs to determine which set of outcomes is most desirable engineering technology and processes simulation is an important feature in engineering systems or any system that involves many processes for example in electrical engineering delay lines may be used to simulate propagation delay and phase shift caused by an actual transmission line similarly dummy loads may be used to simulate impedance without simulating propagation and is used in situations where propagation is unwanted simulator may imitate only few of the operations and functions of the unit it simulates contrast with emulate most engineering simulations entail mathematical modeling and computer assisted investigation there are many cases however where mathematical modeling is not reliable simulation of fluid dynamics problems often require both mathematical and physical simulations in these cases the physical models require dynamic similitude physical and chemical simulations have also direct realistic uses rather than research uses in chemical engineering for example process simulations are used to give the process parameters immediately used for operating chemical plants such as oil refineries simulators are also used for plant operator training it is called operator training simulator ots and has been widely adopted by many industries from chemical to oil gas and to power industry this created safe and realistic virtual environment to train board operators and engineers mimic is capable of providing high fidelity dynamic models of nearly all chemical plants for operator training and control system testing equipment due to the dangerous and expensive nature of training on heavy equipment simulation has become common solution across many industries types of simulated equipment include cranes mining reclaimers and construction equipment among many others often the simulation units will include pre built scenarios by which to teach trainees as well as the ability to customize new scenarios such equipment simulators are intended to create safe and cost effective alternative to training on live equipment ergonomics ergonomic simulation involves the analysis of virtual products or manual tasks within virtual environment in the engineering process the aim of ergonomics is to develop and to improve the design of products and work environments ergonomic simulation utilizes an anthropometric virtual representation of the human commonly referenced as mannequin or digital human models dhms to mimic the postures mechanical loads and performance of human operator in simulated environment such as an airplane automobile or manufacturing facility dhms are recognized as evolving and valuable tool for performing proactive ergonomics analysis and design the simulations employ graphics and physics based models to animate the virtual humans ergonomics software uses inverse kinematics ik capability for posing the dhms several ergonomic simulation tools have been developed including jack safework ramsis and sammie the software tools typically calculate biomechanical properties including individual muscle forces joint forces and moments most of these tools employ standard ergonomic evaluation methods such as the niosh lifting equation and rapid upper limb assessment rula some simulations also analyze physiological measures including metabolism energy expenditure and fatigue limits cycle time studies design and process validation user comfort reachability and line of sight are other human factors that may be examined in ergonomic simulation packages modeling and simulation of task can be performed by manually manipulating the virtual human in the simulated environment some ergonomics simulation software permits interactive real time simulation and evaluation through actual human input via motion capture technologies however motion capture for ergonomics requires expensive equipment and the creation of props to represent the environment or product some applications of ergonomic simulation in include analysis of solid waste collection disaster management tasks interactive gaming automotive assembly line virtual prototyping of rehabilitation aids and aerospace product design ford engineers use ergonomics simulation software to perform virtual product design reviews using engineering data the simulations assist evaluation of assembly ergonomics the company uses siemen jack and jill ergonomics simulation software in improving worker safety and efficiency without the need to build expensive prototypes finance in finance computer simulations are often used for scenario planning risk adjusted net present value for example is computed from well defined but not always known or fixed inputs by imitating the performance of the project under evaluation simulation can provide distribution of npv over range of discount rates and other variables simulations are frequently used in financial training to engage participants in experiencing various historical as well as fictional situations there are stock market simulations portfolio simulations risk management simulations or models and forex simulations such simulations are typically based on stochastic asset models using these simulations in training program allows for the application of theory into something akin to real life as with other industries the use of simulations can be technology or case study driven flight flight simulation training devices fstd are used to train pilots on the ground in comparison to training in an actual aircraft simulation based training allows for the training of maneuvers or situations that may be impractical or even dangerous to perform in the aircraft while keeping the pilot and instructor in relatively low risk environment on the ground for example electrical system failures instrument failures hydraulic system failures and even flight control failures can be simulated without risk to the pilots or an aircraft instructors can also provide students with higher concentration of training tasks in given period of time than is usually possible in the aircraft for example conducting multiple instrument approaches in the actual aircraft may require significant time spent repositioning the aircraft while in simulation as soon as one approach has been completed the instructor can immediately preposition the simulated aircraft to an ideal or less than ideal location from which to begin the next approach flight simulation also provides an economic advantage over training in an actual aircraft once fuel maintenance and insurance costs are taken into account the operating costs of an fstd are usually substantially lower than the operating costs of the simulated aircraft for some large transport category airplanes the operating costs may be several times lower for the fstd than the actual aircraft some people who use simulator software especially flight simulator software build their own simulator at home some people to further the realism of their homemade simulator buy used cards and racks that run the same software used by the original machine while this involves solving the problem of matching hardware and software and the problem that hundreds of cards plug into many different racks many still find that solving these problems is well worthwhile some are so serious about realistic simulation that they will buy real aircraft parts like complete nose sections of written off aircraft at aircraft boneyards this permits people to simulate hobby that they are unable to pursue in real life marine bearing resemblance to flight simulators marine simulators train ships personnel the most common marine simulators include ship bridge simulators engine room simulators cargo handling simulators communication gmdss simulators rov simulators simulators like these are mostly used within maritime colleges training institutions and navies they often consist of replication of ships bridge with operating console and number of screens on which the virtual surroundings are projected military military simulations also known informally as war games are models in which theories of warfare can be tested and refined without the need for actual hostilities they exist in many different forms with varying degrees of realism in recent times their scope has widened to include not only military but also political and social factors for example the nationlab series of strategic exercises in latin america while many governments make use of simulation both individually and collaboratively little is known about the model specifics outside professional circles payment and securities settlement system simulation techniques have also been applied to payment and securities settlement systems among the main users are central banks who are generally responsible for the oversight of market infrastructure and entitled to contribute to the smooth functioning of the payment systems central banks have been using payment system simulations to evaluate things such as the adequacy or sufficiency of liquidity available in the form of account balances and intraday credit limits to participants mainly banks to allow efficient settlement of payments the need for liquidity is also dependent on the availability and the type of netting procedures in the systems thus some of the studies have focus on system comparisons another application is to evaluate risks related to events such as communication network breakdowns or the inability of participants to send payments in case of possible bank failure this kind of analysis falls under the concepts of stress testing or scenario analysis common way to conduct these simulations is to replicate the settlement logics of the real payment or securities settlement systems under analysis and then use real observed payment data in case of system comparison or system development naturally also the other settlement logics need to be implemented to perform stress testing and scenario analysis the observed data needs to be altered some payments delayed or removed to analyze the levels of liquidity initial liquidity levels are varried system comparisons benchmarking or evaluations of new netting algorithms or rules are performed by running simulations with fixed set of data and varying only the system setups inference is usually done by comparing the benchmark simulation results to the results of altered simulation setups by comparing indicators such as unsettled transactions or settlement delays project management project management simulation is simulation used for project management training and analysis it is often used as training simulation for project managers in other cases it is used for what if analysis and for supporting decision making in real projects frequently the simulation is conducted using software tools robotics robotics simulator is used to create embedded applications for specific or not robot without being dependent on the real robot in some cases these applications can be transferred to the real robot or rebuilt without modifications robotics simulators allow reproducing situations that cannot be created in the real world because of cost time or the uniqueness of resource simulator also allows fast robot prototyping many robot simulators feature physics engines to simulate robot dynamics production simulations of production systems is used mainly to examine the effect of improvements or investments in production system most often this is done using static spreadsheet with process times and transportation times for more sophisticated simulations discrete event simulation des is used with the advantages to simulate dynamics in the production system production system is very much dynamic depending on variations in manufacturing processes assembly times machine set ups breaks breakdowns and small stoppages there are lots of software commonly used for discrete event simulation they differ in usability and markets but do often share the same foundation sales process simulations are useful in modeling the flow of transactions through business processes such as in the field of sales process engineering to study and improve the flow of customer orders through various stages of completion say from an initial proposal for providing goods services through order acceptance and installation such simulations can help predict the impact of how improvements in methods might impact variability cost labor time and the quantity of transactions at various stages in the process full featured computerized process simulator can be used to depict such models as can simpler educational demonstrations using spreadsheet software pennies being transferred between cups based on the roll of die or dipping into tub of colored beads with scoop sports in sports computer simulations are often done to predict the outcome of events and the performance of individual sportspeople they attempt to recreate the event through models built from statistics the increase in technology has allowed anyone with knowledge of programming the ability to run simulations of their models the simulations are built from series of mathematical algorithms or models and can vary with accuracy accuscore which is licensed by companies such as espn is well known simulation program for all major sports it offers detailed analysis of games through simulated betting lines projected point totals and overall probabilities with the increased interest in fantasy sports simulation models that predict individual player performance have gained popularity companies like what if sports and statfox specialize in not only using their simulations for predicting game results but how well individual players will do as well many people use models to determine who to start in their fantasy leagues another way simulations are helping the sports field is in the use of biomechanics models are derived and simulations are run from data received from sensors attached to athletes and video equipment sports biomechanics aided by simulation models answer questions regarding training techniques such as the effect of fatigue on throwing performance height of throw and biomechanical factors of the upper limbs reactive strength index hand contact time computer simulations allow their users to take models which before were too complex to run and give them answers simulations have proven to be some of the best insights into both play performance and team predictability space shuttle countdown firing room configured for space shuttle launches simulation is used at kennedy space center ksc to train and certify space shuttle engineers during simulated launch countdown operations the space shuttle engineering community participates in launch countdown integrated simulation before each shuttle flight this simulation is virtual simulation where real people interact with simulated space shuttle vehicle and ground support equipment gse hardware the shuttle final countdown phase simulation also known as involves countdown processes that integrate many of the space shuttle vehicle and gse systems some of the shuttle systems integrated in the simulation are the main propulsion system main engines solid rocket boosters ground liquid hydrogen and liquid oxygen external tank flight controls navigation and avionics the high level objectives of the shuttle final countdown phase simulation are to demonstrate firing room final countdown phase operations to provide training for system engineers in recognizing reporting and evaluating system problems in time critical environment to exercise the launch teams ability to evaluate prioritize and respond to problems in an integrated manner within time critical environment to provide procedures to be used in performing failure recovery testing of the operations performed in the final countdown phase the shuttle final countdown phase simulation takes place at the kennedy space center launch control center firing rooms the firing room used during the simulation is the same control room where real launch countdown operations are executed as result equipment used for real launch countdown operations is engaged command and control computers application software engineering plotting and trending tools launch countdown procedure documents launch commit criteria documents hardware requirement documents and any other items used by the engineering launch countdown teams during real launch countdown operations are used during the simulation the space shuttle vehicle hardware and related gse hardware is simulated by mathematical models written in shuttle ground operations simulator sgos modeling language that behave and react like real hardware during the shuttle final countdown phase simulation engineers command and control hardware via real application software executing in the control consoles just as if they were commanding real vehicle hardware however these real software applications do not interface with real shuttle hardware during simulations instead the applications interface with mathematical model representations of the vehicle and gse hardware consequently the simulations bypass sensitive and even dangerous mechanisms while providing engineering measurements detailing how the hardware would have reacted since these math models interact with the command and control application software models and simulations are also used to debug and verify the functionality of application software satellite navigation the only true way to test gnss receivers commonly known as sat nav in the commercial world is by using an rf constellation simulator receiver that may for example be used on an aircraft can be tested under dynamic conditions without the need to take it on real flight the test conditions can be repeated exactly and there is full control over all the test parameters this is not possible in the real world using the actual signals for testing receivers that will use the new galileo satellite navigation there is no alternative as the real signals do not yet exist weather predicting weather conditions by extrapolating interpolating previous data is one of the real use of simulation most of the weather forecasts use this information published by weather buereaus this kind of simulations help in predicting and forewarning about extreme weather conditions like the path of an active hurricane cyclone numerical weather prediction for forecasting involves complicated numeric computer models to predict weather accurately by taking many parameters into account simulation games strategy games both traditional and modern may be viewed as simulations of abstracted decision making for the purpose of training military and political leaders see history of go for an example of such tradition or kriegsspiel for more recent example many other video games are simulators of some kind such games can simulate various aspects of reality from business to government to construction to piloting vehicles see above historical usage historically the word had negative connotations however the connection between simulation and dissembling later faded out and is now only of linguistic interest see also references external links bibliographies containing more references to be found on the website of the journal simulation gaming", "Theoretical computer science": "an artistic representation of turing machine turing machines are used to model general computing devices theoretical computer science is division or subset of general computer science and mathematics that focuses on more abstract or mathematical aspects of computing and includes the theory of computation it is not easy to circumscribe the theory areas precisely and the acm special interest group on algorithms and computation theory sigact describes its mission as the promotion of theoretical computer science and notes to this list the acm journal transactions on computation theory adds coding theory computational learning theory and theoretical computer science aspects of areas such as databases information retrieval economic models and networks despite this broad scope the theory people in computer science self identify as different from the applied people some characterize themselves as doing the more fundamental science underlying the field of computing other theory applied people suggest that it is impossible to separate theory and application this means that the so called theory people regularly use experimental science done in less theoretical areas such as software system research it also means that there is more cooperation than mutually exclusive competition between theory and application history while of an algorithm in terms of computation while logical inference and mathematical proof had existed previously in kurt g\u00f6del proved with his incompleteness theorem that there were fundamental limitations on what statements could be proved or disproved these developments have led to the modern study of logic and computability and indeed the field of theoretical computer science as whole information theory was added to the field with mathematical theory of communication by claude shannon in the same decade donald hebb introduced mathematical model of learning in the brain with mounting biological data supporting this hypothesis with some modification the fields of neural networks and parallel distributed processing were established in stephen cook and working independently leonid levin proved that there exist practically relevant problems that are np complete landmark result in computational complexity theory with the development of quantum mechanics in the beginning of the th century came the concept that mathematical operations could be performed on an entire particle wavefunction in other words one could compute functions on multiple states simultaneously this led to the concept of quantum computer in the latter half of the th century that took off in the when peter shor showed that such methods could be used to factor large numbers in polynomial time which if implemented would render most modern public key cryptography systems uselessly insecure modern theoretical computer science research is based on these basic developments but includes many other mathematical and problems that have been posed px px px px np mathematical logic automata theory number theory graph theory computability theory computational complexity theory gnitirw terces px px px px cryptography type theory category theory computational geometry combinatorial optimization quantum computing theory topics algorithms an algorithm is step by step procedure for calculations algorithms are used for calculation data processing and automated reasoning an algorithm is an effective method expressed as finite list of well defined instructions for calculating function starting from an initial state and initial input perhaps empty the instructions describe computation that when executed proceeds through finite number of well defined successive states eventually producing output and terminating at final ending state the transition from one state to the next is not necessarily deterministic some algorithms known as randomized algorithms incorporate random input data structures data structure is particular way of organizing data in computer so that it can be used efficiently different kinds of data structures are suited to different kinds of applications and some are highly specialized to specific tasks for example databases use tree indexes for small percentages of data retrieval and compilers and databases use dynamic hash tables as look up tables data structures provide means to manage large amounts of data efficiently for uses such as large databases and internet indexing services usually efficient data structures are key to designing efficient algorithms some formal design methods and programming languages emphasize data structures rather than algorithms as the key organizing factor in software design storing and retrieving can be carried out on data stored in both main memory and in secondary memory computational complexity theory computational complexity theory is branch of the theory of computation that focuses on classifying computational problems according to their inherent difficulty and relating those classes to each other computational problem is understood to be task that is in principle amenable to being solved by computer which is equivalent to stating that the problem may be solved by mechanical application of mathematical steps such as an algorithm problem is regarded as inherently difficult if its solution requires significant resources whatever the algorithm used the theory formalizes this intuition by introducing mathematical models of computation to study these problems and quantifying the amount of resources needed to solve them such as time and storage other complexity measures are also used such as the amount of communication used in communication complexity the number of gates in circuit used in circuit complexity and the number of processors used in parallel computing one of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do distributed computation distributed computing studies distributed systems distributed system is software system in which components located on networked computers communicate and coordinate their actions by passing messages the components interact with each other in order to achieve common goal three significant characteristics of distributed systems are concurrency of components lack of global clock and independent failure of components examples of distributed systems vary from soa based systems to massively multiplayer online games to peer to peer applications computer program that runs in distributed system is called distributed program and distributed programming is the process of writing such programs there are many alternatives for the message passing mechanism including rpc like connectors and message queues an important goal and challenge of distributed systems is location transparency parallel computation parallel computing is form of computation in which many calculations are carried out simultaneously operating on the principle that large problems can often be divided into smaller ones which are then solved concurrently in parallel there are several different forms of parallel computing bit level instruction level data and task parallelism parallelism has been employed for many years mainly in high performance computing but interest in it has grown lately due to the physical constraints preventing frequency scaling as power consumption and consequently heat generation by computers has become concern in recent years parallel computing has become the dominant paradigm in computer architecture mainly in the form of multi core processors parallel computer programs are more difficult to write than sequential ones because concurrency introduces several new classes of potential software bugs of which race conditions are the most common communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance the maximum possible speed up of single program as result of parallelization is known as amdahl law very large scale integration very large scale integration vlsi is the process of creating an integrated circuit ic by combining thousands of transistors into single chip vlsi began in the when complex semiconductor and communication technologies were being developed the microprocessor is vlsi device before the introduction of vlsi technology most ics had limited set of functions they could perform an electronic circuit might consist of cpu rom ram and other glue logic vlsi lets ic makers add all of these into one chip machine learning machine learning is scientific discipline that deals with the construction and study of algorithms that can learn from data such algorithms operate by building model based on inputs and using that to make predictions or decisions rather than following only explicitly programmed instructions machine learning can be considered subfield of computer science and statistics it has strong ties to artificial intelligence and optimization which deliver methods theory and application domains to the field machine learning is employed in range of computing tasks where designing and programming explicit rule based algorithms is infeasible example applications include spam filtering optical character recognition ocr search engines and computer vision machine learning is sometimes conflated with data mining although that focuses more on exploratory data analysis machine learning and pattern recognition can be viewed as two facets of the same field computational biology computational biology involves the development and application of data analytical and theoretical methods mathematical modeling and computational simulation techniques to the study of biological behavioral and social systems the field is broadly defined and includes foundations in computer science applied mathematics animation statistics biochemistry chemistry biophysics molecular biology genetics genomics ecology evolution anatomy neuroscience and visualization computational biology is different from biological computation which is subfield of computer science and computer engineering using bioengineering and biology to build computers but is similar to bioinformatics which is an science using computers to store and process biological data computational geometry computational geometry is branch of computer science devoted to the study of algorithms that can be stated in terms of geometry some purely geometrical problems arise out of the study of computational geometric algorithms and such problems are also considered to be part of computational geometry while modern computational geometry is recent development it is one of the oldest fields of computing with history stretching back to antiquity an ancient precursor is the sanskrit treatise shulba sutras or rules of the chord that is book of algorithms written in bce the book prescribes step by step procedures for constructing geometric objects like altars using peg and chord the main impetus for the development of computational geometry as discipline was progress in computer graphics and computer aided design and manufacturing cad cam but many problems in computational geometry are classical in nature and may come from mathematical visualization other important applications of computational geometry include robotics motion planning and visibility problems geographic information systems gis geometrical location and search route planning integrated circuit design ic geometry design and verification computer aided engineering cae mesh generation computer vision reconstruction information theory information theory is branch of applied mathematics electrical engineering and computer science involving the quantification of information information theory was developed by claude shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data since its inception it has broadened to find applications in many other areas including statistical inference natural language processing cryptography neurobiology the evolution and function of molecular codes model selection in ecology thermal physics quantum computing linguistics plagiarism detection pattern recognition anomaly detection and other forms of data analysis applications of fundamental topics of information theory include lossless data compression zip files lossy data compression mp and jpegs and channel coding for digital subscriber line dsl the field is at the intersection of mathematics statistics computer science physics neurobiology and electrical engineering its impact has been crucial to the success of the voyager missions to deep space the invention of the compact disc the feasibility of mobile phones the development of the internet the study of linguistics and of human perception the understanding of black holes and numerous other fields important sub fields of information theory are source coding channel coding algorithmic complexity theory algorithmic information theory information theoretic security and measures of information cryptography cryptography is the practice and study of techniques for secure communication in the presence of third parties called adversaries more generally it is about constructing and analyzing protocols that overcome the influence of adversaries and that are related to various aspects in information security such as data confidentiality data integrity authentication and non repudiation modern cryptography intersects the disciplines of mathematics computer science and electrical engineering applications of cryptography include atm cards computer passwords and electronic commerce modern cryptography is heavily based on mathematical theory and computer science practice cryptographic algorithms are designed around computational hardness assumptions making such algorithms hard to break in practice by any adversary it is theoretically possible to break such system but it is infeasible to do so by any known practical means these schemes are therefore termed computationally secure theoretical advances improvements in integer factorization algorithms and faster computing technology require these solutions to be continually adapted there exist information theoretically secure schemes that cannot be broken even with unlimited computing power an example is the one time pad but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms quantum computation quantum computer is computation system that makes direct use of quantum mechanical phenomena such as superposition and entanglement to perform operations on data quantum computers are different from digital computers based on transistors whereas digital computers require data to be encoded into binary digits bits each of which is always in one of two definite states or quantum computation uses qubits quantum bits which can be in superpositions of states theoretical model is the quantum turing machine also known as the universal quantum computer quantum computers share theoretical similarities with non deterministic and probabilistic computers one example is the ability to be in more than one state simultaneously the field of quantum computing was first introduced by yuri manin in and richard feynman in quantum computer with spins as quantum bits was also formulated for use as quantum space time in quantum computing is still in its infancy but experiments have been carried out in which quantum computational operations were executed on very small number of qubits both practical and theoretical research continues and many national governments and military funding agencies support quantum computing research to develop quantum computers for both civilian and national security purposes such as cryptanalysis computational number theory computational number theory also known as algorithmic number theory is the study of algorithms for performing number theoretic computations the best known problem in the field is integer factorization symbolic computation computer algebra also called symbolic computation or algebraic computation is scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects although properly speaking computer algebra should be subfield of scientific computing they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers while symbolic computation emphasizes exact computation with expressions containing variables that have not any given value and are thus manipulated as symbols therefore the name of symbolic computation software applications that perform symbolic calculations are called computer algebra systems with the term system alluding to the complexity of the main applications that include at least method to represent mathematical data in computer user programming language usually different from the language used for the implementation dedicated memory manager user interface for the input output of mathematical expressions large set of routines to perform usual operations like simplification of expressions differentiation using chain rule polynomial factorization indefinite integration etc program semantics in programming language theory semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages it does so by evaluating the meaning of syntactically legal strings defined by specific programming language showing the computation involved in such case that the evaluation would be of syntactically illegal strings the result would be non computation semantics describes the processes computer follows when executing program in that specific language this can be shown by describing the relationship between the input and output of program or an explanation of how the program will execute on certain platform hence creating model of computation formal methods formal methods are particular kind of mathematics based techniques for the specification development and verification of software and hardware systems the use of formal methods for software and hardware design is motivated by the expectation that as in other engineering disciplines performing appropriate mathematical analysis can contribute to the reliability and robustness of design formal methods are best described as the application of fairly broad variety of theoretical computer science fundamentals in particular logic calculi formal languages automata theory and program semantics but also type systems and algebraic data types to problems in software and hardware specification and verification automata theory automata theory is the study of abstract machines and automata as well as the computational problems that can be solved using them it is theory in theoretical computer science under discrete mathematics section of mathematics and also of computer science automata comes from the greek word \u03b1\u1f50\u03c4\u03cc\u03bc\u03b1\u03c4\u03b1 meaning self acting automata theory is the study of self operating virtual machines to help in logical understanding of input and output process without or with intermediate stage of computation or any function process coding theory coding theory is the study of the properties of codes and their fitness for specific application codes are used for data compression cryptography error correction and more recently also for network coding codes are studied by various scientific disciplines such as information theory electrical engineering mathematics and computer science for the purpose of designing efficient and reliable data transmission methods this typically involves the removal of redundancy and the correction or detection of errors in the transmitted data computational learning theory theoretical results in machine learning mainly deal with type of inductive learning called supervised learning in supervised learning an algorithm is given samples that are labeled in some useful way for example the samples might be descriptions of mushrooms and the labels could be whether or not the mushrooms are edible the algorithm takes these previously labeled samples and uses them to induce classifier this classifier is function that assigns labels to samples including the samples that have never been previously seen by the algorithm the goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples organizations european association for theoretical computer science sigact journals and newsletters information and computation theory of computing open access journal formal aspects of computing journal of the acm siam journal on computing sicomp sigact news theoretical computer science theory of computing systems international journal of foundations of computer science chicago journal of theoretical computer science open access journal foundations and trends in theoretical computer science journal of automata languages and combinatorics acta informatica fundamenta informaticae acm transactions on computation theory computational complexity acm transactions on algorithms information processing letters conferences annual acm symposium on theory of computing stoc annual ieee symposium on foundations of computer science focs acm siam symposium on discrete algorithms soda annual symposium on computational geometry socg international colloquium on automata languages and programming icalp symposium on theoretical aspects of computer science stacs international conference on theory and applications of models of computation tamc european symposium on algorithms esa ieee symposium on logic in computer science lics international symposium on algorithms and computation isaac workshop on approximation algorithms for combinatorial optimization problems approx workshop on randomization and computation random computational complexity conference ccc acm symposium on parallelism in algorithms and architectures spaa acm symposium on principles of distributed computing podc international symposium on fundamentals of computation theory fct annual conference on learning theory colt international workshop on graph theoretic concepts in computer science wg see also formal science unsolved problems in computer science list of important publications in theoretical computer science notes further reading martin davis ron sigal elaine weyuker computability complexity and languages fundamentals of theoretical computer science nd ed academic press isbn covers theory of computation but also program semantics and quantification theory aimed at graduate students external links sigact directory of additional theory links theory matters wiki theoretical computer science tcs advocacy wiki usenet comp theory list of academic conferences in the area of theoretical computer science at confsearch theoretical computer science stackexchange question and answer site for researchers in theoretical computer science computer science animated http theory csail mit edu massachusetts institute of technology", "Speech recognition": "in computer science and electrical engineering speech recognition sr is the translation of spoken words into text it is also known as automatic speech recognition asr computer speech recognition or just speech to text stt some sr systems use training also called enrolment where an individual speaker reads text or isolated vocabulary into the system the system analyzes the person specific voice and uses it to fine tune the recognition of that person speech resulting in increased accuracy systems that do not use training are called speaker independent systems systems that use training are called speaker dependent speech recognition applications include voice user interfaces such as voice dialling call home call routing would like to make collect call domotic appliance control search find podcast where particular words were spoken simple data entry entering credit card number preparation of structured documents radiology report speech to text processing word processors or emails and aircraft usually termed direct voice input the term voice recognition or speaker identification refers to identifying the speaker rather than what they are saying recognizing the speaker can simplify the task of translating speech in systems that have been trained on specific person voice or it can be used to authenticate or verify the identity of speaker as part of security process from the technology perspective speech recognition has long history with several waves of major innovations most recently the field has benefited from advances in deep learning and big data the advances are evidenced not only by the surge of academic papers published in the field but more importantly by the world wide industry adoption of variety of deep learning methods in designing and deploying speech recognition systems these speech industry players include microsoft google ibm baidu china apple amazon nuance iflytek china many of which have publicized the core technology in their speech recognition systems being based on deep learning history as early as bell labs researchers like harvey fletcher were investigating the science of speech perception in three bell labs researchers built system for single speaker digit recognition their system worked by locating the formants in the power spectrum of each utterance the era technology was limited to single speaker systems with vocabularies of around ten words unfortunately funding at bell labs dried up for several years when in the influential john pierce wrote an open letter that was critical of speech recognition research pierce letter compared speech recognition to schemes for turning water into gasoline extracting gold from the sea curing cancer or going to the moon pierce defunded speech recognition research at bell labs raj reddy was the first person to take on continuous speech recognition as graduate student at stanford university in the late previous systems required the users to make pause after each word reddy system was designed to issue spoken commands for the game of chess also around this time soviet researchers invented the dynamic time warping algorithm and used it to create recognizer capable of operating on word vocabulary achieving speaker independence was major unsolved goal of researchers during this time period in darpa funded five years of speech recognition research through its speech understanding research program with ambitious end goals including minimum vocabulary size of words bbn ibm carnegie mellon and stanford research institute all participated in the program the government funding revived speech recognition research that had been largely abandoned in the united states after john pierce letter despite the fact that cmu harpy system met the goals established at the outset of the program many of the predictions turned out to be nothing more than hype disappointing darpa administrators this disappointment led to darpa not continuing the funding several innovations happened during this time such as the invention of beam search for use in cmu harpy system the field also benefited from the discovery of several algorithms in other fields such as linear predictive coding and cepstral analysis during the late leonard baum developed the mathematics of markov chains at the institute for defense analysis at cmu raj reddy student james baker and his wife janet baker began using the hidden markov model hmm for speech recognition james baker had learned about hmms from summer job at the institute of defense analysis during his undergraduate education the use of hmms allowed researchers to combine different sources of knowledge such as acoustics language and syntax in unified probabilistic model under fred jelinek lead ibm created voice activated typewriter called tangora which could handle word vocabulary by the mid jelinek statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like hmms jelinek group independently discovered the application of hmms to speech this was controversial with linguists since hmms are too simplistic to account for many common features of human languages however the hmm proved to be highly useful way for modeling speech and replaced dynamic time warping to become the dominate speech recognition algorithm in the ibm had few competitors including dragon systems founded by james and janet baker in the also saw the introduction of the gram language model much of the progress in the field is owed to the rapidly increasing capabilities of computers at the end of the darpa program in the best computer available to researchers was the pdp with mb ram few decades later researchers had access to tens of thousands of times as much computing power as the technology advanced and computers got faster researchers began tackling harder problems such as larger vocabularies speaker independence noisy environments and conversational speech in particular this shifting to more difficult tasks has characterized darpa funding of speech recognition since the for example progress was made on speaker independence first by training on larger variety of speakers and then later by doing explicit speaker adaptation during decoding further reductions in word error rate came as researchers shifted acoustic models to be discriminative instead of using maximum likelihood models another one of raj reddy former students xuedong huang developed the sphinx ii system at cmu the sphinx ii system was the first to do speaker independent large vocabulary continuous speech recognition and it had the best performance in darpa evaluation huang went on to found the speech recognition group at microsoft in the saw the first introduction of commercially successful speech recognition technologies by this point the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary in lernout hauspie acquired dragon systems and was an industry leader until an accounting scandal brought an end to the company in the speech technology was bought by scansoft which became nuance in apple originally licensed software from nuance to provide speech recognition capability to its digital assistant siri in the darpa sponsored two speech recognition programs effective affordable reusable speech to text ears in and global autonomous language exploitation gale four teams participated in the ears program ibm bbn cambridge university and team composed of isci sri and university of washington the gale program focused on mandarin broadcast news speech google first effort at speech recognition came in after hiring some researchers from nuance the first product was goog telephone based directory service the recordings from goog produced valuable data that helped google improve their recognition systems google voice search is now supported in over languages in the united states the national security agency has made use of type of speech recognition for keyword spotting since at least this technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords recordings can be indexed and analysts can run queries over the database to find conversations of interest some government research programs focused on intelligence applications of speech recognition darpa ears program and iarpa babel program the use of deep learning for acoustic modeling was introduced during later part of by geoffrey hinton and his students at university of toronto and by li deng and colleagues at microsoft research initially in the collaborative work between microsoft and university of toronto which was subsequently expanded to include ibm and google hence the shared views of four research groups subtitle in their review paper microsoft research executive called this innovation the most dramatic change in accuracy since in contrast to the steady incremental improvements of the past few decades the application of deep learning decreased word error rate by this innovation was quickly adopted across the field researchers have begun to use deep learning techniques for language modeling as well in the long history of speech recognition both shallow form and deep form recurrent nets of artificial neural networks had been explored for many years during and few years into but these methods never won over the non uniform internal handcrafting gaussian mixture model hidden markov model gmm hmm technology based on generative models of speech trained number of key difficulties had been analyzed in the including gradient diminishing and weak temporal correlation structure in the neural predictive models all these difficulties were in addition to the lack of big training data and big computing power in these early days most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around that had overcome all these difficulties hinton et al and deng et al reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups university of toronto microsoft google and ibm ignited the renaissance of neural networks and initiated deep learning research and applications in speech recognition models methods and algorithms both acoustic modeling and language modeling are important parts of modern statistically based speech recognition algorithms hidden markov models hmms are widely used in many systems language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation hidden markov models modern general purpose speech recognition systems are based on hidden markov models these are statistical models that output sequence of symbols or quantities hmms are used in speech recognition because speech signal can be viewed as piecewise stationary signal or short time stationary signal in short time scale milliseconds speech can be approximated as stationary process speech can be thought of as markov model for many stochastic purposes another reason why hmms are popular is because they can be trained automatically and are simple and computationally feasible to use in speech recognition the hidden markov model would output sequence of dimensional real valued vectors with being small integer such as outputting one of these every milliseconds the vectors would consist of cepstral coefficients which are obtained by taking fourier transform of short time window of speech and decorrelating the spectrum using cosine transform then taking the first most significant coefficients the hidden markov model will tend to have in each state statistical distribution that is mixture of diagonal covariance gaussians which will give likelihood for each observed vector each word or for more general speech recognition systems each phoneme will have different output distribution hidden markov model for sequence of words or phonemes is made by concatenating the individual trained hidden markov models for the separate words and phonemes described above are the core elements of the most common hmm based approach to speech recognition modern speech recognition systems use various combinations of number of standard techniques in order to improve results over the basic approach described above typical large vocabulary system would need context dependency for the phonemes so phonemes with different left and right context have different realizations as hmm states it would use cepstral normalization to normalize for different speaker and recording conditions for further speaker normalization it might use vocal tract length normalization vtln for male female normalization and maximum likelihood linear regression mllr for more general speaker adaptation the features would have so called delta and delta delta coefficients to capture speech dynamics and in addition might use heteroscedastic linear discriminant analysis hlda or might skip the delta and delta delta coefficients and use splicing and an lda based projection followed perhaps by heteroscedastic linear discriminant analysis or global semi tied co variance transform also known as maximum likelihood linear transform or mllt many systems use so called discriminative training techniques that dispense with purely statistical approach to hmm parameter estimation and instead optimize some classification related measure of the training data examples are maximum mutual information mmi minimum classification error mce and minimum phone error mpe decoding of the speech the term for what happens when the system is presented with new utterance and must compute the most likely source sentence would probably use the viterbi algorithm to find the best path and here there is choice between dynamically creating combination hidden markov model which includes both the acoustic and language model information and combining it statically beforehand the finite state transducer or fst approach possible improvement to decoding is to keep set of good candidates instead of just keeping the best candidate and to use better scoring function re scoring to rate these good candidates so that we may pick the best one according to this refined score the set of candidates can be kept either as list the best list approach or as subset of the models lattice re scoring is usually done by trying to minimize the bayes risk or an approximation thereof instead of taking the source sentence with maximal probability we try to take the sentence that minimizes the expectancy of given loss function with regards to all possible transcriptions we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability the loss function is usually the levenshtein distance though it can be different distances for specific tasks the set of possible transcriptions is of course pruned to maintain tractability efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as finite state transducer verifying certain assumptions dynamic time warping dtw based speech recognition dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful hmm based approach dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed for instance similarities in walking patterns would be detected even if in one video the person was walking slowly and if in another he or she were walking more quickly or even if there were accelerations and deceleration during the course of one observation dtw has been applied to video audio and graphics indeed any data that can be turned into linear representation can be analyzed with dtw well known application has been automatic speech recognition to cope with different speaking speeds in general it is method that allows computer to find an optimal match between two given sequences time series with certain restrictions that is the sequences are warped non linearly to match each other this sequence alignment method is often used in the context of hidden markov models neural networks neural networks emerged as an attractive acoustic modeling approach in asr in the late since then neural networks have been used in many aspects of speech recognition such as phoneme classification isolated word recognition and speaker adaptation in contrast to hmms neural networks make no assumptions about feature statistical properties and have several qualities making them attractive recognition models for speech recognition when used to estimate the probabilities of speech feature segment neural networks allow discriminative training in natural and efficient manner few assumptions on the statistics of input features are made with neural networks however in spite of their effectiveness in classifying short time units such as individual phones and isolated words neural networks are rarely successful for continuous recognition tasks largely because of their lack of ability to model temporal dependencies however recently recurrent neural networks rnn and time delay neural networks tdnn have been used which have been shown to be able to identify latent temporal dependencies and use this information to perform the task of speech recognition this however enormously increases the computational cost involved and hence makes the process of speech recognition slower lot of research is still going on in this field to ensure that tdnn and rnn can be used in more computationally affordable way to improve the speech recognition accuracy immensely deep neural networks and denoising autoencoders are also being experimented with to tackle this problem in an effective manner due to the inability of traditional neural networks to model temporal dependencies an alternative approach is to use neural networks as pre processing feature transformation dimensionality reduction for the hmm based recognition deep neural networks and other deep learning models deep neural network dnn is an artificial neural network with multiple hidden layers of units between the input and output layers similar to shallow neural networks dnns can model complex non linear relationships dnn architectures generate compositional models where extra layers enable composition of features from lower layers giving huge learning capacity and thus the potential of modeling complex patterns of speech data the dnn is the most popular type of deep learning architectures successfully used as an acoustic model for speech recognition since the success of dnns in large vocabulary speech recognition occurred in by industrial researchers in collaboration with academic researchers where large output layers of the dnn based on context dependent hmm states constructed by decision trees were adopted see comprehensive reviews of this development and of the state of the art as of october in the recent springer book from microsoft research see also the related background of automatic speech recognition and the impact of various machine learning paradigms including notably deep learning in recent overview article one fundamental principle of deep learning is to do away with hand crafted feature engineering and to use raw features this principle was first explored successfully in the architecture of deep autoencoder on the raw spectrogram or linear filter bank features showing its superiority over the mel cepstral features which contain few stages of fixed transformation from spectrograms the true raw features of speech waveforms have more recently been shown to produce excellent larger scale speech recognition results since the initial successful debut of dnns for speech recognition around there have been huge new progresses made this progress as well as future directions has been summarized into the following eight major areas scaling up out and speedup dnn training and decoding sequence discriminative training of dnns feature processing by deep models with solid understanding of the underlying mechanisms adaptation of dnns and of related deep models multi task and transfer learning by dnns and related deep models convolution neural networks and how to design them to best exploit domain knowledge of speech recurrent neural network and its rich lstm variants other types of deep models including tensor based models and integrated deep generative discriminative models large scale automatic speech recognition is the first and the most convincing successful case of deep learning in the recent history embraced by both industry and academic across the board between and the two major conferences on signal processing and speech recognition ieee icassp and interspeech have seen near exponential growth in the numbers of accepted papers in their respective annual conference papers on the topic of deep learning for speech recognition more importantly all major commercial speech recognition systems microsoft cortana xbox skype translator google now apple siri baidu and iflytek voice search and range of nuance speech products etc nowadays are based on deep learning methods see also the recent media interview with the cto of nuance communications applications in car systems typically manual control input for example by means of finger control on the steering wheel enables the speech recognition system and this is signalled to the driver by an audio prompt following the audio prompt the system has listening window during which it may accept speech input for recognition simple voice commands may be used to initiate phone calls select radio stations or play music from compatible smartphone mp player or music loaded flash drive voice recognition capabilities vary between car make and model some of the most recent car models offer natural language speech recognition in place of fixed set of commands allowing the driver to use full sentences and common phrases with such systems there is therefore no need for the user to memorize set of fixed command words health care medical documentation in the health care sector speech recognition can be implemented in front end or back end of the medical documentation process front end speech recognition is where the provider dictates into speech recognition engine the recognized words are displayed as they are spoken and the dictator is responsible for editing and signing off on the document back end or deferred speech recognition is where the provider dictates into digital dictation system the voice is routed through speech recognition machine and the recognized draft document is routed along with the original voice file to the editor where the draft is edited and report finalised deferred speech recognition is widely used in the industry currently one of the major issues relating to the use of speech recognition in healthcare is that the american recovery and reinvestment act of arra provides for substantial financial benefits to physicians who utilize an emr according to meaningful use standards these standards require that substantial amount of data be maintained by the emr now more commonly referred to as an electronic health record or ehr the use of speech recognition is more naturally suited to the generation of narrative text as part of radiology pathology interpretation progress note or discharge summary the ergonomic gains of using speech recognition to enter structured discrete data numeric values or codes from list or controlled vocabulary are relatively minimal for people who are sighted and who can operate keyboard and mouse more significant issue is that most ehrs have not been expressly tailored to take advantage of voice recognition capabilities large part of the clinician interaction with the ehr involves navigation through the user interface using menus and tab button clicks and is heavily dependent on keyboard and mouse voice based navigation provides only modest ergonomic benefits by contrast many highly customized systems for radiology or pathology dictation implement voice macros where the use of certain phrases normal report will automatically fill in large number of default values and or generate boilerplate which will vary with the type of the exam chest ray vs contrast series for radiology system as an alternative to this navigation by hand cascaded use of speech recognition and information extraction has been studied as way to fill out handover form for clinical proofing and sign off the results are encouraging and the paper also opens data together with the related performance benchmarks and some processing software to the research and development community for studying clinical documentation and language processing therapeutic use prolonged use of speech recognition software in conjunction with word processors has shown benefits to short term memory restrengthening in brain avm patients who have been treated with resection further research needs to be conducted to determine cognitive benefits for individuals whose avms have been treated using radiologic techniques military high performance fighter aircraft substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft of particular note is the program in speech recognition for the advanced fighter technology integration afti aircraft vista and program in france installing speech recognition systems on mirage aircraft and also programs in the uk dealing with variety of aircraft platforms in these programs speech recognizers have been operated successfully in fighter aircraft with applications including setting radio frequencies commanding an autopilot system setting steer point coordinates and weapons release parameters and controlling flight display working with swedish pilots flying in the jas gripen cockpit englund found recognition deteriorated with increasing loads it was also concluded that adaptation greatly improved the results in all cases and introducing models for breathing was shown to improve recognition scores significantly contrary to what might be expected no effects of the broken english of the speakers were found it was evident that spontaneous speech caused problems for the recognizer as could be expected restricted vocabulary and above all proper syntax could thus be expected to improve recognition accuracy substantially the eurofighter typhoon currently in service with the uk raf employs speaker dependent system it requires each pilot to create template the system is not used for any safety critical or weapon critical tasks such as weapon release or lowering of the undercarriage but is used for wide range of other cockpit functions voice commands are confirmed by visual and or aural feedback the system is seen as major design feature in the reduction of pilot workload and even allows the pilot to assign targets to himself with two simple voice commands or to any of his wingmen with only five commands speaker independent systems are also being developed and are in testing for the lightning ii jsf and the alenia aermacchi master lead in fighter trainer these systems have produced word accuracies in excess of helicopters the problems of achieving high recognition accuracy under stress and noise pertain strongly to the helicopter environment as well as to the jet fighter environment the acoustic noise problem is actually more severe in the helicopter environment not only because of the high noise levels but also because the helicopter pilot in general does not wear facemask which would reduce acoustic noise in the microphone substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters notably by the army avionics research and development activity avrada and by the royal aerospace establishment rae in the uk work in france has included speech recognition in the puma helicopter there has also been much useful work in canada results have been encouraging and voice applications have included control of communication radios setting of navigation systems and control of an automated target handover system as in fighter applications the overriding issue for voice in helicopters is the impact on pilot effectiveness encouraging results are reported for the avrada tests although these represent only feasibility demonstration in test environment much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings training air traffic controllers training for air traffic controllers atc represents an excellent application for speech recognition systems many atc training systems currently require person to act as pseudo pilot engaging in voice dialog with the trainee controller which simulates the dialog that the controller would have to conduct with pilots in real atc situation speech recognition and synthesis techniques offer the potential to eliminate the need for person to act as pseudo pilot thus reducing training and support personnel in theory air controller tasks are also characterized by highly structured speech as the primary output of the controller hence reducing the difficulty of the speech recognition task should be possible in practice this is rarely the case the faa document details the phrases that should be used by air traffic controllers while this document gives less than examples of such phrases the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of the usaf usmc us army us navy and faa as well as number of international atc training organizations such as the royal australian air force and civil aviation authorities in italy brazil and canada are currently using atc simulators with speech recognition from number of different vendors telephony and other domains asr in the field of telephony is now commonplace and in the field of computer gaming and simulation is becoming more widespread despite the high level of integration with word processing in general personal computing however asr in the field of document production has not seen the expected increases in use the improvement of mobile processor speeds made feasible the speech enabled symbian and windows mobile smartphones speech is used mostly as part of user interface for creating predefined or custom speech commands leading software vendors in this field are google microsoft corporation microsoft voice command digital syphon sonic extractor lumenvox nuance communications nuance voice control voicebox technology speech technology center vito technologies vito voice go speereo software speereo voice translator verbyx vrx and svox usage in education and daily life for language learning speech recognition can be useful for learning second language it can teach proper pronunciation in addition to helping person develop fluency with their speaking skills students who are blind see blindness and education or have very low vision can benefit from using the technology to convey words and then hear the computer recite them as well as use computer by commanding with their voice instead of having to look at the screen and keyboard students who are physically disabled or suffer from repetitive strain injury other injuries to the upper extremities can be relieved from having to worry about handwriting typing or working with scribe on school assignments by using speech to text programs they can also utilize speech recognition technology to freely enjoy searching the internet or using computer at home without having to physically operate mouse and keyboard speech recognition can allow students with learning disabilities to become better writers by saying the words aloud they can increase the fluidity of their writing and be alleviated of concerns regarding spelling punctuation and other mechanics of writing also see learning disability voice recognition software use in conjunction with digital audio recorder personal computer and microsoft word has proven to be positive for restoring damaged short term memory capacity in stroke and craniotomy individuals people with disabilities people with disabilities can benefit from speech recognition programs for individuals that are deaf or hard of hearing speech recognition software is used to automatically generate closed captioning of conversations such as discussions in conference rooms classroom lectures and or religious services speech recognition is also very useful for people who have difficulty using their hands ranging from mild repetitive stress injuries to involved disabilities that preclude using conventional computer input devices in fact people who used the keyboard lot and developed rsi became an urgent early market for speech recognition speech recognition is used in deaf telephony such as voicemail to text relay services and captioned telephone individuals with learning disabilities who have problems with thought to paper communication essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper can possibly benefit from the software but the technology is not bug proof also the whole idea of speak to text can be hard for intellectually disabled person due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability this type of technology can help those with dyslexia but other disabilities are still in question the effectiveness of the product is the problem that is hindering it being effective although kid may be able to say word depending on how clear they say it the technology may think they are saying another word and input the wrong one giving them more work to fix causing them to have to take more time with fixing the wrong word further applications aerospace space exploration spacecraft etc nasa mars polar lander used speech recognition from technology sensory inc in the mars microphone on the lander automatic subtitling with speech recognition automatic translation court reporting realtime speech writing hands free computing speech recognition computer user interface home automation interactive voice response mobile telephony including mobile email multimodal interaction pronunciation evaluation in computer aided language learning applications robotics speech to text reporter transcription of speech into text video captioning court reporting telematics vehicle navigation systems transcription digital speech to text video games with tom clancy endwar and lifeline as working examples performance the performance of speech recognition systems is usually evaluated in terms of accuracy and speed accuracy is usually rated with word error rate wer whereas speed is measured with the real time factor other measures of accuracy include single word error rate swer and command success rate csr however speech recognition by machine is very complex problem vocalizations vary in terms of accent pronunciation articulation roughness nasality pitch volume and speed speech is distorted by background noise and echoes electrical characteristics accuracy of speech recognition vary with the following vocabulary size and confusability speaker dependence vs independence isolated discontinuous or continuous speech task and language constraints read vs spontaneous speech adverse conditions accuracy as mentioned earlier in this article accuracy of speech recognition varies in the following error rates increase as the vocabulary size grows the digits zero to nine can be recognized essentially perfectly but vocabulary sizes of or may have error rates of or respectively vocabulary is hard to recognize if it contains confusable words the letters of the english alphabet are difficult to discriminate because they are confusable words most notoriously the set an error rate is considered good for this vocabulary speaker dependence vs independence speaker dependent system is intended for use by single speaker speaker independent system is intended for use by any speaker more difficult isolated discontinuous or continuous speech with isolated speech single words are used therefore it becomes easier to recognize the speech with discontinuous speech full sentences separated by silence are used therefore it becomes easier to recognize the speech as well as with isolated speech with continuous speech naturally spoken sentences are used therefore it becomes harder to recognize the speech different from both isolated and discontinuous speech task and language constraints querying application may dismiss the hypothesis the apple is red constraints may be semantic rejecting the apple is angry syntactic rejecting red is apple the constraints are often represented by grammar read vs spontaneous speech when person reads it usually in context that has been previously prepared but when person uses spontaneous speech it is difficult to recognize the speech because of the disfluencies like uh and um false starts incomplete sentences stuttering coughing and laughter and limited vocabulary adverse conditions environmental noise noise in car or factory acoustical distortions echoes room acoustics speech recognition is multi levelled pattern recognition task acoustical signals are structured into hierarchy of units phonemes words phrases and sentences each level provides additional constraints known word pronunciations or legal word sequences which can compensate for errors or uncertainties at lower level this hierarchy of constraints are exploited by combining decisions at all lower levels and making more deterministic decisions only at the highest level speech recognition by machine is process broken into several phases computationally it is problem in which sound pattern has to be recognized or classified into category that represents meaning to human every acoustic signal can be broken in smaller more basic sub signals as the more complex sound signal is broken into the smaller sub sounds different levels are created where at the top level we have complex sounds which are made of simpler sounds on lower level and going to lower levels even more we create more basic and shorter and simpler sounds the lowest level where the sounds are the most fundamental machine would check for simple and more probabilistic rules of what sound should represent once these sounds are put together into more complex sound on upper level new set of more deterministic rules should predict what new complex sound should represent the most upper level of deterministic rule should figure out the meaning of complex expressions in order to expand our knowledge about speech recognition we need to take into consideration neural networks there are four steps of neural network approaches digitize the speech that we want to recognize for telephone speech the sampling rate is samples per second compute features of spectral domain of the speech with fourier transform computed every ms with one ms section called frame analysis of four step neural network approaches can be explained by further information sound is produced by air or some other medium vibration which we register by ears but machines by receivers basic sound creates wave which has descriptions amplitude how strong is it and frequency how often it vibrates per second the sound waves can be digitized sample strength at short intervals like in picture above to get bunch of numbers that approximate at each time step the strength of wave collection of these numbers represent analog wave this new wave is digital sound waves are complicated because they superimpose one on top of each other like the waves would this way they create odd looking waves for example if there are two waves that interact with each other we can add them which creates new odd looking wave neural network classifies features into phonetic based categories given basic sound blocks that machine digitized one has bunch of numbers which describe wave and waves describe words each frame has unit block of sound which are broken into basic sound waves and represented by numbers which after fourier transform can be statistically evaluated to set to which class of sounds it belongs the nodes in the figure on slide represent feature of sound in which feature of wave from the first layer of nodes to the second layer of nodes based on statistical analysis this analysis depends on programmer instructions at this point second layer of nodes represents higher level features of sound input which is again statistically evaluated to see what class they belong to last level of nodes should be output nodes that tell us with high probability what original sound really was search to match the neural network output scores for the best word to determine the word that was most likely uttered in kurzweil applied intelligence and dragon systems released speech recognition products by kurzweil software had vocabulary of words if uttered one word at time two years later in its lexicon reached words entering the realm of human vocabularies which range from to words but recognition accuracy was only in two years later the error rate crossed below dragon systems released naturally speaking in which recognized normal human speech progress mainly came from improved computer performance and larger source text databases the brown corpus was the first major database available containing several million words carnegie mellon university researchers found no significant increase in recognition accuracy further information conferences and journals popular speech recognition conferences held each year or two include speechtek and speechtek europe icassp interspeech eurospeech and the ieee asru conferences in the field of natural language processing such as acl naacl emnlp and hlt are beginning to include papers on speech processing important journals include the ieee transactions on speech and audio processing later renamed ieee transactions on audio speech and language processing and since sept renamed ieee acm transactions on audio speech and language processing after merging with an acm publication computer speech and language and speech communication books books like fundamentals of speech recognition by lawrence rabiner can be useful to acquire basic knowledge but may not be fully up to date another good source can be statistical methods for speech recognition by frederick jelinek and spoken language processing by xuedong huang etc more up to date are computer speech by manfred schroeder second edition published in and speech processing dynamic and optimization oriented approach published in by li deng and doug shaughnessey the recently updated textbook of speech and language processing by jurafsky and martin presents the basics and the state of the art for asr speaker recognition also uses the same features most of the same front end processing and classification techniuqes as is done in speech recognition most recent comprehensive textbook fundamentals of speaker recognition by homayoon beigi is an in depth source for up to date details on the theory and practice good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by darpa the largest speech recognition related project ongoing as of is the gale project which involves both speech recognition and translation components good and accessible introduction to speech recognition technology and its history is provided by the general audience book the voice in the machine building computers that understand speech by roberto pieraccini the most recent book on speech recognition is automatic speech recognition deep learning approach publisher springer written by yu and deng published near the end of with highly mathematically oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on dnns and related deep learning methods related book published earlier in deep learning methods and applications by deng and yu provides less technical but more methodology focused overview of dnn based speech recognition during placed within the more general context of deep learning applications including not only speech recognition but also image recognition natural language processing information retrieval multimodal processing and multitask learning software in terms of freely available resources carnegie mellon university sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting another resource free but copyrighted is the htk book and the accompanying htk toolkit the at libraries grm and dcd are also general software libraries for large vocabulary speech recognition for more recent and state of the art techniques kaldi toolkit can be used for more software resources see list of speech recognition software see also ai effect alpac applications of artificial intelligence articulatory speech recognition audio mining audio visual speech recognition automatic language translator cache language model google voice search jott keyword spotting kinect mondegreen multimedia information retrieval origin of speech phonetic search technology silvia speaker diarisation speech analytics speech interface guideline speech recognition software for linux speech verification voicexml voxforge windows speech recognition yap software lists list of emerging technologies outline of artificial intelligence references further reading external links signer beat and hoste lode speeg speech and gesture based interface for efficient controller free text entry in proceedings of icmi th international conference on multimodal interaction sydney australia december", "Parallel computing": "ibm blue gene massively parallel supercomputer parallel computing is form type of computation in which many calculations are carried out simultaneously operating on the principle that large problems can often be divided into smaller ones which are then solved at the same time there are several different forms of parallel computing bit level instruction level data and task parallelism parallelism has been employed for many years mainly in high performance computing but interest in it has grown lately due to the physical constraints preventing frequency scaling as power consumption and consequently heat generation by computers has become concern in recent years parallel computing has become the dominant paradigm in computer architecture mainly in the form of multi core processors parallel computing is closely related to concurrent computing they are frequently used together and often conflated though the two are distinct it is possible to have parallelism without concurrency such as bit level parallelism and concurrency without parallelism such as multitasking by time sharing on single core cpu in parallel computing computational task is typically broken down in several often many very similar subtasks that can be processed independently and whose results are combined afterwards upon completion in contrast in concurrent computing the various processes often do not address related tasks when they do as is typical in distributed computing the separate tasks may have varied nature and often require some inter process communication during execution parallel computers can be roughly classified according to the level at which the hardware supports parallelism with multi core and multi processor computers having multiple processing elements within single machine while clusters mpps and grids use multiple computers to work on the same task specialized parallel computer architectures are sometimes used alongside traditional processors for accelerating specific tasks in some cases parallelism is transparent to the programmer such as in bit level or instruction level parallelism but explicitly parallel algorithms particularly those that use concurrency are more difficult to write than sequential ones because concurrency introduces several new classes of potential software bugs of which race conditions are the most common communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting good parallel program performance theoretical upper bound on the speed up of single program as result of parallelization is given by amdahl law background traditionally computer software has been written for serial computation to solve problem an algorithm is constructed and implemented as serial stream of instructions these instructions are executed on central processing unit on one computer only one instruction may execute at time after that instruction is finished the next is executed parallel computing on the other hand uses multiple processing elements simultaneously to solve problem this is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others the processing elements can be diverse and include resources such as single computer with multiple processors several networked computers specialized hardware or any combination of the above frequency scaling was the dominant reason for improvements in computer performance from the mid until the runtime of program is equal to the number of instructions multiplied by the average time per instruction maintaining everything else constant increasing the clock frequency decreases the average time it takes to execute an instruction an increase in frequency thus decreases runtime for all compute bound programs however power consumption by chip is given by the equation where is power is the capacitance being switched per clock cycle proportional to the number of transistors whose inputs change is voltage and is the processor frequency cycles per second increases in frequency increase the amount of power used in processor increasing processor power consumption led ultimately to intel may cancellation of its tejas and jayhawk processors which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm moore law is the empirical observation that transistor density in microprocessor doubles every to months despite power consumption issues and repeated predictions of its end moore law is still in effect with the end of frequency scaling these additional transistors which are no longer used for frequency scaling can be used to add extra hardware for parallel computing amdahl law and gustafson law graphical representation of amdahl law the speed up of program from parallelization is limited by how much of the program can be parallelized for example if of the program can be parallelized the theoretical maximum speed up using parallel computing would be no matter how many processors are used optimally the speed up from parallelization would be linear doubling the number of processing elements should halve the runtime and doubling it second time should again halve the runtime however very few parallel algorithms achieve optimal speed up most of them have near linear speed up for small numbers of processing elements which flattens out into constant value for large numbers of processing elements the potential speed up of an algorithm on parallel computing platform is given by amdahl law originally formulated by gene amdahl in the it states that small portion of the program which cannot be parallelized will limit the overall speed up available from parallelization program solving large mathematical or engineering problem will typically consist of several parallelizable parts and several non parallelizable sequential parts if is the fraction of running time program spends on non parallelizable parts then is the maximum speed up with parallelization of the program with being the number of processors used if the sequential portion of program accounts for of the runtime we can get no more than speed up regardless of how many processors are added this puts an upper limit on the usefulness of adding more parallel execution units when task cannot be partitioned because of sequential constraints the application of more effort has no effect on the schedule the bearing of child takes nine months no matter how many women are assigned gustafson law is another law in computing closely related to amdahl law it states that the speedup with processors is assume that task has two independent parts and takes roughly of the time of the whole computation with effort programmer may be able to make this part five times faster but this only reduces the time for the whole computation by little in contrast one may need to perform less work to make part twice as fast this will make the computation much faster than by optimizing part even though got greater speed up versus both amdahl law and gustafson law assume that the running time of the sequential portion of the program is independent of the number of processors amdahl law assumes that the entire problem is of fixed size so that the total amount of work to be done in parallel is also independent of the number of processors whereas gustafson law assumes that the total amount of work to be done in parallel varies linearly with the number of processors dependencies understanding data dependencies is fundamental in implementing parallel algorithms no program can run more quickly than the longest chain of dependent calculations known as the critical path since calculations that depend upon prior calculations in the chain must be executed in order however most algorithms do not consist of just long chain of dependent calculations there are usually opportunities to execute independent calculations in parallel let pi and pj be two program segments bernstein conditions describe when the two are independent and can be executed in parallel for let be all of the input variables and the output variables and likewise for and are independent if they satisfy violation of the first condition introduces flow dependency corresponding to the first segment producing result used by the second segment the second condition represents an anti dependency when the second segment produces variable needed by the first segment the third and final condition represents an output dependency when two segments write to the same location the result comes from the logically last executed segment consider the following functions which demonstrate several kinds of dependencies function dep end function operation in dep cannot be executed before or even in parallel with operation because operation uses result from operation it violates condition and thus introduces flow dependency function nodep end function in this example there are no dependencies between the instructions so they can all be run in parallel bernstein conditions do not allow memory to be shared between different processes for that some means of enforcing an ordering between accesses is necessary such as semaphores barriers or some other synchronization method race conditions mutual exclusion synchronization and parallel slowdown subtasks in parallel program are often called threads some parallel computer architectures use smaller lightweight versions of threads known as fibers while others use bigger versions known as processes however threads is generally accepted as generic term for subtasks threads will often need to update some variable that is shared between them the instructions between the two programs may be interleaved in any order for example consider the following program thread thread read variable read variable add to variable add to variable write back to variable write back to variable if instruction is executed between and or if instruction is executed between and the program will produce incorrect data this is known as race condition the programmer must use lock to provide mutual exclusion lock is programming language construct that allows one thread to take control of variable and prevent other threads from reading or writing it until that variable is unlocked the thread holding the lock is free to execute its critical section the section of program that requires exclusive access to some variable and to unlock the data when it is finished therefore to guarantee correct program execution the above program can be rewritten to use locks thread thread lock variable lock variable read variable read variable add to variable add to variable write back to variable write back to variable unlock variable unlock variable one thread will successfully lock variable while the other thread will be locked out unable to proceed until is unlocked again this guarantees correct execution of the program locks while necessary to ensure correct program execution can greatly slow program locking multiple variables using non atomic locks introduces the possibility of program deadlock an atomic lock locks multiple variables all at once if it cannot lock all of them it does not lock any of them if two threads each need to lock the same two variables using non atomic locks it is possible that one thread will lock one of them and the second thread will lock the second variable in such case neither thread can complete and deadlock results many parallel programs require that their subtasks act in synchrony this requires the use of barrier barriers are typically implemented using software lock one class of algorithms known as lock free and wait free algorithms altogether avoids the use of locks and barriers however this approach is generally difficult to implement and requires correctly designed data structures not all parallelization results in speed up generally as task is split up into more and more threads those threads spend an ever increasing portion of their time communicating with each other eventually the overhead from communication dominates the time spent solving the problem and further parallelization that is splitting the workload over even more threads increases rather than decreases the amount of time required to finish this is known as parallel slowdown fine grained coarse grained and embarrassing parallelism applications are often classified according to how often their subtasks need to synchronize or communicate with each other an application exhibits fine grained parallelism if its subtasks must communicate many times per second it exhibits coarse grained parallelism if they do not communicate many times per second and it is embarrassingly parallel if they rarely or never have to communicate embarrassingly parallel applications are considered the easiest to parallelize consistency models parallel programming languages and parallel computers must have consistency model also known as memory model the consistency model defines rules for how operations on computer memory occur and how results are produced one of the first consistency models was leslie lamport sequential consistency model sequential consistency is the property of parallel program that its parallel execution produces the same results as sequential program specifically program is sequentially consistent if the results of any execution is the same as if the operations of all the processors were executed in some sequential order and the operations of each individual processor appear in this sequence in the order specified by its program software transactional memory is common type of consistency model software transactional memory borrows from database theory the concept of atomic transactions and applies them to memory accesses mathematically these models can be represented in several ways petri nets which were introduced in carl adam petri doctoral thesis were an early attempt to codify the rules of consistency models dataflow theory later built upon these and dataflow architectures were created to physically implement the ideas of dataflow theory beginning in the late process calculi such as calculus of communicating systems and communicating sequential processes were developed to permit algebraic reasoning about systems composed of interacting components more recent additions to the process calculus family such as the calculus have added the capability for reasoning about dynamic topologies logics such as lamport tla and mathematical models such as traces and actor event diagrams have also been developed to describe the behavior of concurrent systems flynn taxonomy michael flynn created one of the earliest classification systems for parallel and sequential computers and programs now known as flynn taxonomy flynn classified programs and computers by whether they were operating using single set or multiple sets of instructions and whether or not those instructions were using single set or multiple sets of data the single instruction single data sisd classification is equivalent to an entirely sequential program the single instruction multiple data simd classification is analogous to doing the same operation repeatedly over large data set this is commonly done in signal processing applications multiple instruction single data misd is rarely used classification while computer architectures to deal with this were devised such as systolic arrays few applications that fit this class materialized multiple instruction multiple data mimd programs are by far the most common type of parallel programs according to david patterson and john hennessy some machines are hybrids of these categories of course but this classic model has survived because it is simple easy to understand and gives good first approximation it is also perhaps because of its the most widely used scheme types of parallelism bit level parallelism from the advent of very large scale integration vlsi computer chip fabrication technology in the until about speed up in computer architecture was driven by doubling computer word size the amount of information the processor can manipulate per cycle increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word for example where an bit processor must add two bit integers the processor must first add the lower order bits from each integer using the standard addition instruction then add the higher order bits using an add with carry instruction and the carry bit from the lower order addition thus an bit processor requires two instructions to complete single operation where bit processor would be able to complete the operation with single instruction historically bit microprocessors were replaced with bit then bit then bit microprocessors this trend generally came to an end with the introduction of bit processors which has been standard in general purpose computing for two decades not until recently with the advent of architectures have bit processors become commonplace instruction level parallelism canonical five stage pipeline in risc machine if instruction fetch id instruction decode ex execute mem memory access wb register write back computer program is in essence stream of instructions executed by processor these instructions can be re ordered and combined into groups which are then executed in parallel without changing the result of the program this is known as instruction level parallelism advances in instruction level parallelism dominated computer architecture from the mid until the mid modern processors have multi stage instruction pipelines each stage in the pipeline corresponds to different action the processor performs on that instruction in that stage processor with an stage pipeline can have up to different instructions at different stages of completion the canonical example of pipelined processor is risc processor with five stages instruction fetch decode execute memory access and write back the pentium processor had stage pipeline five stage pipelined superscalar processor capable of issuing two instructions per cycle it can have two instructions in each stage of the pipeline for total of up to instructions shown in green being simultaneously executed in addition to instruction level parallelism from pipelining some processors can issue more than one instruction at time these are known as superscalar processors instructions can be grouped together only if there is no data dependency between them scoreboarding and the tomasulo algorithm which is similar to scoreboarding but makes use of register renaming are two of the most common techniques for implementing out of order execution and instruction level parallelism task parallelism task parallelisms is the characteristic of parallel program that entirely different calculations can be performed on either the same or different sets of data this contrasts with data parallelism where the same calculation is performed on the same or different sets of data task parallelism involves the decomposition of task into sub tasks and then allocating each sub task to processor for execution the processors would then execute these sub tasks simultaneously and often cooperatively task parallelism does not usually scale with the size of problem hardware memory and communication main memory in parallel computer is either shared memory shared between all processing elements in single address space or distributed memory in which each processing element has its own local address space distributed memory refers to the fact that the memory is logically distributed but often implies that it is physically distributed as well distributed shared memory and memory virtualization combine the two approaches where the processing element has its own local memory and access to the memory on non local processors accesses to local memory are typically faster than accesses to non local memory logical view of non uniform memory access numa architecture processors in one directory can access that directory memory with less latency than they can access memory in the other directory memory computer architectures in which each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access uma systems typically that can be achieved only by shared memory system in which the memory is not physically distributed system that does not have this property is known as non uniform memory access numa architecture distributed memory systems have non uniform memory access computer systems make use of caches small fast memories located close to the processor which store temporary copies of memory values nearby in both the physical and logical sense parallel computer systems have difficulties with caches that may store the same value in more than one location with the possibility of incorrect program execution these computers require cache coherency system which keeps track of cached values and strategically purges them thus ensuring correct program execution bus snooping is one of the most common methods for keeping track of which values are being accessed and thus should be purged designing large high performance cache coherence systems is very difficult problem in computer architecture as result shared memory computer architectures do not scale as well as distributed memory systems do processor processor and processor memory communication can be implemented in hardware in several ways including via shared either multiported or multiplexed memory crossbar switch shared bus or an interconnect network of myriad of topologies including star ring tree hypercube fat hypercube hypercube with more than one processor at node or dimensional mesh parallel computers based on interconnect networks need to have some kind of routing to enable the passing of messages between nodes that are not directly connected the medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines classes of parallel computers parallel computers can be roughly classified according to the level at which the hardware supports parallelism this classification is broadly analogous to the distance between basic computing nodes these are not mutually exclusive for example clusters of symmetric multiprocessors are relatively common multicore computing multicore processor is processor that includes multiple execution units cores on the same chip these processors differ from superscalar processors which can issue multiple instructions per cycle from one instruction stream thread in contrast multicore processor can issue multiple instructions per cycle from multiple instruction streams ibm cell microprocessor designed for use in the sony playstation is another prominent multicore processor each core in multicore processor can potentially be superscalar as well that is on every cycle each core can issue multiple instructions from one instruction stream simultaneous multithreading of which intel hyperthreading is the best known was an early form of pseudo multicoreism processor capable of simultaneous multithreading has only one execution unit core but when that execution unit is idling such as during cache miss it uses that execution unit to process second thread symmetric multiprocessing symmetric multiprocessor smp is computer system with multiple identical processors that share memory and connect via bus bus contention prevents bus architectures from scaling as result smps generally do not comprise more than processors because of the small size of the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches such symmetric multiprocessors are extremely cost effective provided that sufficient amount of memory bandwidth exists distributed computing distributed computer also known as distributed memory multiprocessor is distributed memory computer system in which the processing elements are connected by network distributed computers are highly scalable cluster computing beowulf cluster cluster is group of loosely coupled computers that work together closely so that in some respects they can be regarded as single computer clusters are composed of multiple standalone machines connected by network while machines in cluster do not have to be symmetric load balancing is more difficult if they are not the most common type of cluster is the beowulf cluster which is cluster implemented on multiple identical commercial off the shelf computers connected with tcp ip ethernet local area network beowulf technology was originally developed by thomas sterling and donald becker the vast majority of the top supercomputers are clusters because grid computing systems described below can easily handle embarrassingly parallel problems modern clusters are typically designed to handle more difficult problems problems that require nodes to share intermediate results with each other more often this requires high bandwidth and more importantly low latency interconnection network many historic and current supercomputers use customized high performance network hardware specifically designed for cluster computing such as the cray gemini network as of most current supercomputers use some off the shelf standard network hardware often myrinet infiniband or gigabit ethernet massive parallel processing cabinet from blue gene ranked as the fourth fastest supercomputer in the world according to the top rankings blue gene is massively parallel processor massively parallel processor mpp is single computer with many networked processors mpps have many of the same characteristics as clusters but mpps have specialized interconnect networks whereas clusters use commodity hardware for networking mpps also tend to be larger than clusters typically having far more than processors in mpp each cpu contains its own memory and copy of the operating system and application each subsystem communicates with the others via high speed interconnect blue gene the fifth fastest supercomputer in the world according to the june top ranking is mpp grid computing grid computing is the most distributed form of parallel computing it makes use of computers communicating over the internet to work on given problem because of the low bandwidth and extremely high latency available on the internet distributed computing typically deals only with embarrassingly parallel problems many distributed computing applications have been created of which seti home and folding home are the best known examples most grid computing applications use middleware software that sits between the operating system and the application to manage network resources and standardize the software interface the most common distributed computing middleware is the berkeley open infrastructure for network computing boinc often distributed computing software makes use of spare cycles performing computations at times when computer is idling specialized parallel computers within parallel computing there are specialized parallel devices that remain niche areas of interest while not domain specific they tend to be applicable to only few classes of parallel problems reconfigurable computing with field programmable gate arrays reconfigurable computing is the use of field programmable gate array fpga as co processor to general purpose computer an fpga is in essence computer chip that can rewire itself for given task fpgas can be programmed with hardware description languages such as vhdl or verilog however programming in these languages can be tedious several vendors have created to hdl languages that attempt to emulate the syntax and semantics of the programming language with which most programmers are familiar the best known to hdl languages are mitrion impulse dime and handel specific subsets of systemc based on can also be used for this purpose amd decision to open its hypertransport technology to third party vendors has become the enabling technology for high performance reconfigurable computing according to michael amour chief operating officer of drc computer corporation when we first walked into amd they called us the socket stealers now they call us their partners general purpose computing on graphics processing units gpgpu tesla gpgpu card general purpose computing on graphics processing units gpgpu is fairly recent trend in computer engineering research gpus are co processors that have been heavily optimized for computer graphics processing computer graphics processing is field dominated by data parallel operations particularly linear algebra matrix operations in the early days gpgpu programs used the normal graphics apis for executing programs however several new programming languages and platforms have been built to do general purpose computation on gpus with both nvidia and amd releasing programming environments with cuda and stream sdk respectively other gpu programming languages include brookgpu peakstream and rapidmind nvidia has also released specific products for computation in their tesla series the technology consortium khronos group has released the opencl specification which is framework for writing programs that execute across platforms consisting of cpus and gpus amd apple intel nvidia and others are supporting opencl application specific integrated circuits several application specific integrated circuit asic approaches have been devised for dealing with parallel applications because an asic is by definition specific to given application it can be fully optimized for that application as result for given application an asic tends to outperform general purpose computer however asics are created by ray lithography this process requires mask which can be extremely expensive single mask can cost over million us dollars the smaller the transistors required for the chip the more expensive the mask will be meanwhile performance increases in general purpose computing over time as described by moore law tend to wipe out these gains in only one or two chip generations high initial cost and the tendency to be overtaken by moore law driven general purpose computing has rendered asics unfeasible for most parallel computing applications however some have been built one example is the peta flop riken mdgrape machine which uses custom asics for molecular dynamics simulation vector processors the cray is the most famous vector processor vector processor is cpu or computer system that can execute the same instruction on large sets of data vector processors have high level operations that work on linear arrays of numbers or vectors an example vector operation is where and are each element vectors of bit floating point numbers they are closely related to flynn simd classification cray computers became famous for their vector processing computers in the and however vector processors both as cpus and as full computer systems have generally disappeared modern processor instruction sets do include some vector processing instructions such as with altivec and streaming simd extensions sse software parallel programming languages concurrent programming languages libraries apis and parallel programming models such as algorithmic skeletons have been created for programming parallel computers these can generally be divided into classes based on the assumptions they make about the underlying memory architecture shared memory distributed memory or shared distributed memory shared memory programming languages communicate by manipulating shared memory variables distributed memory uses message passing posix threads and openmp are two of most widely used shared memory apis whereas message passing interface mpi is the most widely used message passing system api one concept used in programming parallel programs is the future concept where one part of program promises to deliver required datum to another part of program at some future time caps entreprise and pathscale are also coordinating their effort to make hmpp hybrid multicore parallel programming directives an open standard called openhmpp the openhmpp directive based programming model offers syntax to efficiently offload computations on hardware accelerators and to optimize data movement to from the hardware memory openhmpp directives describe remote procedure call rpc on an accelerator device gpu or more generally set of cores the directives annotate or fortran codes to describe two sets of functionalities the offloading of procedures denoted codelets onto remote device and the optimization of data transfers between the cpu main memory and the accelerator memory automatic parallelization automatic parallelization of sequential program by compiler is the holy grail of parallel computing despite decades of work by compiler researchers automatic parallelization has had only limited success mainstream parallel programming languages remain either explicitly parallel or at best partially implicit in which programmer gives the compiler directives for parallelization few fully implicit parallel programming languages exist sisal parallel haskell system for fpgas mitrion vhdl and verilog application checkpointing as computer system grows in complexity the mean time between failures usually decreases application checkpointing is technique whereby the computer system takes snapshot of the application record of all current resource allocations and variable states akin to core dump this information can be used to restore the program if the computer should fail application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning while checkpointing provides benefits in variety of situations it is especially useful in highly parallel systems with large number of processors used in high performance computing algorithmic methods as parallel computers become larger and faster it becomes feasible to solve problems that previously took too long to run parallel computing is used in wide range of fields from bioinformatics protein folding and sequence analysis to economics mathematical finance common types of problems found in parallel computing applications are dense linear algebra sparse linear algebra spectral methods such as cooley tukey fast fourier transform body problems such as barnes hut simulation structured grid problems such as lattice boltzmann methods unstructured grid problems such as found in finite element analysis monte carlo simulation combinational logic such as brute force cryptographic techniques graph traversal such as sorting algorithms dynamic programming branch and bound methods graphical models such as detecting hidden markov models and constructing bayesian networks finite state machine simulation fault tolerance parallel computing can also be applied to the design of fault tolerant computer systems particularly via lockstep systems performing the same operation in parallel this provides redundancy in case one component should fail and also allows automatic error detection and error correction if the results differ these methods can be used to help prevent single event upsets caused by transient errors although additional measures may be required in embedded or specialized systems this method can provide cost effective approach to achieve modular redundancy in commercial off the shelf systems history illiac iv perhaps the most infamous of supercomputers the origins of true mimd parallelism go back to luigi federico menabrea and his sketch of the analytic engine invented by charles babbage ibm introduced the in through project in which gene amdahl was one of the principal architects it became the first commercially available computer to use fully automatic floating point arithmetic commands in april gill ferranti discussed parallel programming and the need for branching and waiting also in ibm researchers john cocke and daniel slotnick discussed the use of parallelism in numerical calculations for the first time burroughs corporation introduced the in four processor computer that accessed up to memory modules through crossbar switch in amdahl and slotnick published debate about the feasibility of parallel processing at american federation of information processing societies conference it was during this debate that amdahl law was coined to define the limit of speed up due to parallelism in us company honeywell introduced its first multics system symmetric multiprocessor system capable of running up to eight processors in parallel mmp multi processor project at carnegie mellon university was among the first multiprocessors with more than few processors the first bus connected multi processor with snooping caches was the synapse in simd parallel computers can be traced back to the the motivation behind early simd computers was to amortize the gate delay of the processor control unit over multiple instructions in slotnick had proposed building massively parallel computer for the lawrence livermore national laboratory his design was funded by the us air force which was the earliest simd parallel computing effort illiac iv the key to its design was fairly high parallelism with up to processors which allowed the machine to work on large datasets in what would later be known as vector processing however illiac iv was called the most infamous of supercomputers because the project was only one fourth completed but took years and cost almost four times the original estimate when it was finally ready to run its first real application in it was outperformed by existing commercial supercomputers such as the cray see also list of important publications in concurrent parallel and distributed computing list of distributed computing conferences concurrency computer science synchronous programming content addressable parallel processor manycore serializability transputer references further reading external links go parallel translating multicore power into application performance instructional videos on caf in the fortran standard by john reid see appendix lawrence livermore national laboratory introduction to parallel computing comparing programmability of open mp and pthreads what makes parallel programming hard designing and building parallel programs by ian foster internet parallel computing archive parallel processing topic area at ieee distributed computing online parallel computing works free on line book frontiers of supercomputing free on line book covering topics like algorithms and industrial applications universal parallel computing research center course in parallel programming at columbia university in collaboration with ibm watson project parallel and distributed gr obner bases computation in jas course in parallel computing at university of wisconsin madison openhmpp new standard for manycore berkeley par lab progress in the parallel computing landscape editors david patterson dennis gannon and michael wrinn august the trouble with multicore by david patterson posted jun the landscape of parallel computing research view from berkeley one too many dead link at this site", "Computational science": "computational science also scientific computing or scientific computation is concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific and engineering problems in practical use it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to problems in various scientific disciplines the field is different from theory and laboratory experiment which are the traditional forms of science and engineering the scientific computing approach is to gain understanding mainly through the analysis of mathematical models implemented on computers scientists and engineers develop computer programs application software that model systems being studied and run these programs with various sets of input parameters in some cases these models require massive amounts of calculations usually floating point and are often executed on supercomputers or distributed computing platforms numerical analysis is an important underpinning for techniques used in computational science applications of computational science problem domains for computational science scientific computing include numerical simulations numerical simulations have different objectives depending on the nature of the task being simulated reconstruct and understand known events earthquake tsunamis and other natural disasters predict future or unobserved situations weather sub atomic particle behaviour and primordial explosions model fitting and data analysis appropriately tune models or solve equations to reflect observations subject to model constraints oil exploration geophysics computational linguistics use graph theory to model networks such as those connecting individuals organizations websites and biological systems computational optimization optimize known scenarios technical and manufacturing processes front end engineering machine learning methods and algorithms algorithms and mathematical methods used in computational science are varied commonly applied methods include numerical analysis application of taylor series as convergent and asymptotic series computing derivatives by automatic differentiation ad computing derivatives by finite differences graph theoretic suites high order difference approximations via taylor series and richardson extrapolation methods of integration on uniform mesh rectangle rule also called midpoint rule trapezoid rule simpson rule runge kutta method for solving ordinary differential equations monte carlo methods molecular dynamics linear programming branch and cut branch and bound numerical linear algebra computing the lu factors by gaussian elimination cholesky factorizations discrete fourier transform and applications newton method space mapping time stepping methods for dynamical systems programming languages and computer algebra systems commonly used for the more mathematical aspects of scientific computing applications include programming language tk solver matlab mathematica scilab gnu octave python programming language with scipy and pdl the more computationally intensive aspects of scientific computing will often use some variation of or fortran and optimized algebra libraries such as blas or lapack computational science application programs often model real world changing conditions such as weather air flow around plane automobile body distortions in crash the motion of stars in galaxy an explosive device etc such programs might create logical mesh in computer memory where each item corresponds to an area in space and contains information about that space relevant to the model for example in weather models each item might be square kilometer with land elevation current wind direction humidity temperature pressure etc the program would calculate the likely next state based on the current state in simulated time steps solving equations that describe how the system operates and then repeat the process to calculate the next state the term computational scientist is used to describe someone skilled in scientific computing this person is usually scientist an engineer or an applied mathematician who applies high performance computing in different ways to advance the state of the art in their respective applied disciplines in physics chemistry or engineering scientific computing has increasingly also impacted on other areas including economics biology and medicine computational science is now commonly considered third mode of science complementing and adding to experimentation observation and theory the essence of computational science is numerical algorithm and or computational mathematics in fact substantial effort in computational sciences has been devoted to the development of algorithms the efficient implementation in programming languages and validation of computational results collection of problems and solutions in computational science can be found in steeb hardy hardy and stoop reproducibility and open research computing the complexity of computational methods is threat to the reproducibility of research jon claerbout has become prominent for pointing out that reproducible research requires archiving and documenting all raw data and all code used to obtain result nick barnes in the science code manifesto proposed five principles that should be followed when software is used in open science publication tomi kauppinen et al established and defined linked open science an approach to interconnect scientific assets to enable transparent reproducible and research journals most scientific journals do not accept software papers because description of reasonably mature software usually does not meet the criterion of novelty outside computer science itself there are only few journals dedicated to scientific software established journals like elsevier computer physics communications publish papers that are not open access though the described software usually is to fill this gap new journal entitled open research computation was announced in it closed in without having published single paper for lack of submissions probably due to excessive quality requirements new initiative was launched in the journal of open research software in new journal dedicated to the replication of computational results has been started on github education scientific computation is most often studied through an applied mathematics or computer science program or within standard mathematics sciences or engineering program at some institutions specialization in scientific computation can be earned as minor within another program which may be at varying levels however there are increasingly many bachelor and master programs in computational science some schools also offer the ph in computational science computational engineering computational science and engineering or scientific computation there are also programs in areas such as computational physics computational chemistry etc related fields bioinformatics cheminformatics chemometrics computational archaeology computational biology computational chemistry computational economics computational computational engineering computational finance computational fluid dynamics computational forensics computational geophysics computational informatics computational intelligence computational law computational linguistics computational mathematics computational mechanics computational neuroscience computational particle physics computational physics computational sociology computational statistics computer algebra environmental simulation financial modeling geographic information system gis high performance computing machine learning network analysis numerical linear algebra numerical weather prediction pattern recognition scientific visualization see also computational science and engineering comparison of computer algebra systems list of molecular modeling software list of numerical analysis software list of statistical packages timeline of scientific computing simulated reality references additional sources hager and wellein introduction to high performance computing for scientists and engineers chapman and hall hartmann practical guide to computer simulations world scientific journal computational methods in science and technology open access polish academy of sciences journal computational science and discovery institute of physics landau bordeianu and jose paez survey of computational physics introductory computational science princeton university press external links john von neumann institut for computing nic at juelich germany the national center for computational science at oak ridge national laboratory educational materials for undergraduate computational studies computational science at the national laboratories bachelor in computational science university of medellin colombia south america simulation optimization systems sos research laboratory mcmaster university hamilton on", "Information retrieval": "information retrieval ir is the activity of obtaining information resources relevant to an information need from collection of information resources searches can be based on metadata or on full text or other content based indexing automated information retrieval systems are used to reduce what has been called information overload many universities and public libraries use ir systems to provide access to books journals and other documents web search engines are the most visible ir applications overview an information retrieval process begins when user enters query into the system queries are formal statements of information needs for example search strings in web search engines in information retrieval query does not uniquely identify single object in the collection instead several objects may match the query perhaps with different degrees of relevancy an object is an entity that is represented by information in database user queries are matched against the database information depending on the application the data objects may be for example text documents images audio mind maps or videos often the documents themselves are not kept or stored directly in the ir system but are instead represented in the system by document surrogates or metadata most ir systems compute numeric score on how well each object in the database matches the query and rank the objects according to this value the top ranking objects are then shown to the user the process may then be iterated if the user wishes to refine the query history the idea of using computers to search for relevant pieces of information was popularized in the article as we may think by vannevar bush in it would appear that bush was inspired by patents for statistical machine filed by emanuel goldberg in the and that searched for documents stored on film the first description of computer searching for information was described by holmstrom in detailing an early mention of the univac computer automated information retrieval systems were introduced in the one even featured in the romantic comedy desk set in the the first large information retrieval research group was formed by gerard salton at cornell by the several different retrieval techniques had been shown to perform well on small text corpora such as the cranfield collection several thousand documents large scale retrieval systems such as the lockheed dialog system came into use early in the in the us department of defense along with the national institute of standards and technology nist cosponsored the text retrieval conference trec as part of the tipster text program the aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on very large text collection this catalyzed research on methods that scale to huge corpora the introduction of web search engines has boosted the need for very large scale retrieval systems even further model types german entry original source dominik kuropka for effectively retrieving relevant documents by ir strategies the documents are typically transformed into suitable representation each retrieval strategy incorporates specific model for its document representation purposes the picture on the right illustrates the relationship of some common models in the picture the models are categorized according to two dimensions the mathematical basis and the properties of the model first dimension mathematical basis set theoretic models represent documents as sets of words or phrases similarities are usually derived from set theoretic operations on those sets common models are standard boolean model extended boolean model fuzzy retrieval algebraic models represent documents and queries usually as vectors matrices or tuples the similarity of the query vector and document vector is represented as scalar value vector space model generalized vector space model enhanced topic based vector space model extended boolean model latent semantic indexing latent semantic analysis probabilistic models treat the process of document retrieval as probabilistic inference similarities are computed as probabilities that document is relevant for given query probabilistic theorems like the bayes theorem are often used in these models binary independence model probabilistic relevance model on which is based the okapi bm relevance function uncertain inference language models divergence from randomness model latent dirichlet allocation feature based retrieval models view documents as vectors of values of feature functions or just features and seek the best way to combine these features into single relevance score typically by learning to rank methods feature functions are arbitrary functions of document and query and as such can easily incorporate almost any other retrieval model as just yet another feature second dimension properties of the model models without term treat different terms words as independent this fact is usually represented in vector space models by the orthogonality assumption of term vectors or in probabilistic models by an independency assumption for term variables models with immanent term allow representation of between terms however the degree of the interdependency between two terms is defined by the model itself it is usually directly or indirectly derived by dimensional reduction from the co occurrence of those terms in the whole set of documents models with transcendent term allow representation of between terms but they do not allege how the interdependency between two terms is defined they rely an external source for the degree of interdependency between two terms for example human or sophisticated algorithms performance and correctness measures the evaluation of an information retrieval system is the process of assessing how well system meets the information needs of its users traditional evaluation metrics designed for boolean retrieval or top retrieval include precision and recall many more measures for evaluating the performance of information retrieval systems have also been proposed in general measurement considers collection of documents to be searched and search query all common measures described here assume ground truth notion of relevancy every document is known to be either relevant or non relevant to particular query in practice queries may be ill posed and there may be different shades of relevancy virtually all modern evaluation metrics mean average precision discounted cumulative gain are designed for ranked retrieval without any explicit rank cutoff taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks the mathematical symbols used in the formulas below mean intersection in this case specifying the documents in both sets and cardinality in this case the number of documents in set integral summation symmetric difference precision precision is the fraction of the documents retrieved that are relevant to the user information need in binary classification precision is analogous to positive predictive value precision takes all retrieved documents into account it can also be evaluated at given cut off rank considering only the topmost results returned by the system this measure is called precision at or note that the meaning and usage of precision in the field of information retrieval differs from the definition of accuracy and precision within other branches of science and statistics recall recall is the fraction of the documents that are relevant to the query that are successfully retrieved in binary classification recall is often called sensitivity so it can be looked at as the probability that relevant document is retrieved by the query it is trivial to achieve recall of by returning all documents in response to any query therefore recall alone is not enough but one needs to measure the number of non relevant documents also for example by computing the precision fall out the proportion of non relevant documents that are retrieved out of all non relevant documents available in binary classification fall out is closely related to specificity and is equal to it can be looked at as the probability that non relevant document is retrieved by the query it is trivial to achieve fall out of by returning zero documents in response to any query score measure the weighted harmonic mean of precision and recall the traditional measure or balanced score is this is also known as the measure because recall and precision are evenly weighted the general formula for non negative real is two other commonly used measures are the measure which weights recall twice as much as precision and the measure which weights precision twice as much as recall the measure was derived by van rijsbergen so that measures the effectiveness of retrieval with respect to user who attaches times as much importance to recall as precision it is based on van rijsbergen effectiveness measure their relationship is where measure can be better single metric when compared to precision and recall both precision and recall give different information that can complement each other when combined if one of them excels more than the other measure will reflect it average precision precision and recall are single value metrics based on the whole list of documents returned by the system for systems that return ranked sequence of documents it is desirable to also consider the order in which the returned documents are presented by computing precision and recall at every position in the ranked sequence of documents one can plot precision recall curve plotting precision as function of recall average precision computes the average value of over the interval from to that is the area under the precision recall curve this integral is in practice replaced with finite sum over every position in the ranked sequence of documents where is the rank in the sequence of retrieved documents is the number of retrieved documents is the precision at cut off in the list and is the change in recall from items to this finite sum is equivalent to where is an indicator function equaling if the item at rank is relevant document zero otherwise note that the average is over all relevant documents and the relevant documents not retrieved get precision score of zero some authors choose to interpolate the function to reduce the impact of wiggles in the curve for example the pascal visual object classes challenge benchmark for computer vision object detection computes average precision by averaging the precision over set of evenly spaced recall levels where is an interpolated precision that takes the maximum precision over all recalls greater than an alternative is to derive an analytical function by assuming particular parametric distribution for the underlying decision values for example binormal precision recall curve can be obtained by assuming decision values in both classes to follow gaussian distribution precision at for modern web scale information retrieval recall is no longer meaningful metric as many queries have thousands of relevant documents and few users will be interested in reading all of them precision at documents is still useful metric or precision at corresponds to the number of relevant results on the first search results page but fails to take into account the positions of the relevant documents among the top another shortcoming is that on query with fewer relevant results than even perfect system will have score less than it easier to score manually since only the top results need to be examined to determine if they are relevant or not precision precision requires knowing all documents that are relevant to query the number of relevant documents is used as the cutoff for calculation and this varies from query to query for example if there are documents relevant to red in corpus precision for red looks at the top documents returned counts the number that are relevant turns that into relevancy fraction precision is equal to recall at the th position empirically this measure is often highly correlated to mean average precision mean average precision mean average precision for set of queries is the mean of the average precision scores for each query where is the number of queries discounted cumulative gain dcg uses graded relevance scale of documents from the result set to evaluate the usefulness or gain of document based on its position in the result list the premise of dcg is that highly relevant documents appearing lower in search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result the dcg accumulated at particular rank position is defined as since result set may vary in size among different queries or systems to compare performances the normalised version of dcg uses an ideal dcg to this end it sorts documents of result list by relevance producing an ideal dcg at position which normalizes the score the ndcg values for all queries can be averaged to obtain measure of the average performance of ranking algorithm note that in perfect ranking algorithm the will be the same as the producing an ndcg of all ndcg calculations are then relative values on the interval to and so are cross query comparable other measures mean reciprocal rank spearman rank correlation coefficient bpref summation based measure of how many relevant documents are ranked before irrelevant documents gmap geometric mean of per topic average precision gain improvement over random of any of the above measures measures based on marginal relevance and document diversity see visualization visualizations of information retrieval performance include graphs which chart precision on one axis and recall on the other histograms of average precision over various topics receiver operating characteristic roc curve confusion matrix timeline before the joseph marie jacquard invents the jacquard loom the first machine to use punched cards to control sequence of operations herman hollerith invents an electro mechanical data tabulator using punch cards as machine readable medium hollerith cards keypunches and tabulators used to process the us census data emanuel goldberg submits patents for his statistical machine document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents late the us military confronted problems of indexing and retrieval of wartime scientific research documents captured from germans vannevar bush as we may think appeared in atlantic monthly hans peter luhn research engineer at ibm since began work on mechanized punch card based system for searching chemical compounds growing concern in the us for science gap with the ussr motivated encouraged funding and provided backdrop for mechanized literature searching systems allen kent et al and the invention of citation indexing eugene garfield the term information retrieval appears to have been coined by calvin mooers philip bagley conducted the earliest experiment in computerized document retrieval in master thesis at mit allen kent joined case western reserve university and eventually became associate director of the center for documentation and communications research that same year kent and colleagues published paper in american documentation describing the precision and recall measures as well as detailing proposed framework for evaluating an ir system which included statistical sampling methods for determining the number of relevant documents not retrieved international conference on scientific information washington dc included consideration of ir systems as solution to problems identified see proceedings of the international conference on scientific information national academy of sciences washington dc hans peter luhn published auto encoding of documents for information retrieval early gerard salton began work on ir at harvard later moved to cornell melvin earl maron and john lary kuhns published on relevance probabilistic indexing and information retrieval in the journal of the acm july cyril cleverdon published early findings of the cranfield studies developing model for ir system evaluation see cyril cleverdon report on the testing and analysis of an investigation into the comparative efficiency of indexing systems cranfield collection of aeronautics cranfield england kent published information analysis and retrieval weinberg report science government and information gave full articulation of the idea of crisis of scientific information the report was named after dr alvin weinberg joseph becker and robert hayes published text on information retrieval becker joseph hayes robert mayo information storage and retrieval tools elements theories new york wiley karen sp\u00e4rck jones finished her thesis at cambridge synonymy and semantic classification and continued work on computational linguistics as it applies to ir the national bureau of standards sponsored symposium titled statistical association methods for mechanized documentation several highly significant papers including salton first published reference we believe to the smart system mid national library of medicine developed medlars medical literature analysis and retrieval system the first major machine readable database and batch retrieval system project intrex at mit licklider published libraries of the future don swanson was involved in studies at university of chicago on requirements for future catalogs late wilfrid lancaster completed evaluation studies of the medlars system and published the first edition of his text on information retrieval gerard salton published automatic information organization and retrieval john sammon jr radc tech report some mathematics of information storage and retrieval outlined the vector model sammon nonlinear mapping for data structure analysis ieee transactions on computers was the first proposal for visualization interface to an ir system early first online systems nlm aim twx medline lockheed dialog sdc orbit theodor nelson promoting concept of hypertext published computer lib dream machines nicholas jardine and cornelis van rijsbergen published the use of hierarchic clustering in information retrieval which articulated the cluster hypothesis three highly influential publications by salton fully articulated his vector processing framework and term discrimination model theory of indexing society for industrial and applied mathematics theory of term importance in automatic text analysis jasis vector space model for automatic indexing cacm the first acm sigir conference van rijsbergen published information retrieval butterworths heavy emphasis on probabilistic models tamas doszkocs implemented the cite natural language user interface for medline at the national library of medicine the cite system supported free form query input ranked output and relevance feedback first international acm sigir conference joint with british computer society ir group in cambridge nicholas belkin robert oddy and helen brooks proposed the ask anomalous state of knowledge viewpoint for information retrieval this was an important concept though their automated analysis tool proved ultimately disappointing salton and michael mcgill published introduction to modern information retrieval mcgraw hill with heavy emphasis on vector models david blair and bill maron publish an evaluation of retrieval effectiveness for full text document retrieval system mid efforts to develop end user versions of commercial ir systems key papers on and experimental systems for visualization interfaces work by donald crouch robert korfhage matthew chalmers anselm spoerri and others first world wide web proposals by tim berners lee at cern first trec conference publication of korfhage information storage and retrieval with emphasis on visualization and multi reference point systems late web search engines implementation of many features formerly found only in experimental ir systems search engines become the most common and maybe best instantiation of ir models awards in the field tony kent strix award gerard salton award see also adversarial information retrieval collaborative information seeking controlled vocabulary cross language information retrieval data mining european summer school in information retrieval human computer information retrieval information extraction information retrieval facility knowledge visualization multimedia information retrieval list of information retrieval systems personal information management relevance information retrieval relevance feedback rocchio classification search index social information seeking special interest group on information retrieval subject indexing temporal information retrieval tf idf xml retrieval references further reading christopher manning prabhakar raghavan and hinrich sch\u00fctze introduction to information retrieval cambridge university press stefan b\u00fcttcher charles clarke and gordon cormack information retrieval implementing and evaluating search engines mit press cambridge mass external links acm sigir information retrieval special interest group bcs irsg british computer society information retrieval specialist group text retrieval conference trec forum for information retrieval evaluation fire information retrieval online book by van rijsbergen information retrieval wiki information retrieval facility information retrieval duth trec report on information retrieval evaluation techniques how ebay measures search relevance", "Computer literacy": "computer literacy is the ability to use computers and related technology efficiently with range of skills covering levels from elementary use to programming and advanced problem solving computer literacy can also refer to the comfort level someone has with using computer programs and other applications that are associated with computers another valuable component is understanding how computers work and operate computer literacy may be distinguished from computer programming which is design and coding of computer programs rather than familiarity and skill in their use in developed countries computer literacy is considered to be very important skill to possess in developed countries employers want their workers to have basic computer skills because their company becomes ever more dependent on computers many companies try to use computers and other technology to improve business efficiency computers are just as common as pen and paper are for writing especially among youth there seems to be an inversely proportional relationship between computer literacy and compositional literacy among first world computer users for many applications especially communicating computers are preferred over pen paper and typewriters because of their ability to duplicate and retain information and ease of editing as personal computers become commonplace and they become more powerful the concept of computer literacy is moving beyond basic functionality to more powerful applications under the heading of multimedia literacy or new literacies it is frequently assumed that as computer and internet access is common place in the first world everyone in those countries must have equal and ready access to this technology and to skills in how to effectively use it there is however significant digital divide in even the most technologically advanced and enabled countries with digital haves and have nots older workers who do not use the internet at home and are computer illiterate may be frozen out of the job market even for relatively unskilled jobs such as clerking in an auto parts store the digital inclusion forum consortium set up through joint participation from the wireless internet institute ibm intel microsoft and ohio one community is just one organization developed to address this their organizational mission in this is to provide comprehensive resource center to inform educate and share best practices among state and local government leaders industry and institutional stakeholders on identifying and implementing sustainable market solutions to bridge the digital divide in north america variety of private sector nonprofits and foundations also contribute to this in addressing the needs of underserved communities per scholas for example runs programs offering free and low cost computers to children and their families in underserved communities in the south bronx new york miami florida and in columbus ohio see also computer science education digital literacy digital divide european computer driving licence new literacies tagibook references", "Data mining": "data mining the analysis step of the knowledge discovery in databases process or kdd an subfield of computer science is the computational process of discovering patterns in large data sets big data involving methods at the intersection of artificial intelligence machine learning statistics and database systems the overall goal of the data mining process is to extract information from data set and transform it into an understandable structure for further use aside from the raw analysis step it involves database and data management aspects data pre processing model and inference considerations interestingness metrics complexity considerations post processing of discovered structures visualization and online updating the term is misnomer because the goal is the extraction of patterns and knowledge from large amount of data not the extraction of data itself it also is buzzword and is frequently applied to any form of large scale data or information processing collection extraction warehousing analysis and statistics as well as any application of computer decision support system including artificial intelligence machine learning and business intelligence the popular book data mining practical machine learning tools and techniques with java which covers mostly machine learning material was originally to be named just practical machine learning and the term data mining was only added for marketing reasons often the more general terms large scale data analysis or analytics or when referring to actual methods artificial intelligence and machine learning are more appropriate the actual data mining task is the automatic or semi automatic analysis of large quantities of data to extract previously unknown interesting patterns such as groups of data records cluster analysis unusual records anomaly detection and dependencies association rule mining this usually involves using database techniques such as spatial indices these patterns can then be seen as kind of summary of the input data and may be used in further analysis or for example in machine learning and predictive analytics for example the data mining step might identify multiple groups in the data which can then be used to obtain more accurate prediction results by decision support system neither the data collection data preparation nor result interpretation and reporting are part of the data mining step but do belong to the overall kdd process as additional steps the related terms data dredging data fishing and data snooping refer to the use of data mining methods to sample parts of larger population data set that are or may be too small for reliable statistical inferences to be made about the validity of any patterns discovered these methods can however be used in creating new hypotheses to test against the larger data populations etymology in the statisticians used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an priori hypothesis the term data mining appeared around in the database community for short time in phrase database mining was used but since it was trademarked by hnc san diego based company to pitch their database mining workstation researchers consequently turned to data mining other terms used include data archaeology information harvesting information discovery knowledge extraction etc gregory piatetsky shapiro coined the term knowledge discovery in databases for the first workshop on the same topic kdd and this term became more popular in ai and machine learning community however the term data mining became more popular in the business and press communities currently data mining and knowledge discovery are used interchangeably since about predictive analytics and since data science terms were also used to describe this field background the manual extraction of patterns from data has occurred for centuries early methods of identifying patterns in data include bayes theorem and regression analysis the proliferation ubiquity and increasing power of computer technology has dramatically increased data collection storage and manipulation ability as data sets have grown in size and complexity direct hands on data analysis has increasingly been augmented with indirect automated data processing aided by other discoveries in computer science such as neural networks cluster analysis genetic algorithms decision trees and decision rules and support vector machines data mining is the process of applying these methods with the intention of uncovering hidden patterns in large data sets it bridges the gap from applied statistics and artificial intelligence which usually provide the mathematical background to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently allowing such methods to be applied to ever larger data sets research and evolution the premier professional body in the field is the association for computing machinery acm special interest group sig on knowledge discovery and data mining sigkdd since this acm sig has hosted an annual international conference and published its proceedings and since it has published biannual academic journal titled sigkdd explorations computer science conferences on data mining include cikm conference acm conference on information and knowledge management dmin conference international conference on data mining dmkd conference research issues on data mining and knowledge discovery ecdm conference european conference on data mining ecml pkdd conference european conference on machine learning and principles and practice of knowledge discovery in databases edm conference international conference on educational data mining infocom conference ieee infocom icdm conference ieee international conference on data mining kdd conference acm sigkdd conference on knowledge discovery and data mining mldm conference machine learning and data mining in pattern recognition pakdd conference the annual pacific asia conference on knowledge discovery and data mining paw conference predictive analytics world sdm conference siam international conference on data mining siam sstd symposium symposium on spatial and temporal databases wsdm conference acm conference on web search and data mining data mining topics are also present on many data management database conferences such as the icde conference sigmod conference and international conference on very large data bases process the knowledge discovery in databases kdd process is commonly defined with the stages selection pre processing transformation data mining interpretation evaluation it exists however in many variations on this theme such as the cross industry standard process for data mining crisp dm which defines six phases business understanding data understanding data preparation modeling evaluation deployment or simplified process such as pre processing data mining and results validation polls conducted in and show that the crisp dm methodology is the leading methodology used by data miners the only other data mining standard named in these polls was semma however times as many people reported using crisp dm several teams of researchers have published reviews of data mining process models and azevedo and santos conducted comparison of crisp dm and semma in pre processing before data mining algorithms can be used target data set must be assembled as data mining can only uncover patterns actually present in the data the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit common source for data is data mart or data warehouse pre processing is essential to analyze the multivariate data sets before data mining the target set is then cleaned data cleaning removes the observations containing noise and those with missing data data mining data mining involves six common classes of tasks anomaly detection outlier change deviation detection the identification of unusual data records that might be interesting or data errors that require further investigation association rule learning dependency modelling searches for relationships between variables for example supermarket might gather data on customer purchasing habits using association rule learning the supermarket can determine which products are frequently bought together and use this information for marketing purposes this is sometimes referred to as market basket analysis clustering is the task of discovering groups and structures in the data that are in some way or another similar without using known structures in the data classification is the task of generalizing known structure to apply to new data for example an mail program might attempt to classify an mail as legitimate or as spam regression attempts to find function which models the data with the least error summarization providing more compact representation of the data set including visualization and report generation results validation data mining can unintentionally be misused and can then produce results which appear to be significant but which do not actually predict future behavior and cannot be reproduced on new sample of data and bear little use often this results from investigating too many hypotheses and not performing proper statistical hypothesis testing simple version of this problem in machine learning is known as overfitting but the same problem can arise at different phases of the process and thus train test split when applicable at all may not be sufficient to prevent this from happening the final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set not all patterns found by the data mining algorithms are necessarily valid it is common for the data mining algorithms to find patterns in the training set which are not present in the general data set this is called overfitting to overcome this the evaluation uses test set of data on which the data mining algorithm was not trained the learned patterns are applied to this test set and the resulting output is compared to the desired output for example data mining algorithm trying to distinguish spam from legitimate emails would be trained on training set of sample mails once trained the learned patterns would be applied to the test set of mails on which it had not been trained the accuracy of the patterns can then be measured from how many mails they correctly classify number of statistical methods may be used to evaluate the algorithm such as roc curves if the learned patterns do not meet the desired standards subsequently it is necessary to re evaluate and change the pre processing and data mining steps if the learned patterns do meet the desired standards then the final step is to interpret the learned patterns and turn them into knowledge standards there have been some efforts to define standards for the data mining process for example the european cross industry standard process for data mining crisp dm and the java data mining standard jdm development on successors to these processes crisp dm and jdm was active in but has stalled since jdm was withdrawn without reaching final draft for exchanging the extracted models in particular for use in predictive analytics the key standard is the predictive model markup language pmml which is an xml based language developed by the data mining group dmg and supported as exchange format by many data mining applications as the name suggests it only covers prediction models particular data mining task of high importance to business applications however extensions to cover for example subspace clustering have been proposed independently of the dmg notable uses data mining is used wherever there is digital data available today notable examples of data mining can be found throughout business medicine science and surveillance privacy concerns and ethics while the term data mining itself has no ethical implications it is often associated with the mining of information in relation to peoples behavior ethical and otherwise the ways in which data mining can be used can in some cases and contexts raise questions regarding privacy legality and ethics in particular data mining government or commercial data sets for national security or law enforcement purposes such as in the total information awareness program or in advise has raised privacy concerns data mining requires data preparation which can uncover information or patterns which may compromise confidentiality and privacy obligations common way for this to occur is through data aggregation data aggregation involves combining data together possibly from various sources in way that facilitates analysis but that also might make identification of private individual level data deducible or otherwise apparent this is not data mining per se but result of the preparation of data before and for the purposes of the analysis the threat to an individual privacy comes into play when the data once compiled cause the data miner or anyone who has access to the newly compiled data set to be able to identify specific individuals especially when the data were originally anonymous it is recommended that an individual is made aware of the following before data are collected the purpose of the data collection and any known data mining projects how the data will be used who will be able to mine the data and use the data and their derivatives the status of security surrounding access to the data how collected data can be updated data may also be modified so as to become anonymous so that individuals may not readily be identified however even de identified anonymized data sets can potentially contain enough information to allow identification of individuals as occurred when journalists were able to find several individuals based on set of search histories that were inadvertently released by aol situation in europe europe has rather strong privacy laws and efforts are underway to further strengthen the rights of the consumers however the safe harbor principles currently effectively expose european users to privacy exploitation by companies as consequence of edward snowden global surveillance disclosure there has been increased discussion to revoke this agreement as in particular the data will be fully exposed to the national security agency and attempts to reach an agreement have failed situation in the united states in the united states privacy concerns have been addressed by the us congress via the passage of regulatory controls such as the health insurance portability and accountability act hipaa the hipaa requires individuals to give their informed consent regarding information they provide and its intended present and future uses according to an article in biotech business week in practice hipaa may not offer any greater protection than the longstanding regulations in the research arena says the aahc more importantly the rule goal of protection through informed consent is undermined by the complexity of consent forms that are required of patients and participants which approach level of to average individuals this underscores the necessity for data anonymity in data aggregation and mining practices information privacy legislation such as hipaa and the family educational rights and privacy act ferpa applies only to the specific areas that each such law addresses use of data mining by the majority of businesses in the is not controlled by any legislation copyright law situation in europe due to lack of flexibilities in european copyright and database law the mining of in copyright works such as web mining without the permission of the copyright owner is not legal where database is pure data in europe there is likely to be no copyright but database rights may exist so data mining becomes subject to regulations by the database directive on the recommendation of the hargreaves review this led to the uk government to amend its copyright law in to allow content mining as limitation and exception only the second country in the world to do so after japan which introduced an exception in for data mining however due to the restriction of the copyright directive the uk exception only allows content mining for non commercial purposes uk copyright law also does not allow this provision to be overridden by contractual terms and conditions the european commission facilitated stakeholder discussion on text and data mining in under the title of licences for europe the focus on the solution to this legal issue being licences and not limitations and exceptions led to representatives of universities researchers libraries civil society groups and open access publishers to leave the stakeholder dialogue in may situation in the united states by contrast to europe the flexible nature of us copyright law and in particular fair use means that content mining in america as well as other fair use countries such as israel taiwan and south korea is viewed as being legal as content mining is transformative that is it does not supplant the original work it is viewed as being lawful under fair use for example as part of the google book settlement the presiding judge on the case ruled that google digitisation project of in copyright books was lawful in part because of the transformative uses that the digitisation project displayed one being text and data mining software free open source data mining software and applications carrot text and search results clustering framework chemicalize org chemical structure miner and web search engine elki university research project with advanced cluster analysis and outlier detection methods written in the java language gate natural language processing and language engineering tool knime the konstanz information miner user friendly and comprehensive data analytics framework massive online analysis moa real time big data stream mining with concept drift tool in the java programming language ml flex software package that enables users to integrate with third party machine learning packages written in any programming language execute classification analyses in parallel across multiple computing nodes and produce html reports of classification results mlpack library collection of ready to use machine learning algorithms written in the language nltk natural language toolkit suite of libraries and programs for symbolic and statistical natural language processing nlp for the python language opennn open neural networks library orange component based data mining and machine learning software suite written in the python language programming language and software environment for statistical computing data mining and graphics it is part of the gnu project scavis java cross platform data analysis framework developed at argonne national laboratory senticnet api semantic and affective resource for opinion mining and sentiment analysis tanagra visualisation oriented data mining software also for teaching torch an open source deep learning library for the lua programming language and scientific computing framework with wide support for machine learning algorithms uima the uima unstructured information management architecture is component framework for analyzing unstructured content such as text audio and video originally developed by ibm weka suite of machine learning software applications written in the java programming language commercial data mining software and applications angoss knowledgestudio data mining tool provided by angoss clarabridge enterprise class text analytics solution grapheme data mining and visualization software provided by ichrome hp vertica analytics platform data mining software provided by hp ibm spss modeler data mining software provided by ibm kxen modeler data mining tool provided by kxen lionsolver an integrated software application for data mining business intelligence and modeling that implements the learning and intelligent optimization lion approach microsoft analysis services data mining software provided by microsoft netowl suite of multilingual text and entity analytics products that enable data mining opentext big data analytics visual data mining predictive analysis by open text corporation oracle data mining data mining software by oracle pseven platform for automation of engineering simulation and analysis optimization and data mining provided by datadvance qlucore omics explorer data mining software provided by qlucore rapidminer an environment for machine learning and data mining experiments sas enterprise miner data mining software provided by the sas institute statistica data miner data mining software provided by statsoft marketplace surveys several researchers and organizations have conducted reviews of data mining tools and surveys of data miners these identify some of the strengths and weaknesses of the software packages they also provide an overview of the behaviors preferences and views of data miners some of these reports include wiley reviews data mining and knowledge discovery rexer analytics data miner surveys forrester research predictive analytics and data mining solutions report gartner magic quadrant report robert nisbet three part series of articles data mining tools which one is best for crm haughton et al review of data mining software packages in the american statistician goebel gruenwald survey of data mining knowledge discovery software tools in sigkdd explorations see also methods application domains application examples related topics data mining is about analyzing data for information about extracting information out of data see references further reading cabena peter hadjnian pablo stadler rolf verhees jaap zanasi alessandro discovering data mining from concept to implementation prentice hall isbn chen han yu data mining an overview from database perspective knowledge and data engineering ieee transactions on feldman ronen sanger james the text mining handbook cambridge university press isbn guo yike and grossman robert editors high performance data mining scaling algorithms applications and systems kluwer academic publishers han jiawei micheline kamber and jian pei data mining concepts and techniques morgan kaufmann hastie trevor tibshirani robert and friedman jerome the elements of statistical learning data mining inference and prediction springer isbn liu bing web data mining exploring hyperlinks contents and usage data springer isbn nisbet robert elder john miner gary handbook of statistical analysis data mining applications academic press elsevier isbn poncelet pascal masseglia florent and teisseire maguelonne editors october data mining patterns new methods and applications information science reference isbn tan pang ning steinbach michael and kumar vipin introduction to data mining isbn theodoridis sergios and koutroumbas konstantinos pattern recognition th edition academic press isbn weiss sholom and indurkhya nitin predictive data mining morgan kaufmann see also free weka software ye nong the handbook of data mining mahwah nj lawrence erlbaum external links"}